
\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{array}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[margin=1in]{geometry}
\usepackage{newunicodechar}
\usepackage[most]{tcolorbox}
\usepackage{enumitem}

\linespread{1.15}

% 유니코드 특수 문자 정의
%\newunicodechar{∙}{\textbullet}
%\newunicodechar{--}{--}
%\newunicodechar{---}{---}
%\newunicodechar{...}{\ldots}

% 한글 지원
\usepackage{kotex}

% 이미지 경로
% \graphicspath{{./temp_output/images/}}

% Float 설정
\floatplacement{figure}{H}
\floatplacement{table}{H}

% 커버 페이지 스타일링
\usepackage{titling}
\renewcommand{\maketitlehooka}{\centering}
\renewcommand{\maketitlehookb}{\vfill}
\renewcommand{\maketitlehookc}{\vfill}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\title{\Huge\textbf{Co-Packaged Optics (CPO) Book -- Scaling with Light for the Next Wave of Interconnect}}
\author{\Large HTML to LaTeX Converter}
\date{\large\today}

\begin{document}

% 커버 페이지
\begin{titlepage}
\centering
\vspace*{7cm}

{\Huge\textbf{Co-Packaged Optics (CPO) Book -- Scaling with Light for the Next Wave of Interconnect}}

\vspace{4cm}

{\Huge\today}

\vfill

\end{titlepage}

% 목차 페이지
\tableofcontents
\newpage

\textbf{Co-Packaged Optics (CPO)}는 오랫동안 datacenter 연결성을 혁신할 것으로 기대되어 왔지만, 실질적으로 배포 가능한 제품이 2025년에야 출시되면서 시장에 나오기까지 오랜 시간이 걸렸다. 그동안 pluggable transceiver는 networking 요구사항에 맞춰 발전해왔으며, 상대적인 비용 효율성, 배포의 익숙함, 그리고 표준 기반의 상호운용성 덕분에 여전히 기본 선택지로 남아있다.

하지만 \textbf{Artificial Intelligence (AI)} workload와 함께 오는 막대한 networking 수요는 이번에는 상황이 다르다는 것을 의미한다. AI networking bandwidth roadmap은 interconnect 속도, 거리, 밀도 및 신뢰성 요구사항이 곧 transceiver가 제공할 수 있는 수준을 넘어설 것이다. CPO는 일부 이점을 제공하고 scale-out networking에 더 많은 옵션을 가져다주지만, scale-up networking에서 핵심적인 역할을 할 것이다. \textbf{CPO는 이번 10년 후반과 그 이후 scale-up networking에서 bandwidth 증가의 주요 동력이 될 것이다.}

NVLink와 같은 오늘날의 copper 기반 scale-up 솔루션은 GPU당 7.2 Tbit/s의 엄청난 bandwidth를 제공하며 -- Rubin 세대에서는 곧 GPU당 14.4 Tbit/s가 될 예정이지만, copper 기반 link는 거리가 최대 2 meter로 제한되어, scale-up domain world size가 기껏해야 1개 또는 2개 rack으로 제한된다는 것을 의미한다. 또한 copper를 통해 bandwidth를 확장하는 것이 점점 더 어려워지고 있다. Rubin에서 NVIDIA는 양방향 SerDes를 통해 copper lane당 bandwidth를 또 한 번 두 배로 늘릴 것이지만, 점점 더 빠른 SerDes를 개발하여 copper에서 bandwidth를 두 배로 늘리는 것은 매우 어려운 확장 방향이며 더디게 진행되는 작업이다. \textbf{CPO는 동일하거나 더 나은 bandwidth density를 제공할 수 있으며 bandwidth 확장을 위한 추가적인 방법을 제공하면서, 동시에 더 큰 scale-up domain을 가능하게 한다.}

CPO의 필요성을 이해하기 위한 출발점은 optical communication에 transceiver를 사용할 때의 많은 비효율성과 trade-off를 고려하는 것이다. 
Transceiver는 더 긴 link 거리를 달성하는 데 사용될 수 있지만, transceiver가 꽂히는 networking switch 또는 compute tray의 front panel에 있는 cage는 일반적으로 XPU 또는 switch \textbf{Application-Specific Integrated Circuit (ASIC)}으로부터 15-30cm 떨어져 있다. 
이는 신호가 먼저 \textbf{Long-Reach Serializer-Deserializer (LR SerDes)}를 사용하여 해당 15-30cm 거리를 전기적으로 전송되어야 하며, 
전기 신호가 optical signal로 변환되기 전에 transceiver 내부의 \textbf{Digital Signal Processor (DSP)}에 의해 복구되고 조정되어야 함을 의미한다. 
CPO를 사용하면 optical engine이 대신 XPU 또는 Switch ASIC 옆에 배치되므로, DSP를 제거할 수 있고 XPU에서 Optical Engine으로 데이터를 이동하는 데 더 낮은 전력의 SerDes를 사용할 수 있다. 
이는 DSP Transceiver와 비교하여 데이터 전송에 필요한 에너지를 50\% 이상 줄일 수 있으며, 많은 이들이 bit당 에너지 요구사항을 80\%까지 줄이는 것을 목표로 하고 있다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/23886490-5277-4d66-ba9a-86a9f43fe36b_1024x333.png}
\caption{Source: SemiAnalysis}
\end{figure}
NVIDIA와 Broadcom의 scale-out CPO 솔루션이 더 많은 주목을 받고 있고 최종 고객들에 의해 면밀히 검토되고 있는 반면, 주요 Hyperscaler들은 이미 scale-up CPO 전략을 계획하기 시작했으며 공급업체들과 약속을 하고 있다. 예를 들어, \href{https://d1io3yog0oux5.cloudfront.net/_a2ff1b1766821fdbdf60a17efbf050dd/marvell/files/pages/marvell/db/3831/description/2025-12_02-Marvell-to-Acquire-Celestial-AI-vF2.pdf}{Celestial AI는 2028년 말까지 \$1B revenue run rate를 창출할 수 있을 것으로 추정하고 있다} - 우리는 이것이 주로 Amazon의 Trainium 4와 함께 출하되는 CPO scale-up 솔루션에 의해 주도될 것으로 믿는다.
CPO에 집중하는 기업들은 이제 논문, pilot project 및 demonstration을 훨씬 넘어섰으며 대량 생산을 해결하기 위한 optical port architecture와 같은 핵심 제품 결정을 내리고 있다. 

Scale-up을 위한 CPO는 이제 \textbf{만약과 왜의 문제가 아니라, 언제 그리고 어떻게의 문제이다}. 이러한 시스템을 대량 생산으로 가져오는 방법, 그리고 laser manufacturer와 같은 핵심 component 공급망 회사들이 충분한 생산을 늘릴 수 있는 시기의 문제이다.
이 문서는 CPO의 이점과 과제, CPO architecture가 작동하는 방식, 현재 및 미래의 CPO 제품, CPO에 집중하는 기업, CPO 관련 component 및 각각의 공급망에 대한 심층적인 논의를 제시할 것이다. 이 글은 실무자, 산업 분석가, 투자자 및 interconnect 기술에 관심이 있는 모든 사람들을 위한 가이드 역할을 하도록 의도되었다.
\textbf{우리는 이 문서를 다섯 부분으로 나누었으며, 독자들은 가장 흥미롭거나 관련성이 높은 섹션에 집중할 수 있다.}
\\\\
\textbf{Part 1: Total Cost of Ownership (TCO) Analysis}에서는 CPO 채택이 scale-out 및 scale-up network의 total cost of ownership을 어떻게 변화시키는지 분석하는 것으로 시작한다. 우리는 TCO, 신뢰성, 
그리고 장비 vendor bargaining power가 scale-out network에서 CPO를 채택하는 주요 고려사항이 될 것이라고 생각한다. 
우리는 scale-out과 관련하여 CPO가 본격적으로 준비되었는지 살펴볼 것이며, 
ECOC 2025에서 발표된 \textbf{Meta의 CPO scale-out switch 연구}와 같은 솔루션 신뢰성에 대해 지금까지 확보한 데이터를 다룰 것이다.
\\\\
\textbf{Part 2: CPO Introduction and Implementation}에서는 CPO가 어떻게 작동하는지 더 깊이 탐구할 것이다. 
이 섹션에서는 \textbf{copper에서 co-packaged copper로}, 그리고 \textbf{digital signal processor (DSP) optics에서 linear pluggable optics (LPO)를 거쳐 CPO로}의 시장 진화를 살펴보고 CPO 채택의 동기와 논거를 다룰 것이다. 
\textbf{SerDes scaling limit와 Wide I/O}가 SerDes의 대안으로 논의될 것이다.
\\\\
\textbf{Part 3: Bringing CPO to Market}에서는 CPO가 견인력을 얻고 시장에 출시될 수 있도록 하는 핵심 기술을 설명할 것이다. 먼저 Host와 Optical Engine packaging을 논의하고 \textbf{TSMC COUPE}를 상세히 설명하며 이것이 왜 선택되는 통합 옵션으로 부상하고 있는지 설명한다. 
\textbf{Fiber Attach Unit (FAU)}, \textbf{Fiber coupling} 뿐만 아니라 \textbf{Edge Coupling} vs \textbf{Grating Coupler}가 철저히 설명될 것이다. 
\textbf{Mach-Zender Modulator (MZM), Micro-Ring Modulator (MRM) 및 Electro-Absorption Modulator (EAM)}와 같은 modulator type을 다룰 것이다. 이 섹션은 CPO가 채택되는 핵심 이유인 CPO로 bandwidth를 확장하는 다양한 방법을 설명한다.
\\\\
\textbf{Part 4: CPO Products of Today and Tomorrow}에서는 오늘날 시장에서 사용 가능한 CPO 제품과 관련 공급망을 분석할 것이다. 주요 CPO 기업을 논의하기 전에 \textbf{NVIDIA와 Broadcom}의 솔루션으로 시작할 것이다. 우리는 Ayar Labs, Nubis, Celestial AI, Lightmatter, Xscape Photonics, Ranovus 및 Scintil을 다루며, 각 제공업체의 솔루션을 상세히 설명하고 각 기업의 접근 방식에 대한 중요한 장단점을 평가할 것이다.
\\\\
\textbf{Part 5: NVIDIA's CPO Supply Chain}에서는 NVIDIA의 CPO ecosystem에 대한 공급망을 상세히 설명하여 이 보고서를 마무리할 것이며, Laser Source, ELS Module, FAU, FAU Alight Tool, FAU Assembly, Shuffle Box, MPO Connector, MT Ferrule, Fiber 및 E/O Testing에 대한 주요 공급업체를 명시할 것이다.

\newpage{}

\section{Part 1: CPO Total Cost of Ownership (TCO) Analysis}

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=\textbf{핵심 용어: CPO (Co-Packaged Optics)}]
CPO는 광학 엔진(optical engine)을 네트워크 스위치 ASIC에 직접 패키징하는 기술로, 기존의 플러그형 광 트랜시버(pluggable optical transceiver)를 제거하여 전력 소비와 비용을 크게 줄입니다. 이는 데이터센터 네트워킹의 패러다임 전환을 의미하며, 특히 AI 클러스터의 대규모 interconnect 요구사항을 충족하기 위한 핵심 기술입니다.
\end{tcolorbox}

올해 초 NVIDIA의 GTC 2025에서 가장 큰 기대를 모은 주제 중 하나는 Jensen이 회사의 첫 CPO 지원 scale-out network switch를 발표한 것이었다. 
주목할 만한 점은 scale-up의 경우 NVIDIA는 여전히 copper를 밀고 나가며 2027년과 2028년까지도 optics로 가는 것을 피하기 위해 극단적인 노력을 기울이고 있다는 것이다.

이러한 새로운 CPO 지원 switch에 대한 논의를 TCO를 검토하는 것으로 시작하여, scale-out CPO가 제공할 수 있는 비용 및 전력 절감을 분석해보자.
NVIDIA의 GTC 2025 keynote는 두 가지 다른 CPO 지원 switch ASIC을 활용하는 세 가지 다른 CPO scale-out switch를 발표했다. 
TCO, 전력 및 배포 속도 이점이 있지만 고객들이 완전히 다른 배포 체계로 즉시 뛰어들기에는 충분히 설득력이 없으며, 
우리는 첫 번째 wave의 CPO scale-out switch에 대한 채택이 제한적일 것으로 예상한다. 그 이유를 살펴보자.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/a562b30c-79ed-4b9d-8670-748449a77c45_1020x727.png}
\caption{Source: SemiAnalysis}
\end{figure}

\subsection{Typical AI Cluster Networking Configuration and TCO}
일반적인 AI Cluster는 세 가지 주요 networking fabric인 back-end, front-end 및 out of band management fabric을 갖는다. 

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: AI Cluster의 세 가지 Networking Fabric}]
\textbf{1. Back-end Fabric (GPU 간 통신)}:
\begin{itemize}
\item \textbf{용도}: GPU 간 scale-out communication, collective operation (All-Reduce, All-Gather 등)
\item \textbf{프로토콜}: InfiniBand 또는 Ethernet (RoCE)
\item \textbf{특징}: 가장 기술적으로 까다롭고 비용/전력 소비가 큼
\item \textbf{대역폭}: 매우 높음 (GPU당 800Gbit/s 이상)
\item \textbf{레이턴시}: 극도로 낮아야 함 (마이크로초 단위)
\item \textbf{비중}: Networking 비용의 85\%, 전력의 86\% 차지 (3-layer GB300 NVL72 기준)
\item \textbf{CPO 적용 대상}: 주요 타겟! 비용과 전력 절감 효과가 가장 큼
\end{itemize}

\textbf{2. Front-end Fabric (외부 통신)}:
\begin{itemize}
\item \textbf{용도}: 클러스터와 외부 세계 간 통신 (스토리지, 인터넷, 사용자 요청)
\item \textbf{프로토콜}: 주로 Ethernet
\item \textbf{특징}: Back-end보다 성능 요구사항 낮음, 유연성 중시
\item \textbf{대역폭}: 중간 수준 (일반적으로 100Gbit/s - 400Gbit/s)
\item \textbf{비중}: Networking 비용/전력의 작은 부분 차지
\item \textbf{CPO 적용}: 가능하지만 우선순위 낮음
\end{itemize}

\textbf{3. Out-of-Band Management Fabric (관리 통신)}:
\begin{itemize}
\item \textbf{용도}: 시스템 모니터링, 관리, 펌웨어 업데이트, 원격 접속
\item \textbf{프로토콜}: 저속 Ethernet (1Gbit/s - 10Gbit/s)
\item \textbf{특징}: 메인 네트워크와 물리적으로 분리되어 장애 시에도 관리 가능
\item \textbf{대역폭}: 매우 낮음, 신뢰성이 더 중요
\item \textbf{비중}: 비용/전력의 극소수 차지
\item \textbf{CPO 적용}: 불필요 (낮은 대역폭으로 충분)
\end{itemize}

\textbf{핵심 포인트}: CPO의 주요 적용 대상은 back-end fabric입니다. 이 네트워크가 전체 networking 비용과 전력의 대부분을 차지하기 때문에, CPO의 TCO 개선 효과도 여기에 집중됩니다.
\end{tcolorbox}

가장 많이 활용되고 기술적으로 까다로운 network fabric은 back-end fabric이다. 
Back-end fabric은 GPU 간의 scale-out communication에 사용되어 서로 통신하고 training 및 inference를 병렬화하기 위한 collective operation에서 데이터를 교환한다. 
Back-end network는 일반적으로 InfiniBand 또는 Ethernet protocol을 사용한다.

까다로운 특성으로 인해 back-end network는 전체 networking 비용과 전력의 지배적인 점유율을 차지하며, 
NVIDIA의 X800-Q3400 back-end switch를 사용하는 InfiniBand에 배포된 3-layer GB300 NVL72 cluster의 경우 networking 비용의 85\%와 networking 전력의 86\%를 차지한다. 
CPO 기반 switch와 networking 솔루션은 back-end 및 front-end network 모두에서 사용될 수 있지만, 
이 단계에서 배포의 초점은 back-end network에 있을 것으로 생각한다. 
독자들은 우리의 \href{https://semianalysis.com/2024/03/25/nvidias-optical-boogeyman-nvl72-infiniband/#the-clos-non-blocking-fat-tree-network}{Optical Boogeyman Article from 2024} 및 \href{https://semianalysis.com/ai-networking-model/}{AI Networking Model}에서 back-end network topology, port, switch 및 transceiver 수에 대한 훨씬 더 자세한 내용을 찾을 수 있다. 
Total networking cost of ownership을 이해하고 싶은 분들은 우리의 \href{https://semianalysis.com/2024/10/03/ai-neocloud-playbook-and-anatomy/}{AI Neocloud Anatomy and Playbook article}을 읽을 수 있다.

큰 그림을 보면 networking 비용은 AI server 자체 다음으로 전체 AI cluster 비용의 두 번째로 큰 구성요소이다. 
3-Layer InfiniBand network를 갖춘 GB300 NVL72 Cluster의 경우, 이는 전체 cluster 비용의 15\%에 해당하며, 
4-Layer network의 경우 전체 cluster 비용의 18\%에 도달한다. optical transceiver는 이 비용의 상당 부분을 차지하며, 
상대적으로 더 비싼 NVIDIA LinkX Transceiver를 사용할 때 3-Layer network에서 networking 비용의 60\%를 차지한다. 
또한 3-Layer network에서 전체 networking 전력의 45\%를 소비한다.

\begin{tcolorbox}[colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: 네트워킹 비용 구조}]
AI 클러스터에서 네트워킹은 전체 비용의 15-18\%를 차지하며, 그중 광 트랜시버가 네트워킹 비용의 60\%, 전력의 45\%를 소비합니다. 이는 CPO가 트랜시버를 제거함으로써 상당한 비용 및 전력 절감 가능성을 가진 이유를 설명합니다.
\end{tcolorbox}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/7a51f75a-7d4e-4d00-b119-b8392792d1e5_1289x372.png}
\caption{Source: \href{https://semianalysis.com/ai-networking-model/}{SemiAnalysis AI Networking Model}}
\end{figure}
AI cluster의 GPU 수가 많을수록 더 많은 networking layer가 필요할 가능성이 높다. 
2-layer에서 3-layer network 이상으로 가는 것은 더 높은 비용과 더 큰 전력 예산을 의미한다. 
CPO는 layer 수를 일정하게 유지하면서 전력과 비용을 줄이는 데 도움을 줄 수 있으며, 
주어진 수의 layer를 가진 network에 연결할 수 있는 GPU 수를 확장하여 전체 전력 및 비용 요구사항을 줄일 수 있다.

\subsection{CPO Scale-out Power Budgets}
올해 초 GTC 2025에서 NVIDIA의 CEO Jensen Huang은 transceiver만으로 인한 막대한 전력 소비를 CPO의 핵심 동기로 강조했다. 
위 표의 rack당 전력 예산 중 일부를 사용하면, 
3-layer network의 200,000 GB300 NVL72 (rack당 72 GPU package 및 144 compute chiplet) GPU cluster는 435 MW의 Critical IT Power를 소비할 것이며 
그 중 17 MW가 optical transceiver만으로 소비될 것이다. 
분명히 대부분의 transceiver content를 제거함으로써 절약할 수 있는 엄청난 양의 전력이 있다.

이는 단일 800G DSP transceiver에서 사용되는 전력을 CPO 시스템 내의 optical engine 및 laser source (800G bandwidth당)가 소비하는 전력과 비교하면 쉽게 알 수 있다. 
800G DR4 optical transceiver가 약 16-17W를 소비하는 반면, NVIDIA의 Q3450 CPO switch에 사용되는 optical engine과 external laser source를 합치면 800G bandwidth당 약 4-5W를 소비하여 전력이 73\% 감소하는 것으로 추정된다.

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: DSP, LPO 그리고 CPO의 전력 비교}]
\textbf{DSP (Digital Signal Processing) Transceiver}: 신호 처리 기능이 내장된 전통적인 플러그형 트랜시버로, 800G당 약 16-17W 소비.

\textit{전력 소비 원인:}
\begin{itemize}
\item \textbf{Digital signal processing chip}: 신호 품질 향상을 위한 복잡한 알고리즘 실행으로 5-6W 소비
\item \textbf{Electro-optic/Opto-electric conversion (E/O, O/E)}: 전기 신호와 광 신호 간 변환에 4-5W 소비
\item \textbf{Integrated laser source}: 각 transceiver마다 독립적인 laser 작동에 3-4W 소비
\item \textbf{Cooling 및 power management circuit}: 열 관리와 전력 변환에 2-3W 소비
\item \textbf{긴 electrical signal path}: Pluggable form factor로 인한 신호 감쇠와 재생에 추가 전력 필요
\end{itemize}

\textbf{LPO (Linear-drive Pluggable Optics)}: DSP 기능을 제거한 저전력 트랜시버로, DSP 대비 36\% 전력 절감 (약 10-11W).

\textit{전력 절감 메커니즘:}
\begin{itemize}
\item \textbf{DSP chip 제거}: Digital signal processing을 switch ASIC의 SerDes로 이동하여 5-6W 절감
\item \textbf{Analog direct drive}: 복잡한 digital modulation 없이 linear drive 방식으로 laser 구동
\item \textbf{Energy efficiency}: 비트당 에너지 효율이 16.3pJ/bit (DSP)에서 9.4pJ/bit로 개선
\item \textbf{단점}: 짧은 전송 거리 (일반적으로 100m 이하), 높은 signal integrity 요구
\end{itemize}

\textbf{CPO (Co-Packaged Optics)}: 광학 엔진과 외부 레이저를 스위치에 통합하여 800G당 4-5W만 소비하며, DSP 대비 73\% 전력 절감을 달성합니다.

\textit{획기적인 전력 절감 원리:}
\begin{itemize}
\item \textbf{극단적 path 단축}: Switch ASIC과 optical engine 간 electrical signal path가 수십 cm에서 수 mm로 단축되어 signal attenuation 최소화 (약 2-3W 절감)
\item \textbf{Pluggable interface 제거}: Cable connector와 pluggable interface의 전력 손실 제거 (약 1-2W 절감)
\item \textbf{Centralized laser}: External Light Source (ELS)가 여러 optical engine에 laser 공급하여 laser당 전력 효율 향상 (약 2-3W 절감)
\item \textbf{Unified cooling}: Switch와 optical component가 단일 cooling system 공유로 overhead 감소 (약 1W 절감)
\item \textbf{최적화된 power distribution}: 단일 package 내 power management로 conversion loss 최소화 (약 0.5-1W 절감)
\item \textbf{최고 energy efficiency}: 비트당 에너지 효율 6.3pJ/bit 달성, DSP 대비 61\% 향상
\end{itemize}

\end{tcolorbox}

이러한 수치는 ECOC 2025에서 발표되고 제출된 Meta의 논문에서 제시된 수치와 매우 유사하다. 
이 보고서에서 Meta는 800G 2xFR4 pluggable transceiver가 약 15W를 소비하는 반면 Broadcom Bailly 51.2T CPO switch 내의 optical engine과 laser source는 전달된 800G bandwidth당 약 5.4W를 소비하여 
65\%의 전력 절감을 나타낸 방법을 보여주었다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/02d05df8-27c2-4d88-a85a-354c6d4675cf_2019x2364.jpg}
\caption{Source: Meta}
\end{figure}
이 분석을 cluster 수준으로 확장해보자. three-Layer network에 구축된 GB300 NVL72 cluster로 전환하면, 
back-end network에서 DSP transceiver에서 LPO transceiver를 사용하는 것으로 이동하면 전체 transceiver 전력을 36\%, 전체 network 전력을 16\% 줄일 수 있음을 알 수 있다. 
CPO로의 완전한 전환은 DSP optics 대비 훨씬 더 큰 절감을 제공한다 (transceiver 전력을 84\% 절감). 

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: Optical Engine (OE)과 External Light Source (ELS)}]
\textbf{Optical Engine (OE)}: 전기 신호를 광 신호로 변환하는 광학 컴포넌트로, CPO 시스템에서 스위치 ASIC에 직접 패키징됩니다. 기존 트랜시버의 복잡한 DSP 기능을 제거하여 전력 효율을 높입니다.

\textbf{External Light Source (ELS)}: CPO 시스템에서 여러 optical engine에 레이저 광을 공급하는 중앙집중식 레이저 소스입니다. 각 트랜시버마다 별도의 레이저를 사용하는 것보다 효율적이며, 유지보수가 용이합니다.

\textbf{전력 Tradeoff 상세 계산}:

Networking 전력 구성 (DSP 기준선):
\begin{itemize}
\item Optical transceiver: 전체 networking 전력의 \textbf{45\%} 차지
\item Switch ASIC 및 기타 구성요소: 전체 networking 전력의 \textbf{55\%} 차지
\end{itemize}

CPO 전환 시 전력 변화:
\begin{itemize}
\item \textbf{Transceiver 전력}: 45\% $\rightarrow$ 45\% $\times$ (1 - 0.84) = \textbf{7.2\%}
  \begin{itemize}
  \item 84\% 감소하지만, 원래 전체의 45\%만 차지했으므로
  \item 실제 절감량: 45\% - 7.2\% = \textbf{37.8\%p 절감}
  \end{itemize}
\item \textbf{Switch 전력}: 55\% $\rightarrow$ 55\% $\times$ 1.23 = \textbf{67.7\%}
  \begin{itemize}
  \item Optical Engine (OE)과 External Light Source (ELS) 추가로 23\% 증가
  \item 실제 증가량: 67.7\% - 55\% = \textbf{12.7\%p 증가}
  \end{itemize}
\item \textbf{순 효과}: 7.2\% + 67.7\% = 74.9\%
  \begin{itemize}
  \item 전체 networking 전력 절감: 100\% - 74.9\% = \textbf{25.1\% 감소}
  \item 논문에서 제시된 23\% 감소와 일치
  \end{itemize}
\end{itemize}

\textbf{핵심 통찰}: Transceiver의 대폭적인 전력 감소(37.8\%p)가 switch의 증가분(12.7\%p)을 크게 상회하여, 전체적으로 약 25\% 순 절감을 달성합니다. 이는 transceiver를 제거하는 것이 아니라 "더 효율적인 형태로 switch에 통합"하는 전략입니다.

\textbf{핵심}: CPO는 transceiver를 제거하여 큰 전력 절감을 달성하지만, 그 기능을 스위치 내부로 이동시키므로 스위치 자체의 전력 소비는 증가합니다. 이는 "전력을 없애는" 것이 아니라 "더 효율적인 방식으로 재배치"하는 것입니다.
\end{tcolorbox}

이 전력 절감의 일부는 switch에 \textbf{Optical Engine (OE)}과 \textbf{External Light Source (ELS)}를 추가함으로써 상쇄되며, 
이는 이제 전체적으로 23\% 더 많은 전력을 소비한다. 
아래 예에서 CPO 시나리오의 optical transceiver 전력은 server당 1,000W에 머물러 있는데, 
이는 front-end networking이 여전히 DSP transceiver를 사용할 것으로 가정하기 때문이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/2a73e386-08b3-458c-8c86-46a52fc6efff_1607x374.jpg}
\caption{Source: \href{https://semianalysis.com/ai-networking-model/}{SemiAnalysis AI Networking Model}}
\end{figure}
NVIDIA의 CPO scale-out switch 사용은 암묵적으로 high radix network가 기본적으로 사용됨을 의미하지만, 이는 최종 사용자에게 \textbf{추상화}되는데, 
shuffle이 high radix non-CPO switch를 사용할 때 patch panel이나 octopus cable을 통해 switch box 외부에서 발생하는 것과 달리 switch box 내부에서 발생하기 때문이다. 

\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: High Radix Network와 Internal Shuffle}]

\textbf{1. 핵심 개념 정의}

\textbf{Radix}: Switch가 제공하는 port 수. High radix는 많은 수의 port를 의미합니다 (예: 128포트 이상).

\textbf{Shuffle}: 여러 switch의 port를 상호 연결하여 하나의 논리적 대형 switch처럼 작동하도록 만드는 재배치 작업.

\vspace{0.3cm}

\textbf{2. 구체적 예시: 512포트 달성하기}

\textbf{Traditional 방식 (Non-CPO High Radix):}
\begin{itemize}
\item \textbf{하드웨어 구성}: 4개의 128포트 switch를 별도로 설치
\item \textbf{External shuffle 필요}:
  \begin{itemize}
  \item Switch A의 port 1-32 → Patch panel → Switch B의 특정 port들로 연결
  \item Switch B의 port 1-32 → Octopus cable → Switch C의 특정 port들로 연결
  \item Switch C와 D도 유사하게 복잡하게 교차 연결
  \item 수백 개의 cable이 switch box \textbf{외부}에서 얽힘
  \end{itemize}
\item \textbf{문제점}:
  \begin{itemize}
  \item \textbf{물리적 복잡도}: 수백 개의 cable을 수동으로 연결하고 관리
  \item \textbf{설치 시간}: 며칠이 소요되는 cable routing
  \item \textbf{디버깅 악몽}: Cable 하나가 잘못 연결되면 전체 network 문제 발생
  \item \textbf{공간 낭비}: Patch panel, cable tray, shuffle box가 차지하는 rack 공간
  \item \textbf{전력 낭비}: 각 pluggable transceiver와 긴 cable path에서 신호 손실
  \item \textbf{확장성 한계}: Port 추가 시 전체 cabling을 재구성해야 함
  \end{itemize}
\item \textbf{사용자 관점}: "내가 4개의 switch를 복잡하게 연결하고 있다"는 것을 \textbf{항상 인식}해야 함
\end{itemize}

\vspace{2cm}

\textbf{CPO 방식 (Internal Shuffle):}
\begin{itemize}
\item \textbf{하드웨어 구성}: 단일 switch box (예: Spectrum 6800)
\item \textbf{Internal shuffle 메커니즘}:
  \begin{itemize}
  \item Switch ASIC 내부에 여러 개의 작은 switching unit 통합
  \item 이들을 \textbf{광학적으로 switch box 내부}에서 상호 연결
  \item Optical engine과 optical interconnect가 shuffle 기능 수행
  \item 모든 복잡한 연결이 package 내부에서 완료
  \end{itemize}
\item \textbf{결과}: 512포트를 제공하는 "단일 switch"처럼 보임
\item \textbf{이점}:
  \begin{itemize}
  \item \textbf{Zero external cable}: External shuffle cable 완전 제거
  \item \textbf{Plug-and-play}: Switch box만 rack에 장착하면 즉시 사용 가능
  \item \textbf{추상화 완성}: 사용자는 "512포트 단일 switch"로만 인식
  \item \textbf{공간 절약}: Patch panel과 cable tray 불필요
  \item \textbf{전력 효율}: 짧은 optical path로 신호 손실 최소화
  \item \textbf{안정성}: Factory에서 검증된 내부 연결로 연결 오류 제로
  \end{itemize}
\item \textbf{사용자 관점}: "그냥 512포트 switch 하나"로 \textbf{완전히 추상화}됨
\end{itemize}

\vspace{0.3cm}

\textbf{3. 실제 제품 예시}
\begin{itemize}
\item \textbf{NVIDIA Quantum 3450 (CPO)}: 800G × 144 port를 단일 switch box로 제공
\item \textbf{NVIDIA Spectrum 6800 (CPO)}: 800G × 512 port를 단일 switch box로 제공
\item 비교: Traditional 방식으로 512포트를 달성하려면 최소 4-8개의 switch + 복잡한 external cabling 필요
\end{itemize}

\vspace{0.3cm}

\textbf{4. CPO가 Internal Shuffle을 가능하게 하는 이유}

Traditional pluggable transceiver는 물리적으로 switch 전면 panel에 위치해야 하므로, 여러 switch를 통합하려면 반드시 external cable이 필요합니다. 반면 CPO는 optical engine을 switch ASIC package 내부에 통합하므로, package 내부의 optical waveguide나 optical fiber를 통해 switch unit 간 연결이 가능합니다. 이것이 "internal shuffle"의 핵심 기술적 기반입니다.

\end{tcolorbox}

대신, 이러한 NVIDIA CPO switch는 매우 높은 port 수를 갖는 것으로 나타낸다.
예를 들어 Quantum 3450은 800G의 144 port를 제공하고 Spectrum 6800은 800G의 512 port를 제공한다. 
우리가 \textbf{기본적으로}라는 단어를 사용하는 이유는 NVIDIA의 non-CPO InfiniBand Quantum Q3400 switch도 800G의 144 port를 제공하지만, 
QM9700과 같은 다른 InfiniBand switch는 800G의 32 port만 제공하기 때문이며, 
전자만이 많은 수의 효과적인 port를 제공하기 위해 이 \textbf{high radix in a box}를 제공한다. 
이러한 높은 port 수는 잠재적으로 고객이 three-Layer에서 2-layer network로 network를 평탄화할 수 있게 하며, 
shuffle box와 patch panel 또는 다루기 어려운 octopus cable을 배포하는 번거로움을 고객에게 덜어주어 핵심 판매 포인트가 될 수 있다. 
2-layer의 경우, transceiver 전력은 84\% 감소하고, switch 전력은 21\% 감소하며, 전체 networking 전력은 전통적인 DSP transceiver 대비 48\% 감소할 수 있다.

Spectrum 6800 switch는 사용 가능한 두 가지 logical configuration 모두에서 많은 수의 port를 갖추고 있으며(800G의 512 port),
800G의 128 port, 400G의 256 port 또는 200G의 512 port를 제공하는 Spectrum 6810과 비교할 때 특히 이를 가능하게 한다. 
Spectrum 6810을 사용하는 800G의 128 port 옵션의 경우, network는 2-layer network에서 최대 8,192 GPU를 연결할 수 있는 반면, 
800G의 512 port를 갖춘 Spectrum 6800은 131,072 GPU를 연결할 수 있다.

\begin{tcolorbox}[colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: Network Scaling 수학}]
L-layer network에서 k port의 switch로 지원 가능한 최대 host 수: $\text{Max Hosts} = k^L$

\textbf{Port 수의 지수적 효과}:
\begin{itemize}
\item 2-layer network에서 128 port → 최대 $128^2 = 16,384$ hosts
\item 2-layer network에서 512 port → 최대 $512^2 = 262,144$ hosts
\item Port를 2배로 늘리면 → 지원 가능한 host가 $2^L$배 증가
\item 2-layer에서 port 2배 증가 → host 4배 증가!
\end{itemize}

\textbf{Port 증가 방법}:
\begin{itemize}
\item Internal shuffle (CPO 스위치)
\item Breakout cable (800G → 2x400G)
\item Twin-port transceiver
\end{itemize}

이것이 CPO의 high radix가 network layer를 줄이고 연결 가능한 GPU 수를 극적으로 늘리는 이유입니다.
\end{tcolorbox}

간단히 언급하자면, L-layer network에서 k port의 switch를 사용하여 지원할 수 있는 최대 host 수는 다음과 같다:
마법은 port 수 k가 layer 수로 거듭제곱된다는 사실에서 나온다. 
따라서 2-layer network의 경우, internal shuffle (Spectrum 6800의 경우처럼), 
breakout cable 또는 twin-port transceiver를 사용하여 port당 bandwidth의 절반을 할당하여 (즉, 800G port를 두 개의 400G port로 분할) 
logical port 수를 두 배로 늘리면 지원되는 host가 4배가 된다!

지금까지 이 섹션에서 논의된 전력 절감, 3-layer CPO network의 경우 23\%, 2-layer CPO network로 내려가는 경우 48\%는 환상적으로 들리지만, 
문제는 networking이 3-layer network의 경우 전체 cluster 전력의 단지 9\%에 불과하다는 것이다. 
따라서 결국 CPO로 전환하는 영향은 적어도 scale-out network의 경우 상당히 희석된다. 
3-layer network에서 CPO를 사용하도록 전환하면 networking 전력이 23\% 감소하지만 전체 cluster 전력 절감은 단지 2\%에 불과하다. 
2-layer network로 이동하면 networking 비용이 48\% 낮아지지만, 전체 cluster 전력 절감은 단지 4\%에 불과하다.

\begin{tcolorbox}[colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: 전력 절감의 희석 효과}]
CPO는 네트워킹 전력을 23-48\% 절감하지만, 네트워킹이 전체 클러스터 전력의 9\%에 불과하므로 전체 클러스터 관점에서는 2-4\%의 전력 절감만 달성합니다. 이는 scale-out 네트워크에서 CPO 채택이 더딘 주요 이유 중 하나입니다. GPU 자체가 전력의 대부분을 소비하기 때문에, 네트워킹 개선의 효과가 희석되는 것입니다.
\end{tcolorbox}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/6b9c76b1-6d92-4dec-bfb3-3034bb77dc4a_1294x371.png}
\caption{Source: \href{https://semianalysis.com/ai-networking-model/}{SemiAnalysis AI Networking Model}}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/7964218b-b421-468a-b780-91facbc05292_1607x374.jpg}
\caption{Source: \href{https://semianalysis.com/ai-networking-model/}{SemiAnalysis AI Networking Model}}
\end{figure}
전체 cluster 자본 비용을 살펴볼 때도 비슷한 상황이다.

\subsection{CPO Scale-out Total Cost of Ownership (TCO)}
Transceiver를 CPO 솔루션과 비교할 때 비용 세부사항을 간략히 살펴보자. 
첫 번째 NVIDIA CPO Switch인 Quantum X800-Q3450 CPO는 각각 1.6Tbit/s로 작동하는 72개의 optical engine을 사용할 것이다.
이후 버전의 Quantum CPO switch는 각각 3.2Tbit/s에서 작동하는 36개의 optical engine으로 전환할 가능성이 있으며, 
단위당 (FAU 포함) \textasciitilde{}\$1,000의 비용으로 시스템당 총 OE 비용은 \$36K가 된다.

이를 관점에서 보기 위해, 전통적인 optical transceiver module을 대신 사용한다면 총 비용이 얼마나 될지 고려해보자. 
non-CPO X800-Q3400은 72개의 OSFP cage를 갖추고 있으며, 800G에서 144 port를 제공하기 위해 1.6T twin-port transceiver가 사용된다. 

\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: Twin-Port Transceiver와 OSFP Cage}]
\setlength{\parskip}{2pt}
\setlength{\itemsep}{0pt}

\textbf{1. OSFP (Octal Small Form Factor Pluggable) Cage}

\textbf{정의}: Switch 전면 패널에 있는 물리적 slot으로, pluggable optical transceiver를 삽입하는 socket입니다.

\textbf{특징}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{물리적 크기}: QSFP보다 큰 form factor, 고전력/고대역폭 transceiver 지원
\item \textbf{전력 처리}: 최대 15-20W transceiver 지원 가능
\item \textbf{Hot-swappable}: 시스템 가동 중에도 transceiver 교체 가능
\item \textbf{1 cage = 1 transceiver}: 물리적으로 하나의 cage는 하나의 transceiver를 수용
\end{itemize}

\vspace{0.15cm}

\textbf{2. Twin-Port Transceiver의 개념}

\textbf{정의}: 하나의 물리적 transceiver 모듈 내부에 \textbf{2개의 독립적인 광 채널}을 통합한 transceiver입니다.

\textbf{구체적 예시: 1.6T Twin-Port = 2 × 800G}

\textit{Traditional Single-Port 방식:}
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 1개의 800G transceiver = 1개의 OSFP cage 점유 = 1개의 800G port 제공
\item 144개의 800G port를 만들려면 → 144개의 OSFP cage 필요
\item 문제: Switch 전면 패널에 144개의 cage를 물리적으로 배치하기 어려움
\end{itemize}

\textit{Twin-Port 방식:}
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 1개의 1.6T twin-port transceiver = 1개의 OSFP cage 점유 = \textbf{2개의 800G port 제공}
\item 144개의 800G port를 만들려면 → 72개의 OSFP cage만 필요 (144 ÷ 2 = 72)
\item 이점: 동일한 bandwidth를 절반의 물리적 공간에 구현
\end{itemize}

\vspace{0.15cm}

\textbf{3. X800-Q3400의 Port 구성 계산}

\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{물리적 구성}: 72개의 OSFP cage
\item \textbf{사용 transceiver}: 72개의 1.6T (2 × 800G) twin-port transceiver
\item \textbf{논리적 결과}: 72 × 2 = \textbf{144개의 800G port}
\item \textbf{총 bandwidth}: 144 × 800G = 115.2 Tbps
\end{itemize}

\vspace{0.15cm}

\textbf{4. Twin-Port의 내부 구조}

하나의 1.6T twin-port transceiver 내부:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{2개의 독립적인 laser}: 각각 800G 신호 생성
\item \textbf{2개의 독립적인 modulator}: 각각 데이터를 광 신호로 변환
\item \textbf{2개의 fiber output}: 물리적으로 2개의 광섬유 출력
\item \textbf{공유 DSP chip}: 하나의 DSP가 두 채널 모두 처리 (비용 절감)
\item \textbf{공유 cooling}: 단일 heat sink로 두 채널의 열 관리
\end{itemize}

\vspace{0.15cm}

\textbf{5. 비용 계산 예시 (X800-Q3400)}

\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 1개의 1.6T DR8 twin-port transceiver 가격: \textasciitilde\$1,000
\item 필요한 transceiver 수: 72개 (cage 수와 동일)
\item \textbf{총 transceiver 비용}: 72 × \$1,000 = \textbf{\$72,000}
\item 제공되는 논리적 port: 144개의 800G port
\item Port당 비용: \$72,000 ÷ 144 = \textasciitilde\$500/port
\end{itemize}

\vspace{0.15cm}

\textbf{6. Twin-Port vs CPO 비교}

\textit{Traditional Twin-Port:}
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 72개의 1.6T transceiver = \$72,000
\item 각 transceiver는 독립적으로 교체 가능
\item Vendor 선택의 자유, 가격 협상 가능
\end{itemize}

\textit{CPO:}
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 72개의 1.6T optical engine + ELS = \textasciitilde\$36,000 (원가)
\item 하지만 switch vendor margin (60\%) 적용 시 = \$80-90,000
\item Switch에 통합되어 교체 불가, vendor lock-in
\end{itemize}

\vspace{0.1cm}

\textbf{핵심 통찰}: Twin-port transceiver는 물리적 cage 수를 절반으로 줄여 \textbf{높은 port 밀도}를 달성하는 중간 단계 기술입니다. CPO는 이를 한 단계 더 발전시켜 transceiver 자체를 제거하고 switch 내부에 통합하지만, vendor margin과 lock-in 이슈가 발생합니다.

\end{tcolorbox}

일반적인 1.6T DR8 transceiver의 비용을 \$1,000로 가정하면, 이 switch를 채우기 위한 총 transceiver 비용은 \$72,000가 되며, 
이는 CPO switch에 동일한 양의 bandwidth를 제공하는 데 필요한 optical engine 및 ELS의 추정 비용 \$35-40k의 두 배이다. 

\begin{tcolorbox}[colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: CPO 비용의 함정 - Vendor Margin}]
\textbf{Component 비용 vs 최종 구매 가격}:
\begin{itemize}
\item 72개 1.6T OE + ELS 원가: \textasciitilde\$36k
\item 60\% gross margin 적용 시 최종 가격: \$80-90k
\item 비교: 72개 1.6T DR8 transceiver: \$72k (margin 포함)
\end{itemize}

\textbf{핵심 문제}:
\begin{itemize}
\item CPO component가 switch에 통합되면서 모든 component가 switch vendor의 margin 적용 대상
\item Transceiver는 별도 구매 시 경쟁 시장에서 가격 협상 가능
\item CPO는 vendor lock-in 심화 → 협상력 상실
\item Fiber shuffle 등 추가 component도 margin stack 적용
\end{itemize}

\textbf{결론}: CPO의 기술적 비용 절감이 vendor margin과 협상력 상실로 희석됩니다. 실제 TCO 절감은 기대보다 작을 수 있으며, 이는 scale-out에서 빠른 채택이 어려운 중요한 경제적 이유입니다.
\end{tcolorbox}

그러나 이것은 switch vendor의 margin을 고려하지 않은 것이다. 
60\% gross margin을 적용한다면, 최종 구매자에게 optical engine 비용은 \$80k-\$90k USD가 되어 transceiver에 상응하는 것보다 더 높은 비용이 된다. 
Fiber shuffle과 같은 다른 component들도 이러한 margin stack의 대상이 될 것이다. 
이는 transceiver에 지불하는 비용과 switch vendor가 취하는 margin에 따라 CPO로 전환할 때 비용 절감이 극적이지 않을 수 있는 이유를 설명한다.

아래 표에서 볼 수 있듯이, 3-layer network에서 transceiver에서 CPO로 전환할 때, 
CPO component에 추가로 취해지는 margin이 switch 비용을 81\% 증가시켜 transceiver를 구매하지 않음으로써 얻는 86\% 절감에서 차감된다. 
총 networking 비용은 여전히 DSP transceiver를 사용하는 것보다 CPO에서 31\% 낮지만, 
전력의 경우와 마찬가지로 server rack이 cluster TCO에서 차지하는 지배적인 점유율은 전체 cluster 비용이 단지 3\%만 감소함을 의미한다.

3-layer 대신 2-layer로 network를 평탄화하면 더 많은 비용 절감을 제공할 수 있다. 
최대 7\%의 전체 cluster 비용 감소, transceiver 비용은 86\% 감소, 그리고 전체 networking 비용은 46\% 감소한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/76e3c2a1-8aeb-4787-aca5-c61f160379da_1700x441.png}
\caption{Source: \href{https://semianalysis.com/ai-networking-model/}{SemiAnalysis AI Networking Model}}
\end{figure}
그렇다면 CPO가 한편으로는 최대 7\%의 비용 절감과 최대 4\%의 전력 절감만을 제공하지만, 
다른 한편으로는 어려운 현장 서비스, 신뢰성과 blast radius에 대한 우려 (정당하든 아니든), 그리고 여러 transceiver vendor와 가진 협상력의 상실에 대한 우려를 제기한다면 
왜 GPU cloud에 의해 채택되고 있는가? 
간단한 답은 아직 널리 채택되지 않고 있다는 것이다. 
우리는 가까운 장래에 hyperscaler 내에서 scale-out CPO 시스템에 대한 빠른 채택 곡선을 기대하지 않는다.

\subsection{CPO for Scale-up Networks}

\begin{tcolorbox}[colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: Scale-up이 CPO의 Killer Application인 이유}]
\textbf{Scale-out vs Scale-up 비교:}
\begin{itemize}
\item \textbf{Scale-out}: 서버 간 통신 (GPU당 800Gbit/s), 전체 클러스터 전력의 9\%만 차지 → CPO 효과 희석
\item \textbf{Scale-up}: 동일 시스템 내 GPU 간 통신 (GPU당 7,200Gbit/s - 9배 더 높음), 전력과 비용에서 훨씬 큰 비중 → CPO 효과 극대화
\end{itemize}

Scale-up 네트워크는 더 높은 bandwidth, 더 낮은 latency를 요구하며, copper의 물리적 한계(2m 도달거리)를 극복해야 합니다. CPO는 이러한 문제를 해결하면서 world size(단일 scale-up domain의 GPU 수)를 획기적으로 늘릴 수 있는 유일한 솔루션입니다.
\end{tcolorbox}

반대로, 우리는 scale-up을 위한 CPO를 killer application으로 본다.
앞서 언급했듯이, 주요 hyperscaler는 이미 10년 말까지 CPO 기반 scale-up 솔루션 배포를 위해 공급업체와 약속을 하고 있다.
현재 기존의 copper 기반 scale-up paradigm은 copper cable의 제한된 도달 거리 (lane당 200Gbit/s로 실행될 때 기껏해야 2 meter) 및 lane당 bandwidth를 두 배로 늘리는 것의 증가하는 어려움으로 인해 한계에 도달하고 있다. 
CPO는 bandwidth density 요구사항을 충족하고, 미래에도 bandwidth를 확장하기 위한 여러 방법을 제공하며, 
훨씬 더 큰 scale-up world size를 가능하게 함으로써 이러한 문제를 해결할 수 있다.

CPO가 scale-up networking에 배포되면, scale-up domain은 더 이상 interconnect 도달 거리에 의해 제한되지 않을 것이다. 
원칙적으로 고객은 scale-up domain을 임의로 큰 크기로 성장시킬 수 있을 것이다. 
물론 scale-up domain을 all-to-all connection을 허용하는 single-tier fan-out network 내에 유지하고 싶다면, scale-up domain 크기는 switch radix에 의해 제한될 것이다.

\subsubsection{Scale-out vs Scale-up TAM}
Scale-up fabric의 networking 요구사항은 back-end scale-out network의 요구사항보다 훨씬 더 까다롭다. 
GPU 간 또는 switch link는 GPU가 상호 연결되어 memory와 같은 resource를 coherently하게 공유할 수 있도록 훨씬 더 높은 bandwidth와 더 낮은 latency를 필요로 한다.

예를 들어, NVIDIA Blackwell의 5세대 NVLink는 GPU당 900GB/s의 uni-directional bandwidth를 제공한다. 

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: NVLink, NIC, SerDes}]
\textbf{NVLink}: NVIDIA의 독점 GPU 간 interconnect 기술
\begin{itemize}
\item GPU 간 고속 직접 통신 제공
\item Blackwell (5세대): GPU당 900GB/s
\item Scale-up 네트워크에 사용되며, memory coherency 지원
\item Scale-out (100GB/s)보다 9배 높은 bandwidth
\end{itemize}

\textbf{NIC (Network Interface Card)}: 
\begin{itemize}
\item GPU를 scale-out network에 연결하는 어댑터
\item CX-8: GB300 NVL72에 사용, GPU당 100GB/s 제공
\item RDMA를 통한 낮은 latency 통신 지원
\end{itemize}

\textbf{SerDes (Serializer/Deserializer)}:
\begin{itemize}
\item 병렬 데이터를 직렬로 변환하여 고속 전송
\item Line speed: SerDes가 동작하는 속도 (lane당 200Gbit/s, 224Gbit/s 등)
\item Higher line speed → 더 높은 bandwidth density, 하지만 signal integrity 도전
\end{itemize}
\end{tcolorbox}

이는 back-end scale-out network (GB300 NVL72용 CX-8 \textbf{Network Interface Card (NIC)} 사용)에서 GPU당 100GB/s보다 GPU당 9배 더 많은 bandwidth이다. 이는 또한 host로부터 훨씬 더 높은 shoreline bandwidth density에 대한 필요성을 만들어내며, 이는 GPU SerDes line speed를 앞으로 밀어붙이는 원동력이 되어왔다.

Scale-up domain의 크기가 증가하고 scale-up interconnect의 속도도 증가함에 따라, scale-up interconnect (그리고 결국 scale-up CPO)의 TAM이 이미 scale-out networking의 TAM을 상당히 압도했다는 것을 인식하는 것이 중요하다. CPO TAM은 scale-out networking application보다는 scale-up에 의해 지배될 가능성이 높다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/bc3bd0c3-4dd9-491e-a093-5f518d75ad09_1024x732.jpg}
\caption{Source: SemiAnalysis}
\end{figure}

\subsubsection{Copper vs Optics for Scale-Up: World Sizes, Density and Reach}
현재 scale-up network는 좋은 이유로 전적으로 \textbf{Copper}에서 실행된다. 
현재 pluggable paradigm에서 optical transceiver로 NVLink bandwidth를 맞추는 것은 비용과 전력 측면에서 엄청나게 비쌀 뿐만 아니라 원치 않는 latency를 도입할 것이다. 
또한 compute tray의 face-plate 공간이 이러한 모든 transceiver를 장착하기에 충분하지 않을 수도 있다. 
Copper는 이러한 low-latency, high-throughput connection에서 탁월하다. 
그러나 위에서 언급했듯이, copper의 제한된 도달 거리는 \textbf{world size (단일 scale-up domain 내에서 연결할 수 있는 GPU 수)}를 제한한다.

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: World Size와 그 중요성}]
\textbf{World Size}: 단일 scale-up domain 내에서 직접 연결될 수 있는 GPU의 총 개수를 의미합니다. 

\textbf{중요성}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 더 큰 world size → 더 정교한 collective communication 가능
\item 추론 기반 모델 스케일링과 test-time compute에서 핵심적
\item GB200이 8 GPU → 72 GPU로 확장하여 획기적인 성능 향상 달성
\item Copper 한계: 2m 도달거리로 인해 단일 rack 내로 제한 (144 GPU까지)
\item CPO로 여러 rack에 걸친 확장 가능 → world size를 수천 GPU로 증가
\end{itemize}

\vspace{0.15cm}

\textbf{Copper Cable의 2m 도달거리 제한 이유}:

고속 electrical signal (112 Gbps SerDes)이 copper cable을 통과할 때:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Signal attenuation (신호 감쇠)}: Copper의 저항으로 인해 거리에 따라 신호 세기가 지수적으로 감소. 고주파수 신호일수록 감쇠가 급격히 증가하며, 112 Gbps에서는 2m 이상에서 신호가 noise level 이하로 떨어짐
\item \textbf{Skin effect}: 고주파수에서 전류가 도체 표면에만 흐르면서 유효 단면적 감소 → 저항 증가 → 더 큰 신호 손실
\item \textbf{Dispersion (분산)}: 서로 다른 주파수 성분이 다른 속도로 전파되어 신호 파형이 왜곡되고 bit error rate 급증
\item \textbf{Crosstalk (누화)}: 인접한 copper wire 간 전자기 간섭으로 신호 무결성 저하. 밀집된 cable bundle에서 특히 심각
\item \textbf{Impedance mismatch}: Cable, connector, PCB 간 impedance 불일치로 signal reflection 발생 → 신호 품질 저하
\end{itemize}

\textbf{대조}: Optical fiber는 빛으로 신호를 전송하므로 전자기 간섭, 저항, skin effect 등의 문제가 없어 수 km까지도 신호 무결성을 유지할 수 있습니다. 이것이 long-reach 연결에 optics가 필수적인 물리적 이유입니다.

World size 증가는 inference workload에서 더 많은 compute, memory capacity, memory bandwidth를 제공하여 모델 성능을 극적으로 향상시킵니다.
\end{tcolorbox}

Scale-up world size를 증가시키는 것은 compute scaling의 매우 중요한 방법이다. 
단일 scale-up domain에서 더 많은 compute, memory capacity 및 memory bandwidth를 추가하는 것은 inference 기반 model scaling 및 test time compute의 오늘날 체제에서 점점 더 중요해지고 있다.
NVIDIA의 GB200 시스템은 world size를 단지 8개의 상호 연결된 GPU에서 all-to-all topology로 72개의 상호 연결된 GPU로 가져왔기 때문에 엄청난 성능 향상을 제공했다. 
그 결과 scale-out network에서는 실현 가능하지 않은 더 정교한 collective communication 기술을 구현함으로써 엄청난 throughput 향상이 가능해졌다.

Copper에서 이것은 단일 rack의 footprint 내에서만 수행될 수 있어 power delivery, thermal management 및 manufacturability에 대한 엄청난 수요를 만들어냈다. 
이 시스템의 복잡성으로 인해 downstream supply chain은 여전히 capacity를 늘리는 데 어려움을 겪고 있다.

NVIDIA는 계속해서 copper를 고집할 것이다. 
그들은 또한 자체 scale-up network로 따라잡고 있는 AMD 및 hyperscaler와 같은 경쟁자보다 앞서 나가기 위해 scale-up world size를 더욱 높게 밀어붙여야 한다. 
따라서 NVIDIA는 단일 rack 내에서 scale-up domain을 확장하기 위해 극단적인 조치를 취해야 한다. 
GTC 2025에서 보여진 Rubin Ultra를 위한 NVIDIA의 극단적인 Kyber rack architecture는 144 GPU package (576 GPU die)까지 확장할 수 있다. 
이 rack은 이미 밀도가 높은 GB200/300 NVL72 rack보다 4배 더 밀도가 높다. 
GB200이 이미 제조 및 배포하기에 매우 복잡한 상황에서, Kyber는 이를 다음 단계로 끌어올린다.

Optics는 반대 접근 방식을 가능하게 하며, power delivery 및 thermal density에 어려움이 있는 밀도 높은 footprint에 더 많은 accelerator를 넣는 대신 여러 rack에 걸쳐 확장하여 world size를 증가시킨다. 
이것은 오늘날 pluggable transceiver로 가능하지만, 다시 optical transceiver의 비용과 높은 전력 소비가 이를 비실용적으로 만든다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/64966758-e7f6-4399-8659-1a841ddee5c2_1024x358.jpg}
\caption{Source: SemiAnalysis}
\end{figure}

\subsubsection{Copper vs Optics for Scale-Up: Scaling Bandwidth}
Copper에서 bandwidth를 확장하는 것도 점점 더 어려워지고 있다. 
Rubin을 통해 NVIDIA는 novel bi-directional SerDes 기술을 구현하여 bandwidth의 두 배를 달성하고 있으며, 
여기서 transmit와 receive operation이 동일한 channel을 공유하여 channel당 224Gbit/s transmit + 224Gbit/s receive로 full-duplex communication을 가능하게 한다. 
Copper에서 lane당 \textbf{진정한} 448G를 달성하는 것은 불확실한 시장 출시 시기를 가진 또 다른 도전적인 업적으로 남아 있다. 

\begin{tcolorbox}[colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: Copper vs Optics - Bandwidth Scaling}]
\textbf{Copper의 한계}:
\begin{itemize}
\item Lane당 200Gbit/s에서 signal integrity 문제
\item Bi-directional SerDes로 224Gbit/s Tx + 224Gbit/s Rx 달성 (Rubin)
\item 진정한 448Gbit/s 달성은 불확실한 timeline
\item 도달거리 2m로 제한 (200Gbit/s lane 기준)
\item Bandwidth 증가 방법이 제한적
\end{itemize}

\textbf{CPO/Optics의 Bandwidth Scaling 방법}:
\begin{itemize}
\item \textbf{Baud Rate 증가}: Symbol rate를 높여 bandwidth 증가
\item \textbf{DWDM (Dense Wavelength Division Multiplexing)}: 단일 fiber에 여러 파장 사용
\item \textbf{Additional Fiber Pair}: 더 많은 fiber로 병렬 전송
\item \textbf{Advanced Modulation}: PAM4, QAM 등으로 bit/symbol 증가
\end{itemize}

Optics는 copper보다 훨씬 더 많은 bandwidth scaling 경로를 제공하여 미래의 요구사항을 충족할 수 있습니다.
\end{tcolorbox}

이와 대조적으로, CPO는 bandwidth를 확장하기 위한 여러 방법을 제시한다: Baud rate, DWDM, Additional Fiber Pair 및 Modulation 
이 모든 것은 이 문서의 후반부에서 자세히 논의될 것이다.

\subsection{When will CPO be ready for Primetime?}

그렇다면, CPO가 솔루션이라면 왜 NVIDIA는 Rubin Ultra에 대해 이것을 추구하지 않고 먼저 scale-out switch에 대해서만 추구하는가? 
이것은 supply chain 미성숙, manufacturing 과제 및 배포에 대한 고객 주저로 돌아간다. 
Quantum 및 Spectrum CPO switch는 supply chain을 늘리고 datacenter에서 신뢰성과 serviceability에 대한 더 많은 실제 데이터를 얻는 데 도움을 주기 위해 도입되었다.

\textbf{그 사이에, ECOC 주변에 발표된 Meta의 CPO 신뢰성 데이터는 몇 가지 유용한 정보를 제공한다.} 
Meta는 이 연구를 위해 Broadcom과 협력했으며, Broadcom도 \href{https://www.ecocexhibition.com/wp-content/uploads/Tues-1300-R.Pancholy-R1.1.pdf}{유용한 슬라이드를 게시}했다. 
이 연구에서 Meta는 15개의 Bailly 51.2T CPO Switch에 걸쳐 최대 1,049k 400G port device hour에 걸친 상당한 규모의 테스트 실행을 수행하고 최대 non-zero KP4 
\textbf{Forward Error Correction (FEC)} bin을 발표했다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/85f964f6-9158-4dd0-86cd-ffce9668a28b_2975x2067.jpg}
\caption{Source: Meta}
\end{figure}
논문은 또한 테스트 기간 동안 link에서 failure 또는 \textbf{Uncorrectable Code Word (UCW)}가 관찰되지 않았으며, 
전체 테스트 기간에 걸쳐 최대 1,049k 400G port device hour까지 FEC bin > 10의 단 하나의 사례만 관찰되었다고 설명했다.

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: 신뢰성 측정 지표}]
\textbf{FEC (Forward Error Correction)}: 데이터 전송 중 발생하는 오류를 자동으로 수정하는 기술. FEC bin은 오류 수준을 나타내며, bin > 10은 높은 오류율을 의미합니다.

\textbf{UCW (Uncorrectable Code Word)}: FEC로도 수정할 수 없는 심각한 오류로, 데이터 손실을 초래합니다. UCW 발생 시 link failure로 이어질 수 있습니다.

\textbf{MTBF (Mean Time Before Failure)}: 평균 고장 간격 시간으로, 시스템의 신뢰성을 나타내는 핵심 지표입니다.
\begin{itemize}
\item 기존 2xFR4 transceiver: 0.5-1M device hour MTBF
\item CPO 시스템: 2.6M device hour MTBF (2.6-5배 향상)
\end{itemize}

Meta의 테스트 결과는 CPO가 기존 트랜시버보다 더 높은 신뢰성을 가질 수 있음을 시사하지만, 대규모 production 환경에서의 검증이 여전히 필요합니다.
\end{tcolorbox}

그러나 Meta는 거기서 멈추지 않았다. 
ECOC에서 동일한 논문을 발표하는 강연에서, 그들은 최대 15M 400G port-device hour에 대한 확장된 결과를 발표했다. 
이러한 결과는 첫 4M 400G port device hour 동안 UCW가 없었음을 보여주었으며, 
그들은 또한 400G 2xFR4 transceiver (전 세계적으로 2xFR4의 경우 550k)에 대해 0.5-1M device hour \textbf{Mean Time Before Failure (MTBF)} vs CPO에 대해 2.6M device hour MTBF를 보여주었다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/200a6678-6062-4b81-a7f2-0c7246a4f8c2_2746x2067.jpg}
\caption{Source: Meta}
\end{figure}
15M port device hour가 큰 숫자처럼 들릴 수 있지만, 이것은 400G port hour 단위이다. 
그래서 하나의 51.2T switch를 1 hour 동안 작동시키면 128 400G port hour를 의미한다. 
15개의 51.2T switch에 걸쳐 15M 400G Port hour는 7,812 wall-clock hour 또는 약 325일을 의미한다. 
실제로 이 15M hour 숫자는 종종 단순히 \textbf{hour} 또는 \textbf{device hour}로 인용되며 \textbf{port} 부분이 생략된다. 
4M port device hour까지 zero failure 및 zero UCW 통계는 매우 유용하지만, 
산업계가 CPO scale-out switching으로 전환하고 이 기술에 수십억 달러를 투입하기 전에 lab 환경에서 11개월 동안 테스트된 단지 15개의 CPO switch보다 훨씬 더 많은 것이 필요하다.

동적인 현장 환경에서 수천 개의 scale-out switch를 작동하는 것은 전혀 다른 도전이며, 이러한 switch가 production 환경에서 어떻게 작동할지는 아직 두고 봐야 한다. 
Production 환경에서의 temperature 변동은 lab보다 높을 수 있으며, 
이는 component 성능이나 endurance에서 예상치 못한 변동을 초래할 수 있다. 
\href{https://arxiv.org/pdf/2407.21783}{Meta의 자체 Llama 3 논문은 datacenter에서 1-2\% temperature 변동을 인용했으며} 이는 전력 소비 변동에 악영향을 미쳤다.
이러한 변동이 예상하기 어려운 방식으로 전체 network fabric에 영향을 미칠 수 있을까?

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/71c23008-9f18-42cf-b6f3-8d7c47818718_3866x918.jpg}
\caption{Source: \href{https://arxiv.org/pdf/2407.21783}{Meta}}
\end{figure}
Datacenter의 먼지와 같은 평범하게 들리는 문제조차도 fiber 끝을 청소하는 데 상당한 시간을 보낼 수 있는 지원 기술자의 골칫거리이다.
물론 CPO switch는 LC 또는 MPO type front pluggable connector를 가지고 있지만, CPO switch chassis 내부의 먼지는 어떨까? 
0.06\%의 unserviceable failure rate는 매력적으로 들리지만, 그러한 failure는 64 800G port의 blast radius를 가진다. 
이 논문은 또한 FR optics 기반 CPO switch에 초점을 맞추고 있지만, 다음 세대의 CPO switch는 DR optics를 기반으로 할 것이다. 
이것들은 단지 몇 가지 알려진 미지수이며, field testing에서 나올 수 있는 더 많은 알려지지 않은 미지수가 잠재적으로 있다.

실제로 이러한 결과는 tangible한 신뢰성 데이터를 제공함으로써 업계의 사람들을 설득하는 측면에서 영향력이 있었다. 
여기서 우리의 요점은 \textbf{Fear, Uncertainty 또는 Doubt (FUD)}를 만들기 위한 것이 아니라, 
업계가 예상치 못한 문제를 신속하게 이해하고 해결하여 더 광범위한 CPO 채택, 
특히 scale-up networking을 위한 길을 닦을 수 있도록 훨씬 더 대규모의 field testing을 요구하기 위한 것이다.
결국, NVIDIA의 scale-out CPO 제품 출시는 실제 high-volume 배포를 위한 practice run 및 pipe-cleaner 역할을 하고 있다. 
우리는 scale-out vs scale-up에 대한 훨씬 더 설득력 있는 TCO 및 Performance/TCO 이점을 고려할 때 scale-up에 대한 배포가 훨씬 더 크고 영향력이 있을 것이라고 생각한다.

더욱이 scale-out CPO와 관련하여, Rubin Ultra는 2027년 출시를 목표로 하고 있지만 (우리는 그것이 결국 2027년 후반으로 예상함) supply chain은 GPU 수요를 지원하기 위해 수천만 개의 이러한 CPO endpoint를 출하할 준비가 되지 않을 것이다. 
이 timeline조차 NVIDIA에게는 너무 야심적이다. 이것이 Feynman 세대가 NVIDIA ecosystem에 CPO injection의 초점이 되는 것처럼 보이는 이유이다.

\begin{tcolorbox}[breakable,colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: CPO 배포 전략 - Scale-out이 먼저인 이유}]
\setlength{\parskip}{2pt}

\textbf{핵심 질문}: CPO가 그렇게 좋다면, 왜 NVIDIA는 Rubin Ultra GPU (scale-up)에 먼저 적용하지 않고 datacenter switch (scale-out)부터 시작하는가?

\vspace{0.15cm}

\textbf{1. 물량의 차이 (Scale 문제)}

\textit{Scale-out Switch (먼저 시작)}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{필요 수량}: 대형 datacenter 하나당 수천 개의 switch
\item \textbf{CPO endpoint 수}: 수천 \textasciitilde{} 수만 개 수준
\item \textbf{공급망 부담}: 상대적으로 관리 가능
\item \textbf{실패 시 영향}: 제한적 - 일부 고객의 일부 switch만 영향
\end{itemize}

\textit{Scale-up GPU (진짜 목표, 나중)}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{필요 수량}: 전 세계 모든 AI datacenter의 모든 GPU
\item \textbf{CPO endpoint 수}: \textbf{수천만 \textasciitilde{} 수억 개} 수준 (Rubin Ultra 기준)
\item \textbf{공급망 부담}: 엄청난 manufacturing capacity 필요
\item \textbf{실패 시 영향}: 치명적 - 전체 GPU 출하 지연 또는 중단
\end{itemize}

\textbf{구체적 예시}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 200K GPU cluster: \textasciitilde{}3,000개의 switch 필요 (scale-out)
\item 동일 cluster: 200,000개의 GPU 각각 CPO 필요 (scale-up)
\item \textbf{비율}: GPU가 switch보다 60배 이상 많음!
\end{itemize}

\vspace{0.15cm}

\textbf{2. "Pipe Cleaner" 전략 (공급망 준비)}

\textit{Pipe cleaner란?} 파이프의 막힌 부분을 먼저 청소하는 도구처럼, scale-out CPO가 supply chain의 문제점을 먼저 발견하고 해결하는 역할을 합니다.

\textbf{Supply chain 단계적 확장}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{2025-2026}: 수천 개 switch로 시작 → 공급업체들이 소량 생산 경험 축적
\item \textbf{2026-2027}: 수만 개 수준으로 확대 → Manufacturing process 최적화
\item \textbf{2027-2028}: 수천만 개 GPU용으로 대규모 확장 → 완전한 mass production
\end{itemize}

\vspace{0.15cm}

\textbf{3. Real-World Validation (현장 검증)}

\textit{Lab vs Production의 차이}:

\textbf{Meta의 Lab 테스트} (2024-2025):
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 15개의 CPO switch
\item 325일 동안 테스트
\item 통제된 환경: 일정한 온도, 깨끗한 공기, 안정적인 전력
\item \textbf{결과}: 매우 좋음 (zero failures)
\end{itemize}

\textbf{Production 환경} (2026-2028):
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 수천 개의 CPO switch가 여러 datacenter에 분산
\item 수년간 24/7 가동
\item \textbf{예상치 못한 도전}:
  \begin{itemize}[itemsep=0pt]
  \item 온도 변동 (Meta 논문: 1-2\% 변동이 전력 소비에 영향)
  \item 먼지와 오염 물질 (fiber 끝 청소 필요)
  \item 습도, 진동, 전력 불안정성
  \item 다양한 운영 팀의 실수와 예상 외 사용 패턴
  \end{itemize}
\item \textbf{핵심 데이터 수집}:
  \begin{itemize}[itemsep=0pt]
  \item Failure rate와 failure mode
  \item Blast radius (한 switch 고장 시 영향 범위: 64 ports)
  \item Serviceability (수리 가능성과 소요 시간)
  \item 실제 MTBF (Mean Time Before Failure)
  \end{itemize}
\end{itemize}

\vspace{0.15cm}

\textbf{4. Timeline과 현실}

\begin{tabular}{|l|l|l|}
\hline
\textbf{시기} & \textbf{Scale-out (Switch)} & \textbf{Scale-up (GPU)} \\
\hline
2025-2026 & 제한적 상용화 시작 & 아직 없음 \\
 & (수백 \textasciitilde{} 수천 개) & \\
\hline
2027 & 점진적 확대 & Rubin Ultra 출시 예정 \\
 & (수만 개) & (하지만 CPO 없이) \\
\hline
2028+ & 대규모 배포 & Feynman 세대에서 \\
 & & CPO 본격 적용 예상 \\
\hline
\end{tabular}

\vspace{0.1cm}

\textbf{왜 Rubin Ultra (2027)도 CPO가 없는가?}
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 2027년까지 supply chain이 수천만 개 CPO를 생산할 준비 안 됨
\item Scale-out의 field data가 충분히 축적되지 않음
\item 너무 야심적인 timeline → 리스크가 너무 큼
\item \textbf{보수적 접근}: Feynman (2028-2029)까지 기다려서 확실하게 적용
\end{itemize}

\vspace{0.15cm}

\textbf{5. 경제적/전략적 이유}

\textbf{TCO (Total Cost of Ownership) 이점}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item Scale-out CPO: 23\% networking 전력 절감 (좋지만 제한적)
\item \textbf{Scale-up CPO}: 훨씬 더 큰 이점 (copper 제거, world size 증가)
\item 진짜 큰 돈은 GPU에서 나옴 → GPU CPO가 진짜 목표
\end{itemize}

\vspace{0.15cm}

\textbf{최종 목표}는 scale-up CPO입니다. 하지만 그곳에 도달하려면 scale-out으로 먼저 길을 닦아야 합니다. 이는 기술적 필연성이자 비즈니스적 현명함입니다.

\end{tcolorbox}

이제 CPO가 무엇인지, 기술적 고려사항, 과제 및 오늘날 ecosystem의 상태에 대해 심도 있게 이야기해보자.

\newpage{}

\section{Part 2: CPO Introduction and Implementation}

\subsection{What is CPO about and why is everyone so excited?}
\textbf{CPO}는 optical engine을 고성능 computing 또는 networking ASIC과 동일한 package 또는 module 내에 직접 통합한다. 
이러한 optical engine은 electrical signal을 optical signal로 변환하여 optical link를 통한 고속 데이터 전송을 가능하게 한다. 
Optical link는 몇 meter 이상의 거리에서 데이터 통신을 위해 사용되어야 하는데, copper를 통한 고속 electrical communication은 몇 meter 이상으로 도달할 수 없기 때문이다.

오늘날 대부분의 electrical에서 optical로의 변환은 pluggable optical transceiver를 통해 발생한다. 
이러한 경우, electrical signal은 switch 또는 processing chip에서 \textbf{Printed Circuit Board (PCB)}를 통해 chassis의 front plate 또는 back plate에 있는 물리적 transceiver cage까지 수십 centimeter 이상을 이동한다. 
Pluggable optical transceiver는 그 cage에 위치한다. 
Transceiver는 optical \textbf{Digital Signal Processor (DSP)} chip에 의해 재조정되는 electrical signal을 수신한 다음 electrical signal을 optical signal로 변환하는 optical engine component로 전송된다. 
Optical signal은 그런 다음 optical fiber를 통해 link의 반대편으로 전송될 수 있으며, 
여기서 다른 transceiver가 이 과정을 역으로 수행하여 optical signal을 destination silicon까지 다시 electrical signal로 변환한다.

이 과정에서 electrical signal은 optical link에 도달하기 전에 (적어도 copper의 경우) 상대적으로 긴 거리를 여러 transition point를 거쳐 통과한다. 
이는 electrical signal이 저하되게 하며 이를 구동하고 복구하기 위해 많은 전력과 복잡한 회로 (SerDes)가 필요하다. 
이를 개선하려면 electrical signal이 이동해야 하는 거리를 단축해야 한다. 이것이 \textbf{Co-Packaged Optics}라는 아이디어로 이어지는데, 
여기서 pluggable transceiver에 있던 optical engine이 대신 host chip과 co-package된다. 
Optical engine이 XPU 또는 Switch ASIC에 훨씬 더 가깝기 때문에 electrical trace 길이가 수십 centimeter에서 수십 mm로 줄어든다. 
이는 electrical interconnect 거리를 최소화하고 signal integrity 문제를 완화함으로써 전력 소비를 크게 줄이고, bandwidth density를 향상시키며, latency를 낮춘다.
아래 schematic은 CPO 구현을 보여주며, 여기서 optical engine이 compute 또는 switch chip과 동일한 package에 위치한다. 
Optical engine은 처음에는 substrate에 있을 것이고, 미래에는 OE가 interposer에 배치될 것이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/de4a101e-47e9-4c4e-a773-58bd9011b84a_1362x443.png}
\caption{Source: SemiAnalysis}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/685d1ff0-0003-4a85-a518-b386d4011a3b_1349x442.png}
\caption{Source: SemiAnalysis}
\end{figure}

\begin{tcolorbox}[breakable,colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: CPO의 본질 - Signal Path 혁명}]
\setlength{\parskip}{2pt}

\textbf{1. Traditional Pluggable Optics의 Signal Journey}

\textbf{Electrical signal의 긴 여정}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{출발}: ASIC chip의 SerDes
\item \textbf{경로 1}: Package substrate (1-2 cm)
\item \textbf{경로 2}: PCB trace 또는 flyover cable (15-30 cm)
\item \textbf{경로 3}: Connector 및 transceiver PCB (1-2 cm)
\item \textbf{도착}: Transceiver 내부의 optical engine
\item \textbf{총 거리}: 약 \textbf{20-35 cm}
\end{itemize}

\textbf{이 여정의 대가}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Signal degradation}: 고주파수 성분이 exponentially 감쇠
\item \textbf{LR SerDes 필요}: 50-70 mW/lane의 높은 전력
\item \textbf{DSP chip 필요}: Signal reconditioning에 5-14W
\item \textbf{낮은 bandwidth density}: 많은 회로가 signal recovery에 사용됨
\end{itemize}

\vspace{0.1cm}

\textbf{2. CPO의 혁명적 변화}

\textbf{Electrical signal의 짧은 여정}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{출발}: ASIC chip의 SerDes 또는 wide I/O
\item \textbf{경로}: Package substrate 또는 interposer (\textbf{2-5 mm})
\item \textbf{도착}: 동일 package 내의 optical engine
\item \textbf{총 거리}: \textbf{2-5 mm}
\end{itemize}

\textbf{거리 단축의 마법}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Signal degradation 최소화}: mm 거리에서는 거의 감쇠 없음
\item \textbf{SR SerDes 또는 wide I/O}: 20-30 mW/lane (50\% 절감)
\item \textbf{DSP 제거}
\item \textbf{높은 bandwidth density}: Signal recovery 회로 불필요
\end{itemize}

\vspace{2cm}

\textbf{3. 구체적 수치 비교}

\begin{tabular}{|l|c|c|c|}
\hline
\textbf{항목} & \textbf{Traditional} & \textbf{CPO} & \textbf{절감률} \\
\hline
Signal path 거리 & 20-35 cm & 2-5 mm & 100배↓ \\
\hline
SerDes 전력/lane & 50-70 mW & 20-30 mW & 50-70\% \\
\hline
DSP 전력 & 5-14W & 0W & 100\% \\
\hline
총 전력 (800G) & 16-17W & 4-5W & 73\% \\
\hline
Bandwidth density & 0.4 Tbps/mm & 1.8-10 Tbps/mm & 5-25배↑ \\
\hline
\end{tabular}

\vspace{0.1cm}

\textbf{4. 왜 거리가 이렇게 중요한가?}

\textbf{물리 법칙}: Insertion loss는 거리와 주파수에 exponentially 비례

\textbf{구체적 예시} (112 Gbps, 56 GHz):
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{5 mm} (CPO): 약 2-3 dB loss → 신호 50-70\% 유지
\item \textbf{10 cm} (short PCB): 약 20-25 dB loss → 신호 1-3\% 유지
\item \textbf{30 cm} (long PCB): 약 40-50 dB loss → 신호 0.01\% 유지
\end{itemize}

\vspace{0.1cm}

5mm에서는 신호가 대부분 유지되므로:
- Minimal equalization (전력 절감)
- DSP 불필요 (전력 절감)
- 낮은 transmit power (전력 절감)

\vspace{0.1cm}

30cm에서는 신호가 거의 완전히 소멸하므로:
- 강력한 equalization 필요 (전력 소비)
- DSP로 signal reconstruction 필요 (전력 소비)
- 높은 transmit power 필요 (전력 소비)

\vspace{0.1cm}

\textbf{핵심 통찰}: CPO의 본질은 "optical engine을 chip에 붙이는 것"이 아니라 "\textbf{electrical signal path를 극단적으로 단축}하는 것"입니다. 

\end{tcolorbox}

오늘날 아래 다이어그램에 설명된 것처럼 front pluggable optics 솔루션이 보편적이다. 이 다이어그램에서 주요 요점은 electrical signal이 transceiver의 optical engine에 도달하기 전에 copper trace 또는 flyover cable을 통해 긴 거리 (15-30cm)를 통과해야 함을 보여주는 것이다. 위에서 논의한 바와 같이, 이는 또한 pluggable module로 구동하기 위한 long-reach (LR) SerDes의 필요성을 필요로 한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/a77dcc21-f043-4a19-8177-7100ba4a98b3_1024x370.png}
\caption{Source: SemiAnalysis}
\end{figure}

\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: SerDes, PCB Trace, Flyover Cable}]
\setlength{\parskip}{2pt}

\textbf{SerDes (Serializer/Deserializer)}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{정의}: 병렬 데이터를 직렬로 변환(전송) 또는 직렬을 병렬로 변환(수신)하는 회로
\item \textbf{필요성}: Chip 내부는 넓은 병렬 bus(예: 256 bits), 외부는 좁은 직렬 lane(예: 1 bit)
\item \textbf{Serializer}: 256 bits를 받아서 1 bit씩 순차적으로 고속 전송
\item \textbf{Deserializer}: 1 bit씩 받아서 256 bits로 재구성
\end{itemize}

\vspace{0.1cm}

\textbf{Long-Reach (LR) SerDes vs Short-Reach (SR) SerDes}:

\begin{tabular}{|l|c|c|}
\hline
\textbf{특성} & \textbf{LR SerDes} & \textbf{SR SerDes} \\
\hline
목표 거리 & 15-30 cm & 2-5 mm \\
\hline
전력 (per lane) & 50-70 mW & 20-30 mW \\
\hline
회로 복잡도 & 매우 높음 & 중간 \\
\hline
Equalization & 강력 (FFE, DFE) & 최소 \\
\hline
사용 예 & Pluggable optics & CPO, chiplet \\
\hline
\end{tabular}

\vspace{0.1cm}

\textbf{왜 LR SerDes가 더 많은 전력을 쓰는가?}
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Pre-emphasis}: 전송 전 고주파수 성분 증폭 (signal degradation 예상)
\item \textbf{Equalization}: 수신 후 주파수 왜곡 보정
\item \textbf{CDR (Clock Data Recovery)}: 저하된 신호에서 clock 추출
\item \textbf{더 높은 swing voltage}: 긴 거리로 인한 loss 보상
\end{itemize}

\vspace{0.1cm}

\textbf{PCB (Printed Circuit Board) Trace}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{정의}: PCB 기판 위/내부에 etching된 copper 경로
\item \textbf{CCL (Copper-Clad Laminate)}: PCB의 기본 재료
  \begin{itemize}[itemsep=0pt]
  \item 절연체(laminate)에 copper를 입힌 구조
  \item 고주파수에서 dielectric loss가 큼
  \end{itemize}
\item \textbf{문제}: 112 Gbps 이상에서는 수 cm만 이동해도 신호 품질 심각하게 저하
\item \textbf{Insertion loss}: 고주파수일수록 exponentially 증가
\end{itemize}

\vspace{0.1cm}

\textbf{Flyover Cable}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{정의}: PCB trace를 우회하여 \textbf{공중}으로 연결하는 고품질 cable
\item \textbf{장점}: 
  \begin{itemize}[itemsep=0pt]
  \item PCB trace보다 훨씬 낮은 insertion loss
  \item 더 나은 shielding으로 crosstalk 감소
  \item 고주파수 신호에 최적화된 재료
  \end{itemize}
\item \textbf{단점}:
  \begin{itemize}[itemsep=0pt]
  \item 여전히 15-30cm 거리 필요 (LR SerDes 필수)
  \item 복잡한 설치 (수작업으로 routing)
  \item 물리적 공간 점유
  \item 높은 비용
  \end{itemize}
\item \textbf{사용처}: High-end server/switch에서 PCB trace 대체
\end{itemize}

\end{tcolorbox}

추가로, CPO와 전통적인 front-pluggable optics 사이에 있는 중간 구현이 있는데, \textbf{Near-Packaged Optics (NPO)} 및 \textbf{On-Board Optics (OBO)}와 같은 것들이다.
최근 몇 년 동안 NPO는 CPO로 가는 중간 단계로 등장했다. NPO는 여러 정의를 가지고 있다. 
NPO는 OE가 ASIC의 substrate에 직접 위치하지 않고 다른 substrate에 co-package되는 경우이다. 
Optical engine은 socketable한 상태로 유지되며 substrate에서 분리될 수 있다. 
Electrical signal은 여전히 XPU package의 SerDes에서 일부 copper channel을 통해 Optical Engine으로 이동한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/00107d33-9adb-475d-bc88-354f7192b2e9_1024x296.png}
\caption{Source: SemiAnalysis}
\end{figure}
OBO는 optical engine을 chassis 내부의 system PCB에 통합하여 host ASIC에 더 가깝게 배치한다. 
그러나 OBO는 CPO의 많은 과제를 상속하면서 bandwidth density 및 전력 절감 측면에서 더 적은 이점을 제공한다. 
우리는 OBO를 \textbf{worst of both worlds}로 보는데, CPO의 복잡성을 결합하면서 front-pluggable optics의 일부 제한을 상속하기 때문이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/faa2a72c-e270-4e1f-a13f-a0fe827c9b66_1024x276.png}
\caption{Source: SemiAnalysis}
\end{figure}

\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: NPO, OBO - CPO로 가는 중간 단계들}]
\setlength{\parskip}{2pt}

\textbf{Optical Integration의 진화 스펙트럼}

\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{특성} & \textbf{Pluggable} & \textbf{OBO} & \textbf{NPO} & \textbf{CPO} \\
\hline
OE 위치 & Cage & PCB & Near pkg & In pkg \\
\hline
Signal 거리 & 15-30cm & 5-10cm & 2-5cm & 2-5mm \\
\hline
Serviceability & 최고 & 중간 & 낮음 & 최저 \\
\hline
전력 효율 & 낮음 & 중간 & 중간-높음 & 최고 \\
\hline
복잡도 & 낮음 & 중간 & 높음 & 최고 \\
\hline
\end{tabular}

\vspace{0.1cm}

\textbf{1. OBO (On-Board Optics)}

\textbf{개념}: Optical engine을 system PCB 위에 직접 배치 (chassis 내부, ASIC package 외부)

\textbf{장점}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item Signal path가 pluggable 대비 짧아짐 (30cm → 5-10cm)
\item 일부 전력 절감 가능
\item Pluggable보다 높은 bandwidth density
\end{itemize}

\textbf{단점}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{"Worst of Both Worlds"}: CPO의 복잡도 + Pluggable의 제한
\item 여전히 LR SerDes 필요 (전력 소비 높음)
\item PCB에 고정되어 serviceability 어려움
\item CPO만큼의 bandwidth density나 전력 효율 달성 못함
\item Thermal management 복잡
\end{itemize}

\textbf{결론}: 이득은 적고 복잡도는 높아서 업계에서 선호하지 않음

\vspace{0.3cm}

\textbf{2. NPO (Near-Packaged Optics)}

\textbf{개념}: Optical engine을 별도 substrate에 배치하되 ASIC package 매우 가까이 위치

\textbf{주요 특징}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item OE가 ASIC substrate에 직접 붙지 않고 \textbf{별도 substrate}에 위치
\item \textbf{Socketable}: OE substrate를 분리/교체 가능 (일부 serviceability 유지)
\item Signal은 여전히 일부 copper channel 통과 (2-5cm)
\item CPO보다 긴 electrical path이지만 pluggable보다는 짧음
\end{itemize}

\textbf{장점}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item CPO로 가는 \textbf{중간 단계}로 supply chain 준비 가능
\item 일부 serviceability 유지 (OE substrate 교체)
\item Pluggable 대비 상당한 전력 절감
\item CPO보다 packaging 복잡도 낮음
\end{itemize}

\textbf{단점}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item CPO만큼의 극적인 전력/공간 절감은 못함
\item 여전히 일부 copper channel 통과로 SerDes 전력 소비
\item 표준화 부족
\end{itemize}

\textbf{시장 포지셔닝}: CPO가 완전히 성숙하기 전까지의 과도기 솔루션

\vspace{0.3cm}

\textbf{3. 왜 CPO가 최종 목표인가?}

\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{물리적 한계}: Electrical signal은 거리가 짧을수록 exponentially하게 좋아짐
\item \textbf{경제성}: 중간 단계들은 "일부 복잡도 + 일부 이득"으로 ROI가 불분명
\item \textbf{표준화}: Pluggable은 표준화됨, CPO도 표준화 진행 중, 하지만 중간 단계들은 표준화 어려움
\end{itemize}

\textbf{핵심 통찰}: OBO와 NPO는 CPO로 가는 징검다리 역할을 할 수 있지만, 업계는 대부분 "조금 나은 pluggable"보다 "확실히 다른 CPO"를 기다리는 쪽을 선택하고 있습니다. 

\end{tcolorbox}

\subsection{Co-packaged Copper}
CPO의 또 다른 대안은 \textbf{Co-Packaged Copper (CPC)}이다. 
CPC는 substrate의 connector에서 직접 나오는 copper cabling을 사용한다. 
CPC에 사용되는 cable은 flyover와 동일한 cable이며 동일한 목적 즉, PCB trace를 우회하는 것이다. 
CPC는 flyover cable을 더 나아가게 하여 socket이 package substrate 자체에서 시작한다. 
사용되는 cable은 cross-talk을 줄이기 위해 잘 절연된 twin-axial cable (Twinax cable)이며, 
기존 electrical trace와 비교하여 insertion loss가 상당히 낮다. 이 솔루션은 여전히 copper를 사용하지만 signal integrity에서 핵심 이점을 제공한다. 
CPC는 448G SerDes를 배포하여 off-package interconnect의 또 다른 scaling을 허용하는 실용적인 경로를 제공할 수 있다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/c1921fab-3219-4a40-991a-fba7bdab0126_2533x1272.jpg}
\caption{Source: LuxShare}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/250f37ae-3fdf-4d94-9d40-1c1261dbf011_750x458.png}
\caption{Source: Luxshare}
\end{figure}
CPC의 과제는 package substrate의 추가된 복잡성에 있다. 
Substrate는 수천 개의 이러한 cable에 전력과 신호를 routing해야 한다.
이러한 과제에도 불구하고 CPC는 여전히 supply chain의 여러 부분에서 많은 manufacturing 장애물을 극복해야 하는 CPO보다 훨씬 더 간단하다. 
우리는 CPC를 in-rack scale-up connectivity와 같은 일부 short-reach application에 특히 매력적인 것으로 보며, 이는 아래에서 살펴볼 것이다. 
손실이 많은 CCL trace를 우회함으로써, CPC는 448G line speed를 가능하게 하는 기술이 될 수 있다. 
CPC는 또한 448G를 가능하게 하기 위해 매우 광범위하게 검토되고 있는데, 이 bandwidth의 signal은 PCB를 통해 실행될 때 받아들일 수 없는 attenuation을 경험하기 때문이다.




\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: Package Substrate, Twinax Cable, Cross-talk}]
\setlength{\parskip}{2pt}

\textbf{1. Package Substrate}

\textbf{정의}: ASIC chip과 PCB 사이의 중간층으로, chip의 미세한 bump를 PCB의 큰 solder ball로 연결하는 다층 구조

\textbf{역할}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Fan-out}: Chip의 좁은 bump pitch (예: 100 μm) → PCB의 넓은 ball pitch (예: 1 mm)로 확장
\item \textbf{Signal redistribution}: Chip의 I/O를 적절한 위치로 재배치
\item \textbf{Power delivery}: 깨끗한 전원을 chip에 공급 (decoupling capacitor 내장)
\item \textbf{Thermal management}: 열을 PCB로 전달
\end{itemize}

\textbf{재료와 특성}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Organic substrate}: FR-4 기반, 저렴하지만 electrical 성능 제한적
\item \textbf{HDI substrate}: High-Density Interconnect, 미세 pitch 지원
\item \textbf{Advanced substrate}: ABF (Ajinomoto Build-up Film), 고주파수 특성 우수
\item 일반적으로 4-12 metal layer
\end{itemize}

\textbf{CPC에서의 역할}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item Package substrate 표면에 \textbf{cable connector} 직접 장착
\item PCB trace를 완전히 우회 → Signal degradation 최소화
\item 가장 짧은 electrical path (chip → substrate → cable)
\end{itemize}

\vspace{0.1cm}

\textbf{2. Twinax Cable (Twin-Axial Cable)}

\textbf{정의}: 두 개의 copper conductor가 함께 차폐(shielding)된 고성능 cable

\textbf{구조} (안쪽에서 바깥쪽으로):
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{두 개의 conductor}: 신호를 differential pair로 전송
  \begin{itemize}[itemsep=0pt]
  \item Signal+ 와 Signal- 가 함께 전송
  \item Differential signaling으로 noise immunity 향상
  \end{itemize}
\item \textbf{절연체 (dielectric)}: 두 conductor 사이와 주변을 절연
\item \textbf{Shield (차폐)}: Aluminum foil 또는 braided copper
  \begin{itemize}[itemsep=0pt]
  \item 외부 electromagnetic interference (EMI) 차단
  \item Cross-talk 방지
  \end{itemize}
\item \textbf{Jacket (외피)}: 기계적 보호
\end{itemize}

\textbf{Twinax vs Coaxial}:

\begin{tabular}{|l|c|c|}
\hline
\textbf{특성} & \textbf{Coaxial} & \textbf{Twinax} \\
\hline
Conductor 수 & 1개 (중심) & 2개 (parallel) \\
\hline
Signaling & Single-ended & Differential \\
\hline
Noise immunity & 중간 & 높음 \\
\hline
용도 & RF, video & High-speed data \\
\hline
\end{tabular}

\textbf{Twinax의 장점}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{낮은 insertion loss}: PCB trace 대비 10배 이상 낮음
\item \textbf{우수한 signal integrity}: 차폐로 external noise 차단
\item \textbf{낮은 cross-talk}: 차폐로 인접 cable 간 간섭 최소
\item \textbf{고주파수 지원}: 448G SerDes 가능 (2m 이내)
\item \textbf{Flexible}: 손쉬운 routing
\end{itemize}

\textbf{단점}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item PCB trace 대비 높은 비용
\item 물리적 공간 점유
\item 거리 제한 (copper 물리 한계)
\end{itemize}

\vspace{0.1cm}

\textbf{3. Cross-talk (누화)}

\textbf{정의}: 인접한 신호선 간에 발생하는 원치 않는 전자기 간섭 (electromagnetic coupling)

\textbf{발생 원리}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Capacitive coupling}: 인접 conductor 간 기생 capacitance
  \begin{itemize}[itemsep=0pt]
  \item 고주파수 신호가 AC coupling으로 누설
  \item 거리가 가까울수록 심함
  \end{itemize}
\item \textbf{Inductive coupling}: 전류가 흐르면 자기장 생성
  \begin{itemize}[itemsep=0pt]
  \item 인접 conductor에 유도 전류 발생
  \item 전류가 클수록 심함
  \end{itemize}
\end{itemize}

\textbf{Cross-talk의 영향}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Signal integrity 저하}: 원래 신호에 noise 추가
\item \textbf{Bit error rate (BER) 증가}: 인접 bit 간섭
\item \textbf{Timing margin 감소}: Signal eye diagram 닫힘
\item 고속 신호(224G+)에서 특히 치명적
\end{itemize}

\textbf{PCB Trace에서 Cross-talk가 심한 이유}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 수백-수천 개의 trace가 좁은 공간에 밀집
\item Trace 간 거리: 수십-수백 μm (매우 가까움)
\item 차폐 없음 → Electromagnetic field가 자유롭게 전파
\item 고주파수 신호 → Coupling 증가
\end{itemize}

\textbf{Twinax Cable이 Cross-talk를 막는 방법}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Shield (차폐)}: 
  \begin{itemize}[itemsep=0pt]
  \item Electromagnetic field를 shield 내부에 가둠
  \item 외부로 field 방출 차단
  \item 외부 field 침입 차단
  \end{itemize}
\item \textbf{Differential signaling}: 
  \begin{itemize}[itemsep=0pt]
  \item Signal+ 와 Signal- 가 동일한 noise를 받음
  \item Receiver가 차이만 측정 → Noise 상쇄 (common-mode rejection)
  \end{itemize}
\item \textbf{Physical separation}: Cable 간 물리적 간격 유지 가능
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[breakable,colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: CPC - Copper의 마지막 진화}]
\setlength{\parskip}{2pt}

\textbf{핵심 질문}: CPO가 미래라면, 왜 CPC(Co-Packaged Copper)도 논의되는가?

\vspace{0.1cm}

\textbf{1. CPC의 전략}

\textbf{Traditional copper의 문제 진단}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item ASIC → PCB trace (손실 많은 CCL) → Connector → Cable
\item \textbf{주범}: PCB trace의 높은 insertion loss
\item 112G 이상에서 수 cm만 가도 신호 품질 급격히 저하
\end{itemize}

\textbf{CPC의 해법}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item ASIC → Package substrate → \textbf{직접 twinax cable 연결}
\item \textbf{PCB trace 완전 우회!}
\item Flyover cable과 유사하지만 시작점이 package substrate
\end{itemize}

\vspace{0.1cm}

\textbf{2. CPC의 장점과 과제}

\textbf{장점}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{448G SerDes 가능}: PCB trace 우회로 448G line speed 달성
\item \textbf{CPO보다 훨씬 간단}: Optical engine, laser, fiber coupling 모두 불필요
\item \textbf{Short-reach에 최적}: In-rack scale-up connectivity (1-2m)
\item \textbf{Lower cost}: CPO 대비 단순한 구조
\item \textbf{Proven technology}: Copper cable은 성숙한 기술
\end{itemize}

\textbf{과제}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Substrate 복잡도}: 수천 개의 cable connector를 substrate에 배치
\item \textbf{Power/Signal routing}: 수천 개 cable에 전력과 신호 routing
\item \textbf{기계적 스트레스}: 많은 cable이 package에 연결되어 물리적 부담
\item \textbf{거리 제한}: 여전히 copper 물리 한계 (2m)
\end{itemize}

\vspace{0.1cm}

\textbf{3. CPC vs CPO: 언제 무엇을 쓸 것인가?}

\textbf{CPC가 유리한 경우}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{In-rack scale-up}: 같은 rack 내 GPU 간 연결 (1-2m)
  \begin{itemize}[itemsep=0pt]
  \item 예: GB200 NVL72, Kyber rack
  \item 모든 GPU가 하나의 rack에 밀집
  \item 2m 이내 거리면 충분
  \end{itemize}
\item \textbf{448G adoption}: CPO가 성숙하기 전 448G를 구현하는 practical path
\item \textbf{Cost-sensitive applications}: CPO의 높은 비용 부담스러운 경우
\end{itemize}

\textbf{CPO가 필수인 경우}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Multi-rack scale-up}: Rack 간 연결 (2m 이상)
\item \textbf{Scale-out networking}: Datacenter switch fabric
\item \textbf{극한의 bandwidth density}: 물리적 공간 제약이 심한 경우
\item \textbf{최고 전력 효율}: 전력이 critical한 대규모 deployment
\end{itemize}

\vspace{0.1cm}

\textbf{4. 현실적 미래: Hybrid 접근}

업계는 \textbf{Co-Packaged Copper + Co-Packaged Optics} 병행 사용할 가능성 높음:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{In-rack}: CPC로 copper 사용 (저렴하고 충분)
\item \textbf{Across-rack}: CPO로 optics 사용 (필수)
\item NVIDIA도 이 hybrid 전략 추구 중
\end{itemize}

\textbf{구체적 예시}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item Rack 내부 72 GPU 연결: CPC (NVLink over copper)
\item Rack 간 scale-up: CPO (NVLink over optics)
\item Datacenter switch fabric: CPO (scale-out networking)
\end{itemize}

\vspace{0.1cm}

\textbf{핵심 통찰}: "할 수 있는 곳에서는 copper를 사용하고, 해야 할 때는 optics를 사용하라." 
CPC는 이 격언의 현대적 구현입니다. CPO가 모든 것을 대체하는 것이 아니라, copper와 optics가 각자 최적인 영역에서 co-exist하는 미래가 더 현실적입니다. 

\end{tcolorbox}

\subsection{Past obstacles to CPO market readiness: Why only now?}
기술적 우월성에도 불구하고 CPO는 비용을 증가시키는 여러 과제로 인해 실제 채택이 매우 제한적이었다. 
여기에는 packaging (OE 자체보다 비용이 더 많이 드는)과 manufacturing의 복잡성, 신뢰성 및 yield 우려, 
그리고 긴밀하게 통합된 optical 및 electrical component에서 발생하는 thermal management 문제가 포함된다. 
또 다른 장애물은 산업 전반의 표준화 부족이다. 추가로, 고객들은 serviceability에 대해 우려하고 있으며 이는 전통적인 pluggable optics에서 CPO 솔루션으로의 전환을 방해한다.

\textbf{또 다른 주요 고객 불안은 CPO를 채택함으로써 비용을 통제할 수 있는 능력을 잠재적으로 포기한다는 것이다.}
더 적은 수의 switch vendor보다 더 많은 수의 transceiver 회사에 비용 압박을 가하는 것이 훨씬 쉽다.
한편, CPO가 대체할 기존 기술인 pluggable optics는 계속 개선되고 있으며 여전히 훨씬 적은 최종 사용자 불안으로 거의 모든 application에 충분히 좋은 성능을 제공한다.
Part 2의 나머지 부분에서는 CPO 채택의 동기에 대해 더 깊이 탐구할 것이다. 
SerDes scaling이 어떻게 plateau에 도달하고 있는지 설명하는 것으로 시작하여, Wide I/O와 결합된 CPO와 같은 다른 interface type을 필요로 하게 만들고, 
그 다음 manufacturing 고려사항과 시장 출시로 들어갈 것이다. Optical Engine, Fiber Coupling, External Laser Source 및 Modulator와 같은 주요 개별 CPO component를 논의할 것이다. 
마지막으로, CPO에서 bandwidth를 확장하기 위한 roadmap을 다룰 것이다.

\subsection{Evolving beyond DSP-based Transceivers: From LPO to CPO}
DSP Transceiver는 optical signal의 transmission과 reception을 모두 처리하며 electro-optical conversion을 담당하는 \textbf{Optical Engine (OE)}을 포함한다. 
OE는 optical signal을 전송하기 위한 \textbf{driver (DRV)} 및 \textbf{modulator (MOD)}, 
그리고 optical signal을 수신하기 위한 \textbf{transimpedance amplifier (TIA)} 및 \textbf{photodetector (PD)}로 구성된다.
또 다른 중요한 component는 optical DSP chip인데, 이는 때때로 Driver 및/또는 TIA를 하나의 package로 통합한다. 
Host switching 또는 processing chip에서 전송되는 high frequency electrical signal은 server chassis의 front에 있는 transceiver에 도달하기 위해 손실이 많은 copper trace를 통해 상대적으로 긴 거리를 이동해야 한다. 
DSP는 이 signal의 retiming 및 reconditioning을 담당한다. 
Signal이 switch 또는 ASIC silicon에서 substrate 또는 다른 transmission medium을 통과할 때 electrical signal degradation과 attenuation을 보상하기 위해 error correction 및 clock/data recovery를 수행한다. 
Modulation의 경우, \textbf{PAM4 Modulation (Pulse Amplitude Modulation with 4 Levels)}의 경우, 
DSP는 signal당 bit 수를 증가시키기 위해 binary signal을 4개의 distinct amplitude level로 매핑하여 더 높은 bitrate와 더 많은 bandwidth를 허용한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/350cad4c-11a7-4da3-ba99-5566eac861fc_1353x408.jpg}
\caption{Source: SemiAnalysis}
\end{figure}

\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: DSP Transceiver의 구조와 역할}]
\setlength{\parskip}{2pt}

\textbf{1. Optical Engine (OE)의 구성요소}

Optical Engine은 전기-광 변환을 담당하는 핵심 component:

\textbf{송신 (TX) 측}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Driver (DRV)}: Electrical signal을 증폭하여 modulator 구동
\item \textbf{Modulator (MOD)}: Laser 빛을 electrical signal에 따라 변조
  \begin{itemize}[itemsep=0pt]
  \item 빛의 세기나 위상을 바꿔서 데이터 encoding
  \item 예: PAM4는 4개의 다른 세기 level 사용
  \end{itemize}
\end{itemize}

\textbf{수신 (RX) 측}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Photodetector (PD)}: 광 신호를 전기 신호로 변환
  \begin{itemize}[itemsep=0pt]
  \item 빛을 받으면 전류 생성
  \item 매우 약한 신호 (micro-ampere 수준)
  \end{itemize}
\item \textbf{Transimpedance Amplifier (TIA)}: PD의 약한 전류를 증폭
  \begin{itemize}[itemsep=0pt]
  \item 전류 신호를 전압 신호로 변환하면서 증폭
  \item 신호 대 잡음비 향상
  \end{itemize}
\end{itemize}

\vspace{0.1cm}

\textbf{2. Optical DSP Chip의 역할}

\textbf{왜 DSP가 필요한가?} 긴 copper path(15-30cm)를 거치면서 electrical signal이 심하게 degradation되기 때문

\textbf{주요 기능}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Signal Reconditioning}: 저하된 신호를 원래 모양으로 복원
\item \textbf{Retiming}: Clock 신호를 재생성하여 timing jitter 제거
\item \textbf{Error Correction (FEC)}: Forward Error Correction으로 bit error 수정
\item \textbf{Clock/Data Recovery (CDR)}: Signal에서 clock 추출하고 데이터 복구
\item \textbf{Equalization}: 주파수 dependent loss 보상
  \begin{itemize}[itemsep=0pt]
  \item 고주파수 성분이 저주파수보다 더 감쇠됨
  \item 고주파수를 선택적으로 증폭하여 균형 맞춤
  \end{itemize}
\end{itemize}

\vspace{0.1cm}

\textbf{3. PAM4 Modulation}

\textbf{기본 개념}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{PAM (Pulse Amplitude Modulation)}: 신호의 진폭(세기)으로 데이터 encoding
\item \textbf{PAM2 (NRZ)}: 2개 level (0, 1) → 1 bit per symbol
\item \textbf{PAM4}: 4개 level (00, 01, 10, 11) → \textbf{2 bits per symbol}
\end{itemize}

\textbf{PAM4의 장점}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 동일한 baud rate에서 2배 bandwidth
\item 예: 56 Gbaud × 2 bits = 112 Gbps
\item Symbol rate 낮춰서 frequency 관련 loss 완화
\end{itemize}

\textbf{PAM4의 단점}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item Level 간 간격이 좁아져 noise margin 감소
\item 더 복잡한 DSP 필요 (level 구분)
\item 더 높은 전력 소비
\end{itemize}

\textbf{미래 (448G 시대)}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{PAM6}: 6 level → 약 2.58 bits per symbol
\item \textbf{PAM8}: 8 level → 3 bits per symbol
\item 하지만 noise margin이 더욱 줄어들어 구현 매우 어려움
\end{itemize}

\vspace{0.1cm}

\textbf{4. DSP의 통합 형태}

DSP chip은 종종 다른 component와 통합:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{DSP + Driver}: 송신 측 통합
\item \textbf{DSP + TIA}: 수신 측 통합
\item \textbf{DSP + Driver + TIA}: 완전 통합 (single package)
\end{itemize}

\textbf{핵심 통찰}: DSP transceiver는 "signal degradation을 극복하기 위한 정교한 복구 시스템"입니다. 하지만 이 복구 시스템 자체가 엄청난 전력을 소비합니다. CPO의 본질은 "signal degradation 자체를 예방"하여 이 복구 시스템을 아예 제거하는 것입니다.

\end{tcolorbox}

DSP chip은 transceiver 내에서 가장 전력을 많이 소비하고 비싼 component 중 하나이다. 
800G SR8 Transceiver의 경우 -- DSP는 module의 총 전력 소비의 거의 \textasciitilde{}50\%를 차지하며, 이것이 DSP를 제거하는 데 그토록 많은 초점이 맞춰진 이유이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/49a322c1-c7b4-4e7e-b880-d6a20f501fcc_1024x650.png}
\caption{Source: \href{https://ieeexplore.ieee.org/document/10526441/}{Dr. Radha Nagarajan et al.: Recent Advances in Low-Power Digital Signal Processing Technologies for Data Center Applications}}
\end{figure}
2-layer InfiniBand network로 구축된 18K GB300 Cluster는 18,432개의 800G DR4 transceiver와 27,648개의 1.6T DR8 transceiver를 필요로 할 것이다. 
DSP 사용으로 인한 추가 비용과 전력 요구사항은 total cost of ownership에 상당히 추가될 수 있다. 
800G DSP에 6-7W, 1.6T DSP에 12-14W를 예산으로 잡으면, 이 전체 cluster의 back-end network만을 위해 480kW의 DSP 전력이 추가되며,
이는 server rack당 약 1.8kW이다. 프리미엄 브랜드 공급업체에서 조달할 때, transceiver는 cluster의 total cost of ownership의 거의 10\%를 차지할 수 있다. 
따라서, 일반적인 transceiver의 전력 소비의 50\%와 BoM의 20-30\%를 차지하는 것을 고려할 때 일부는 DSP를 비용 및 전력 효율성의 public enemy number one으로 간주한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/0b4adda8-3877-4a99-a96a-90088595122a_1289x372.png}
\caption{Source: SemiAnalysis AI Networking Model}
\end{figure}

\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: DR4, DR8 - Optical Transceiver 규격}]
\setlength{\parskip}{2pt}

\textbf{1. DR의 의미}

\textbf{DR = Dense Wavelength Division Multiplexing (DWDM) Reach}

\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{D (Dense)}: 좁은 파장 간격으로 여러 wavelength 사용
\item \textbf{R (Reach)}: 전송 거리 category
  \begin{itemize}[itemsep=0pt]
  \item \textbf{SR (Short Reach)}: 100m 이하, multimode fiber
  \item \textbf{DR (DWDM Reach)}: 500m-2km, single-mode fiber, DWDM 지원
  \item \textbf{FR (Far Reach)}: 2km, single-mode fiber
  \item \textbf{LR (Long Reach)}: 10km, single-mode fiber
  \end{itemize}
\item DR은 datacenter 내부 연결에 최적화된 규격
\end{itemize}

\vspace{0.1cm}

\textbf{2. 숫자의 의미 (Lane 수)}

\textbf{DR 뒤의 숫자 = Optical lane의 개수}

\textbf{DR4}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{4개의 optical lane} (4개의 laser + 4개의 photodetector)
\item 각 lane이 독립적으로 데이터 전송
\item \textbf{800G DR4}: 4 lanes × 200G/lane = 800 Gbps
\item \textbf{400G DR4}: 4 lanes × 100G/lane = 400 Gbps
\end{itemize}

\textbf{DR8}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{8개의 optical lane}
\item \textbf{1.6T DR8}: 8 lanes × 200G/lane = 1.6 Tbps
\item \textbf{800G DR8}: 8 lanes × 100G/lane = 800 Gbps
\end{itemize}

\vspace{0.1cm}

\textbf{3. Lane 수와 Form Factor}

\begin{tabular}{|l|c|c|c|}
\hline
\textbf{규격} & \textbf{Lane 수} & \textbf{Fiber 수} & \textbf{Form Factor} \\
\hline
400G DR4 & 4 TX + 4 RX & 8 (4 pair) & QSFP-DD \\
\hline
800G DR4 & 4 TX + 4 RX & 8 (4 pair) & QSFP-DD \\
\hline
800G DR8 & 8 TX + 8 RX & 16 (8 pair) & OSFP \\
\hline
1.6T DR8 & 8 TX + 8 RX & 16 (8 pair) & OSFP \\
\hline
\end{tabular}

\vspace{0.1cm}

\textbf{왜 더 많은 lane을 사용하는가?}
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Lane당 속도 제한}: 단일 lane의 속도를 무한정 올리기 어려움
  \begin{itemize}[itemsep=0pt]
  \item 현재: 100G-200G per lane이 주류
  \item 미래: 400G per lane 목표 (매우 어려움)
  \end{itemize}
\item \textbf{Parallel transmission}: 여러 lane으로 병렬 전송하여 총 bandwidth 증가
\item \textbf{Redundancy}: 일부 lane 고장 시에도 동작 가능 (일부 제품)
\end{itemize}

\vspace{0.1cm}

\textbf{4. 구체적 예시}

\textbf{800G DR4 Transceiver}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 4개의 1310nm laser (CWDM4 wavelength)
\item 각 laser가 200 Gbps 전송 (PAM4 modulation)
\item 4개의 fiber로 TX, 4개의 fiber로 RX
\item Single-mode fiber, 최대 500m
\item 전력: 약 16-17W
\end{itemize}

\textbf{1.6T DR8 Transceiver}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 8개의 laser (CWDM8 wavelength)
\item 각 laser가 200 Gbps 전송
\item 8개의 fiber로 TX, 8개의 fiber로 RX
\item OSFP form factor (QSFP-DD보다 큼)
\item 전력: 약 30-35W (DR4의 약 2배)
\end{itemize}

\vspace{0.1cm}

\textbf{5. 왜 18K GPU Cluster에서 DR4와 DR8을 혼용하는가?}

\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{GPU-to-switch}: 800G DR4
  \begin{itemize}[itemsep=0pt]
  \item 각 GPU는 800G로 충분
  \item QSFP-DD form factor로 공간 효율적
  \item 비용 효율적
  \end{itemize}
\item \textbf{Switch-to-switch}: 1.6T DR8
  \begin{itemize}[itemsep=0pt]
  \item Switch 간 연결은 많은 traffic 집약 (aggregation)
  \item Higher bandwidth 필요
  \item Port 수 절감 (1.6T 1개 = 800G 2개)
  \end{itemize}
\end{itemize}

\textbf{Trade-off}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{더 많은 lane} = 더 높은 총 bandwidth, 하지만 더 많은 fiber, 더 큰 form factor, 더 높은 전력
\item \textbf{적은 lane} = 더 간단, 저렴, 작은 form factor, 하지만 낮은 bandwidth
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[breakable,colback=red!5!white,colframe=red!75!black,title=\textbf{핵심 인사이트: DSP - Public Enemy Number One}]
\setlength{\parskip}{2pt}

\textbf{18K GB300 Cluster의 DSP 전력 비용}

\textbf{DSP 전력 계산}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item (18,432 × 6.5W) + (27,648 × 13W) = \textbf{480 kW}
\item Rack당 (256 racks): \textbf{1.8 kW per rack}
\end{itemize}

\textbf{DSP 점유율}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item Transceiver 전력의 \textbf{50\%}
\item Transceiver BoM의 \textbf{20-30\%}
\item Cluster TCO의 \textbf{2-3\%}
\end{itemize}

\vspace{0.1cm}

\textbf{"공공의 적 1호"로 불리는 이유}

\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item Transceiver 전력/비용의 절반 차지하는 단일 component
\item GPU 수가 늘수록 exponentially 증가
\item \textbf{근본적 비효율}: Signal degradation을 "사후 복구" (원인 미해결)
\item 하지만 \textbf{기술적으로 제거 가능} (LPO, CPO)
\end{itemize}

\vspace{0.1cm}

\textbf{DSP 제거 시 절감 효과}:

\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{전력}: 480 kW 제거 → 10년간 \$4.2M 절감 (cluster당)
\item \textbf{비용}: \$9-14M 절감 (cluster당)
\item \textbf{간접}: 전력 예산 확보, cooling 절감, reliability 향상
\end{itemize}

\vspace{0.1cm}

\textbf{LPO vs CPO 접근}:

\begin{tabular}{|l|c|c|}
\hline
\textbf{특성} & \textbf{LPO} & \textbf{CPO} \\
\hline
DSP 제거 & O & O \\
\hline
Form factor & Pluggable 유지 & 제거 \\
\hline
Copper path & 긴 경로 유지 & 극단적 단축 \\
\hline
결과 & 제한적 성공 & 근본적 해결 \\
\hline
\end{tabular}

\end{tcolorbox}

\subsubsection{The Crusade Against DSP}
DSP가 차지하는 높은 비용과 전력 비율은 업계가 DSP를 중개할 수 있는 기술을 찾도록 동기를 부여했다. 
DSP에 대한 첫 번째 wave의 공격은 \textbf{Linear Pluggable Optics (LPO)}였는데, 이는 DSP를 완전히 제거하고 switch의 SerDes가 transceiver의 TX 및 RX optical element를 직접 구동하도록 시도한다. 
그러나 LPO는 \href{https://semianalysis.com/2023/03/08/marvells-dsp-dilemma-networkings/}{DSP Diviner Loi Nguyen이 2023년 우리와의 인터뷰에서 올바르게 예측한 것처럼} 아직 이륙하지 못했다.

CPO는 optical engine을 compute 또는 switch chip과 동일한 package에 배치함으로써 LPO 개념을 다음 단계로 가져간다. 
CPO의 핵심 이점은 host와 optical engine 사이의 거리가 매우 짧기 때문에 transceiver에 있던 DSP가 더 이상 필요하지 않다는 것이다. 
CPO는 또한 전력 및 면적을 많이 소비하는 LR SerDes를 제거하고 shorter reach SerDes 또는 wide I/O interface의 경우 clock forwarded wide D2D SerDes를 선호함으로써 훨씬 더 큰 chip shoreline density를 가능하게 하기 때문에 LPO보다 더 나아간다.

자주 인용되는 표현은 CPO가 지난 20년 동안 바로 곧 나올 것이라고 했지만, 왜 그렇게 오랫동안 이륙하지 못했는가?
왜 업계는 pluggable DSP transceiver를 고수하는 것을 선호했는가?

Pluggable transceiver의 주요 이점 중 하나는 높은 상호운용성이다. OSFP 및 QSFP-DD와 같은 표준 form factor와 OIF standard를 준수함으로써, 
고객은 일반적으로 switch 및 server vendor와 독립적으로 transceiver vendor를 선택할 수 있어 조달 유연성과 더 강한 협상력을 누린다.

또 다른 큰 이점은 field serviceability이다. 
Transceiver를 설치하고 교체하는 것은 remote hand 한 쌍으로 switch 또는 server chassis에서 플러그를 뽑을 수 있기 때문에 간단하다. 
이와 대조적으로 CPO의 경우 optical engine의 어떤 failure도 전체 switch를 사용할 수 없게 만들 수 있다. 
서비스 가능한 failure조차도 troubleshoot하고 수정하기가 복잡할 수 있다. 종종 laser가 가장 일반적인 failure point이며, 
대부분의 CPO 구현은 이제 더 나은 serviceability 및 replaceability를 위해 pluggable external laser source를 사용하지만, 
다른 non-pluggable CPO component의 failure에 대한 불안은 여전히 남아 있다.

\subsection{Why CPO? The I/O challenge, BW density, and bottlenecks}

전력을 많이 소비하고 비용이 많이 드는 DSP를 제거하고 LR SerDes의 사용을 최소화하거나 제거하는 것 외에도, 
CPO 채택의 또 다른 큰 이점은 에너지 소비 대비 더 큰 interconnect bandwidth density이다.
Bandwidth density는 단위 면적 또는 channel당 전송되는 데이터의 양을 측정하며, 제한된 공간이 고속 데이터 전송을 위해 얼마나 효과적으로 활용될 수 있는지를 반영한다. 
Energy efficiency는 데이터 단위를 전송하는 데 필요한 에너지를 정량화한다.

따라서 에너지 소비 대비 interconnect bandwidth density는 주어진 interconnect의 객관적 품질을 결정할 때 매우 중요한 \textbf{Figure of Merit (FoM)}이다. 
물론 최적의 interconnect는 거리 및 비용 parameter 내에 맞는 것이기도 하다.
아래 차트를 검토할 때 명확한 추세가 나타난다. 이 figure of merit는 거리가 증가함에 따라 electrical link의 경우 exponentially하게 저하된다. 
또한 순수한 electrical interface에서 optical--electrical conversion이 필요한 interface로 이동하면 효율성이 상당히 떨어진다.
잠재적으로 magnitude의 order만큼. 이러한 drop은 chip에서 transceiver가 있는 front-panel까지 일정 거리로 signal을 구동하는 데 에너지가 필요하기 때문에 발생한다. 
Optical DSP에 전력을 공급하는 데 더 많은 에너지가 필요하다. CPO 기반 communication을 위한 figure of merit curve는 pluggable보다 확실히 위에 있다. 
아래 차트에 표시된 바와 같이, CPO는 동일한 거리 범위에서 소비된 에너지당 면적당 더 많은 bandwidth density를 제공하여 객관적으로 더 나은 interconnect를 만든다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/957df714-e3aa-4233-baa6-d5672492cb98_936x558.png}
\caption{Source: G Keeler, DARPA 2019, SemiAnalysis}
\end{figure}

\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{핵심 인사이트: Figure of Merit (FoM) - Interconnect의 성적표}]
\setlength{\parskip}{2pt}

\textbf{1. Figure of Merit (FoM)란?}

\textbf{정의}: Interconnect의 객관적 품질을 나타내는 지표

\textbf{FoM 공식}:
\[
\text{FoM} = \frac{\text{Bandwidth Density (Gbps/mm)}}{\text{Energy Efficiency (pJ/bit)}}
\]

\textbf{더 높은 FoM = 더 좋은 interconnect}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 단위 면적(mm)당 더 많은 데이터 전송
\item 비트당 더 적은 에너지 소비
\item 즉, 공간 효율 × 전력 효율
\end{itemize}

\vspace{0.1cm}

\textbf{2. 구성 요소 분해}

\textbf{Bandwidth Density (대역폭 밀도)}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 단위: Gbps/mm (또는 Tbps/mm)
\item Chip shoreline 1mm당 전송 가능한 데이터량
\item Reticle-sized chip (약 858mm$^2$, perimeter 약 120mm)에서 중요
\item 예: 0.4 Tbps/mm × 120mm = 48 Tbps total bandwidth
\end{itemize}

\textbf{Energy Efficiency (에너지 효율)}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 단위: pJ/bit (pico-Joule per bit)
\item 1 bit 전송에 소비되는 에너지
\item 낮을수록 좋음
\item 예: 10 pJ/bit × 1 Tbps = 10W
\end{itemize}

\vspace{0.1cm}

\textbf{3. 차트 해석: 거리에 따른 FoM 변화}

\textbf{Pure Electrical (Copper)}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Short reach (<1cm)}: 최고 FoM
  \begin{itemize}[itemsep=0pt]
  \item Die-to-die, chiplet interconnect
  \item 예: UCIe, 2.5D/3D packaging
  \item 이유: 신호 감쇠 최소, DSP 불필요
  \end{itemize}
\item \textbf{Medium reach (1-10cm)}: FoM 급격히 저하
  \begin{itemize}[itemsep=0pt]
  \item Package-to-package interconnect
  \item 여전히 copper 가능하지만 LR SerDes 필요
  \end{itemize}
\item \textbf{Long reach (>10cm)}: FoM 매우 낮음
  \begin{itemize}[itemsep=0pt]
  \item PCB trace를 통한 연결
  \item 높은 전력 소비, 낮은 bandwidth density
  \end{itemize}
\end{itemize}

\textbf{Pluggable Optics}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Electrical-Optical transition에서 FoM 급락}
  \begin{itemize}[itemsep=0pt]
  \item E/O, O/E conversion loss
  \item DSP chip의 높은 전력
  \item LR SerDes 전력
  \end{itemize}
\item \textbf{10cm-10m 범위}: FoM이 낮지만 일정
  \begin{itemize}[itemsep=0pt]
  \item Optical signal은 거리에 둔감
  \item 대부분의 전력은 DSP와 E/O conversion
  \item 거리가 늘어나도 추가 전력 거의 없음
  \end{itemize}
\end{itemize}

\textbf{CPO (Co-Packaged Optics)}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Pluggable보다 확실히 위}: 1-10m 범위에서
  \begin{itemize}[itemsep=0pt]
  \item DSP 제거로 전력 대폭 절감
  \item SR SerDes 또는 wide I/O로 전력 효율 향상
  \item 더 높은 bandwidth density (shoreline)
  \end{itemize}
\item \textbf{Electrical과 optical 사이의 "sweet spot"}
  \begin{itemize}[itemsep=0pt]
  \item Electrical의 energy efficiency 상속 (짧은 전기 경로)
  \item Optical의 reach 상속 (긴 광 경로)
  \end{itemize}
\end{itemize}

\vspace{0.1cm}

\textbf{4. "할 수 있는 곳에서는 Copper, 해야 할 때는 Optical"}

이 격언을 FoM 차트가 증명:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{<1cm}: Pure copper가 왕 (chiplet, interposer)
\item \textbf{1-2m}: Copper 가능하지만 FoM 떨어짐 (in-rack)
\item \textbf{>2m}: Optics 필수 (rack-to-rack)
\item \textbf{CPO의 역할}: Optics 영역(>2m)에서 FoM을 최대한 끌어올림
\end{itemize}

\vspace{0.1cm}

\textbf{핵심 통찰}: FoM 차트는 interconnect의 "물리 법칙"을 보여줍니다. Copper는 짧은 거리에서 exponentially 좋지만, 거리가 늘어나면 exponentially 나빠집니다. Optics는 전환 비용이 크지만(E/O conversion), 일단 optical domain에 들어가면 거리에 둔감합니다. CPO는 "전환 비용을 최소화"하여 optics의 최적 FoM을 달성합니다.

\end{tcolorbox}

\textbf{이 차트는 또한 ``할 수 있는 곳에서는 copper를 사용하고 해야 할 때는 optical을 사용하라''는 격언을 보여준다.} 
Copper를 통한 shorter reach communication은 사용 가능할 때 우수하다. 
NVIDIA는 copper를 통해 네트워크로 연결될 수 있는 GPU 수를 최대화하기 위해 intra-rack density의 한계를 밀어붙이는 목적으로만 설계된 rack-scale GPU architecture로 이 mantra를 받아들인다. 
이것이 \href{https://semianalysis.com/2024/07/17/gb200-hardware-architecture-and-component/}{GB200 NVL72}에 사용되는 scale-up network architecture 뒤의 rationale이며, NVIDIA는 \href{https://semianalysis.com/2025/03/19/nvidia-gtc-2025-built-for-reasoning-vera-rubin-kyber-cpo-dynamo-inference-jensen-math-feynman/#kyber-rack-architecture}{Kyber rack}에서 이 아이디어를 더욱 발전시키고 있다. 그러나 -- CPO의 성숙도가 scale-up을 위한 FoM curve의 일부에 접근하는 것을 실행 가능하게 만들고 performance per TCO 관점에서 가치 있게 만드는 것은 시간 문제일 뿐이다.

\subsubsection{Input/Output (I/O) Speedbumps and Roadblocks}
Transistor density와 compute (FLOP로 표현되는)는 잘 확장되었지만, I/O는 훨씬 더 느리게 확장되어 전체 시스템 성능에서 bottleneck을 만들었다.
Off-chip I/O를 위해 사용 가능한 shoreline은 매우 제한적인데, off-chip로 나가는 데이터가 organic package substrate의 제한된 수의 I/O를 통해 escape해야 하기 때문이다.
추가로, 각 개별 I/O의 signaling speed를 증가시키는 것이 점점 더 어려워지고 전력 집약적이 되어, 데이터 이동을 더욱 제약한다. 
이것이 interconnect bandwidth가 다른 computing trend에 비해 지난 수십 년 동안 그렇게 형편없이 확장된 주요 이유이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/be0c6e57-2f6c-477f-9188-9b1fcd08d5c0_1024x523.jpg}
\caption{Source: Amir Gholami}
\end{figure}
HPC application을 위한 off-package I/O density는 단일 flip-chip BGA package의 bump 수 제한으로 인해 plateau에 도달했다. 
이것은 escape bandwidth를 확장하는 데 대한 제약이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/5c865684-e533-400c-8fe8-ce3f6d551da3_865x473.jpg}
\caption{Source: TSMC}
\end{figure}

\begin{tcolorbox}[breakable,colback=red!5!white,colframe=red!75!black,title=\textbf{핵심 인사이트: I/O Wall - AI 시대의 새로운 Bottleneck}]
\setlength{\parskip}{2pt}

\textbf{1. The Great Imbalance}

\textbf{Transistor Density의 폭발적 증가}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Moore's Law}: Transistor 수 18-24개월마다 2배
\item 지난 20년: 약 $2^{10}$ = 1,000배 증가
\item FLOP (compute) 성능도 유사하게 증가
\end{itemize}

\textbf{I/O Bandwidth의 정체}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 같은 기간 I/O bandwidth는 약 10-20배만 증가
\item \textbf{100배 격차 발생!}
\item 차트에서 명확히 보임: Compute curve vs I/O curve 급격히 벌어짐
\end{itemize}

\vspace{0.1cm}

\textbf{2. 왜 I/O가 느리게 증가하는가?}

\textbf{Shoreline의 물리적 한계}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Shoreline}: Chip 외곽 둘레 (perimeter)
\item Die size는 reticle limit (858mm$^2$)로 제한
\item Perimeter: 약 $4 \times \sqrt{858} \approx$ 120mm
\item \textbf{I/O는 shoreline을 통해서만 나갈 수 있음}
\item Bump pitch는 물리적 한계로 더 줄이기 어려움
\end{itemize}

\textbf{Package Substrate의 한계}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item Organic substrate의 \textbf{escape bandwidth} 제한
\item 수천-수만 개의 bump를 모두 routing하기 어려움
\item Layer 추가는 비용과 복잡도 급증
\item HPC용 flip-chip BGA package는 plateau 도달
\end{itemize}

\textbf{SerDes Speed의 한계}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 각 I/O의 속도(Gbps)를 올리는 것도 한계 도달 중
\item 224G SerDes는 이미 매우 어려움
\item 448G는 더욱 어려움 (뒤에서 상세 설명)
\end{itemize}

\vspace{0.1cm}

\textbf{3. AI가 I/O를 더욱 악화시킴}

\textbf{Traditional HPC vs AI}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Traditional HPC}: Compute-bound (I/O 덜 중요)
\item \textbf{AI Training/Inference}: Communication-intensive
  \begin{itemize}[itemsep=0pt]
  \item All-reduce, all-gather 등 collective communication 빈번
  \item Model parameter 교환으로 enormous 데이터 이동
  \item Transformer architecture: Attention mechanism이 communication 유발
  \end{itemize}
\end{itemize}


\vspace{0.1cm}


\vspace{0.1cm}

\textbf{4. I/O Wall의 영향}

\textbf{시스템 레벨}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item GPU utilization 저하 (50-70\%로 떨어질 수 있음)
\item Training time 증가
\item TCO 악화 (비싼 GPU를 놀림)
\end{itemize}

\textbf{Architecture 레벨}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item Model parallelism strategy 제약
\item Scalability limit (GPU 수 늘려도 성능 향상 둔화)
\item 새로운 architecture 탐색 필요 (예: sparse model)
\end{itemize}

\vspace{0.1cm}

\textbf{5. CPO가 해결하는 방법}

\textbf{Shoreline Density 극대화}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item Wide I/O (UCIe 등) 사용 가능: 10 Tbps/mm
\item SR SerDes의 면적/전력 효율 향상
\item 0.4 Tbps/mm → 10 Tbps/mm = \textbf{25배 향상}
\end{itemize}

\textbf{실제 예시} (가상):
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item CPO + Wide I/O로 120mm shoreline 활용
\item 10 Tbps/mm × 120mm = 1.2 Pbps (1,200 Tbps!)
\item Blackwell 대비 50배 이상 I/O bandwidth
\item Arithmetic intensity를 workload에 맞출 수 있음
\end{itemize}

\vspace{0.1cm}

\textbf{핵심 통찰}: "I/O Wall"은 AI 시대의 진짜 bottleneck입니다. Moore's Law가 계속되어도 I/O가 따라가지 못하면 GPU는 "굶주리게"이 됩니다. CPO + Wide I/O는 이 wall을 허무는 가장 유망한 해법입니다. 이것이 CPO가 단순히 "전력 절감 기술"이 아니라 "\textbf{AI scaling의 미래를 결정하는 핵심 기술}"인 이유입니다.

\end{tcolorbox}

\subsubsection{Electrical SerDes Scaling Bottlenecks}
제한된 수의 I/O로 더 많은 escape bandwidth를 실현하는 방법은 각 I/O가 signaling하는 frequency를 밀어붙이는 것이다. 
오늘날 NVIDIA와 Broadcom은 SerDes IP의 선두에 있다. 
NVIDIA는 Blackwell에서 224G SerDes를 출하하고 있으며, 이것이 그들의 blazing fast NVLink를 가능하게 한다. 
마찬가지로 Broadcom은 2024년 후반부터 optical DSP에서 224G SerDes를 sampling하고 있다. 
업계에서 가장 많은 AI FLOP을 출하하는 두 회사가 high-speed SerDes IP에서도 선두를 달리는 것은 우연이 아니다. 
이는 AI 성능과 throughput 사이의 근본적인 연결을 강화하며, 여기서 데이터 이동 효율성을 최대화하는 것은 raw compute power를 제공하는 것만큼 중요하다.

그러나 바람직한 reach에서 더 높은 line speed를 제공하는 것이 점점 더 어려워지고 있다. 
frequency가 증가함에 따라 insertion loss가 증가하며, 아래 차트에 표시된 대로이다. 
Signal path가 길어짐에 따라 특히 더 높은 SerDes signaling speed에서 loss가 증가하는 것을 볼 수 있다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/04d682ff-06fb-4322-bf83-1a496893c91c_1024x564.png}
\caption{Source: Broadcom}
\end{figure}
SerDes scaling은 plateau에 접근하고 있다. 
더 높은 속도는 추가 signal recovery component 없이는 매우 짧은 거리에서만 유지될 수 있으며, 이는 차례로 복잡성, 비용, latency 및 전력 소비를 증가시킨다. 
224G SerDes에 도달하는 것이 어려웠다.

448G SerDes를 앞서 보면, 단지 몇 centimeter 이상을 구동하는 feasibility는 더 불확실하다. 
NVIDIA는 bi-directional SerDes 기술을 사용하여 Rubin에서 electrical channel당 448G connectivity를 제공하고 있다. 
True 448G uni-directional SerDes를 달성하는 것은 추가 개발이 필요할 것이다. 
56G SerDes 시대 이후 널리 퍼진 PAM4 modulation 대신 PAM6 또는 PAM8과 같은 더 높은 order의 modulation으로 이동해야 할 수도 있다. 
Signal당 2 bit를 encoding하는 PAM4를 사용하여 448G에 도달하려면 과도한 전력 소비와 insertion loss로 인해 untenable할 가능성이 있는 244Gbaud의 baud rate가 필요할 것이다.

\subsubsection{SerDes Scaling Plateau as a Roadblock for Scaling NVLink}
NVLink protocol에서 NVLink 5.0의 bandwidth는 NVLink 1.0과 비교하여 11배 이상 증가했다. 
그러나 이 성장은 lane 수의 상당한 증가에서 나온 것이 아니며, NVLink 1.0의 32 lane에서 NVLink 5.0의 36 lane으로 약간만 증가했다. 
Scaling의 핵심 driver는 SerDes lane speed의 10배 증가였으며, 20G에서 200G로 증가했다. 
그러나 NVLink 6.0에서 NVIDIA는 200G SerDes에 머물 것으로 예상되며, 이는 lane 수를 두 배로 늘려야 함을 의미한다. 
동일한 수의 물리적 copper wire를 사용하면서 lane 수를 효과적으로 두 배로 늘리기 위해 bi-directional SerDes를 사용함으로써 이를 영리하게 제공한다. 
이를 넘어서면 SerDes speed를 확장하거나 더 많은 lane을 맞추기 위한 제한된 shoreline 가용성을 극복하는 것이 점점 더 어려워질 것이며 total escape bandwidth는 막히게 될 것이다.

Escape bandwidth를 확장하는 것은 throughput이 차별화 요소인 선두 기업에게 중요하다. 
NVLink scale-up fabric이 중요한 moat인 NVIDIA의 경우, 이 roadblock은 AMD 및 hyperscaler와 같은 경쟁자가 따라잡기 쉽게 만들 수 있다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/ea946590-1ac8-4010-937b-43583e6eedfe_2413x486.png}
\caption{Source: NVIDIA, SemiAnalysis}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/bd2a2f28-f353-47de-903f-f469f78c1316_1184x556.png}
\caption{Source: NVIDIA, SemiAnalysis}
\end{figure}

\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: SerDes Scaling의 물리적 한계}]
\setlength{\parskip}{2pt}

\textbf{1. Insertion Loss의 exponential 증가}

\textbf{Insertion Loss란?}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item Signal이 medium(copper)을 통과하며 잃는 에너지
\item 단위: dB (decibel) - 높을수록 더 많은 loss
\item \textbf{Frequency dependent}: 고주파수일수록 loss 커짐
\item \textbf{Distance dependent}: 거리가 길수록 loss 커짐
\end{itemize}

\textbf{왜 고주파수에서 loss가 커지는가?}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Skin effect}: 전류가 표면에만 흐르면서 저항 증가
\item \textbf{Dielectric loss}: PCB 절연체가 고주파수를 흡수
\item \textbf{Radiation loss}: 고주파수 신호가 전자기파로 방출
\end{itemize}

\textbf{구체적 수치} (차트 기반):
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 56G SerDes (28 GHz): 10cm에서 약 20 dB loss
\item 112G SerDes (56 GHz): 10cm에서 약 35 dB loss
\item \textbf{224G SerDes (112 GHz): 10cm에서 약 50+ dB loss}
\item 448G SerDes (224 GHz): 10cm에서 추정 80+ dB loss (거의 불가능)
\end{itemize}

\vspace{0.1cm}

\textbf{2. 왜 224G SerDes가 어려운가?}

\textbf{PAM4 with 224G}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 224 Gbps ÷ 2 bits/symbol = 112 Gbaud
\item 112 Gbaud = 112 GHz fundamental frequency
\item PCB trace를 통과하면 50+ dB insertion loss
\item \textbf{50 dB = 신호가 1/100,000로 감소!}
\end{itemize}

\textbf{Recovery의 어려움}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{Equalization}: 고주파수 성분을 선택적으로 증폭
  \begin{itemize}[itemsep=0pt]
  \item Transmitter equalization (pre-emphasis)
  \item Receiver equalization (CTLE, DFE)
  \item 매우 복잡한 회로, 높은 전력 소비
  \end{itemize}
\item \textbf{CDR (Clock Data Recovery)}: 저하된 신호에서 clock 추출 극도로 어려움
\item \textbf{Noise margin}: PAM4의 4개 level이 noise에 묻힘
\end{itemize}

\textbf{전력 소비 폭발}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 56G SerDes: 약 20-30 mW/lane
\item 112G SerDes: 약 40-50 mW/lane
\item \textbf{224G SerDes: 추정 80-100+ mW/lane}
\item 448G SerDes: 예상 150-200 mW/lane (untenable)
\end{itemize}

\vspace{0.1cm}

\textbf{3. 448G SerDes의 딜레마}

\textbf{PAM4 approach} (현재 표준):
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 448 Gbps ÷ 2 bits/symbol = \textbf{224 Gbaud}
\item 224 GHz frequency → 거의 불가능한 insertion loss
\item 전력 소비가 untenable 수준 (150-200 mW/lane)
\end{itemize}

\textbf{Higher-order modulation} (PAM6, PAM8):
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{PAM6}: 6 levels → log$_2$(6) ≈ 2.58 bits/symbol
  \begin{itemize}[itemsep=0pt]
  \item 448 Gbps ÷ 2.58 = 174 Gbaud (PAM4보다 나음)
  \item 하지만 level 간 margin 더 좁아짐
  \end{itemize}
\item \textbf{PAM8}: 8 levels → 3 bits/symbol
  \begin{itemize}[itemsep=0pt]
  \item 448 Gbps ÷ 3 = 149 Gbaud (더 나음)
  \item 하지만 noise margin 극도로 작아짐 (level 간 1/8)
  \item DSP 복잡도와 전력 급증
  \end{itemize}
\end{itemize}

\textbf{NVIDIA의 해법: Bi-directional SerDes}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 하나의 wire에서 양방향 동시 전송
\item 2 × 224G = 448G effective
\item 영리하지만 복잡도 증가, echo cancellation 필요
\end{itemize}

\vspace{0.1cm}

\textbf{4. NVLink Scaling의 위기}

\textbf{NVLink 역사} (차트 참고):
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{NVLink 1.0}: 20G × 32 lanes = 640 Gbps
\item \textbf{NVLink 2.0}: 25G × 32 lanes = 800 Gbps
\item \textbf{NVLink 3.0}: 50G × 32 lanes = 1.6 Tbps
\item \textbf{NVLink 4.0}: 100G × 36 lanes = 3.6 Tbps
\item \textbf{NVLink 5.0}: 200G × 36 lanes = 7.2 Tbps
\end{itemize}

\textbf{핵심 관찰}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item Lane 수: 32 → 36 (거의 변화 없음)
\item \textbf{SerDes speed}: 20G → 200G (\textbf{10배 증가})
\item Bandwidth 증가는 대부분 SerDes speed에서 나옴
\end{itemize}

\textbf{NVLink 6.0의 문제} (Rubin):
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item NVIDIA는 200G SerDes에 머물 것으로 예상
\item 대신 \textbf{bi-directional SerDes}로 448G 달성
\item 하지만 이것도 한계: Lane 수를 늘리기 어려움 (shoreline 제한)
\item \textbf{Beyond NVLink 6.0}: Scaling path가 불투명
\end{itemize}

\textbf{경쟁자들의 기회}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item NVIDIA의 NVLink가 plateau → moat 약화
\item AMD, Intel, hyperscaler들이 따라잡기 쉬워짐
\item NVIDIA에게는 existential threat
\end{itemize}

\textbf{핵심 통찰}: SerDes scaling의 물리적 한계는 단순히 "기술적 도전"이 아니라 "\textbf{AI 업계의 구조를 바꿀 수 있는 전환점}"입니다. NVIDIA의 NVLink moat가 SerDes plateau로 무너질 수 있습니다. CPO는 이 위기에서 NVIDIA를 구할 수 있는 핵심 기술입니다.

\end{tcolorbox}

이 딜레마에 대한 솔루션 또는 다른 방식으로 표현하면 필요한 타협은 electrical I/O를 가능한 한 짧게 하고 더 높은 bandwidth를 달성하기 위해 host ASIC에 가능한 한 가까운 곳에서 optical link로 데이터 전송을 offload하는 것이다. 
이것이 CPO가 interconnect의 \textbf{holy grail}로 간주되는 이유이다. 
CPO는 substrate를 통해서든 interposer를 통해서든 ASIC package에서 optical communication이 발생하도록 허용한다. 
Electrical signal은 손실이 많은 \textbf{Copper-Clad Laminate (CCL)}를 통해 수십 centimeter가 아니라 package substrate를 통해 몇 millimeter만 이동하면 되거나, 
이상적으로는 더 높은 품질의 interposer를 통해 훨씬 더 짧은 거리를 이동하면 된다.

SerDes는 대신 shorter reach에 최적화될 수 있으며, 이는 동등한 long reach SerDes보다 훨씬 적은 회로가 필요하다. 
이는 설계를 더 쉽게 만들면서 더 적은 전력과 silicon 면적을 소비한다. 
이러한 단순화는 higher-speed SerDes를 구현하기 더 쉽게 만들고 SerDes scaling roadmap을 확장한다. 
그럼에도 불구하고 우리는 전통적인 bandwidth model에 의해 제약을 받으며, 여기서 bandwidth density는 SerDes speed에 비례하여 계속 확장된다.

훨씬 더 높은 B/W density를 달성하기 위해, wide I/O PHY는 극도로 짧은 거리에서 더 나은 옵션이며, 
SerDes interface보다 소비된 전력당 더 나은 bandwidth density를 제공한다. wide I/O는 또한 훨씬 더 advanced한 package의 비용을 수반한다. 
그러나 CPO의 경우 이것은 무의미한 point이다. 
Packaging이 이미 매우 advanced하므로 wide I/O PHY를 통합하는 것은 추가 packaging 복잡성을 거의 또는 전혀 추가하지 않는다.

\subsection{Wide I/O vs SerDes}
Electrical signal을 상대적으로 긴 거리로 구동할 필요가 더 이상 없으면, 
짧은 거리에서 훨씬 더 나은 shoreline density를 제공하는 wide interface를 사용하여 serialized interface를 완전히 escape할 수 있다.

그러한 예 중 하나는 UCIe interface이다. 
UCIe-A는 advanced package를 위해 설계된 최대 \textasciitilde{}10 Tbit/s/mm의 shoreline density를 제공할 수 있다 (즉, sub-2mm reach의 interposer를 통해 interfacing하는 chiplet). 
Reticle sized chip의 long edge에서 이것은 최대 330 Tbit/s (41TByte/s)의 off-package bandwidth이다. 
이것은 양쪽 edge에서 660 Tbit/s의 bi-directional bandwidth이다. 
이것은 23.6 Tbit/s의 off-package BW만 가진 Blackwell과 비교되며, 이는 약 0.4 Tbit/s/mm의 shoreline density에 해당하며, 이는 큰 차이이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/85cd24a9-c3cf-4239-a2f6-41ef22eff8d0_1254x946.png}
\caption{Source: SemiAnalysis}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/7d2ac70e-d96d-42c9-b8be-3c186d60597e_2392x570.png}
\caption{Source: SemiAnalysis}
\end{figure}

\begin{tcolorbox}[breakable,colback=green!5!white,colframe=green!75!black,title=\textbf{기술 용어: Wide I/O vs SerDes - Paradigm Shift}]
\setlength{\parskip}{2pt}

\textbf{1. SerDes vs Wide I/O의 근본적 차이}

\textbf{SerDes (Serializer/Deserializer)}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{개념}: 병렬 데이터를 직렬로 변환하여 \textbf{소수의 고속 lane}으로 전송
\item \textbf{예시}: 32 bit 데이터를 1개의 224G lane으로 serialize
\item \textbf{장점}: 적은 wire 수, 긴 거리 지원
\item \textbf{단점}: 복잡한 회로, 높은 전력, 고주파수 신호
\end{itemize}

\textbf{Wide I/O (Parallel Interface)}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{개념}: \textbf{다수의 저속 lane}으로 병렬 전송
\item \textbf{예시}: 32 bit 데이터를 32개의 저속 lane으로 동시 전송
\item \textbf{장점}: 간단한 회로, 낮은 전력, 저주파수 신호
\item \textbf{단점}: 많은 wire 필요, 짧은 거리만 지원
\end{itemize}

\vspace{0.1cm}

\textbf{2. UCIe (Universal Chiplet Interconnect Express)}

\textbf{UCIe란?}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item Chiplet 간 interconnect를 위한 \textbf{industry standard}
\item Wide I/O 기반 (parallel interface)
\item Intel, AMD, ARM, TSMC 등 주요 업체 참여
\end{itemize}

\textbf{UCIe-A (Advanced Package)}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{목표}: Interposer 기반 (2.5D/3D packaging)
\item \textbf{거리}: Sub-2mm (매우 짧음)
\item \textbf{Bandwidth density}: \textbf{최대 10 Tbps/mm}
\item \textbf{전력 효율}: 매우 우수 (저주파수)
\end{itemize}

\textbf{UCIe-S (Standard Package)}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{목표}: Organic substrate 기반
\item \textbf{거리}: 수 mm ~ 수 cm
\item \textbf{Bandwidth density}: 약 1.8 Tbps/mm
\item \textbf{전력 효율}: 여전히 SerDes보다 좋음
\end{itemize}

\vspace{0.1cm}

\textbf{3. Bandwidth Density 비교}

\textbf{Reticle-sized chip의 shoreline} (약 120mm):

\begin{tabular}{|l|c|c|}
\hline
\textbf{Technology} & \textbf{Density} & \textbf{Total BW} \\
\hline
224G SerDes (LR) & 0.4 Tbps/mm & 48 Tbps \\
\hline
UCIe-S (substrate) & 1.8 Tbps/mm & 216 Tbps \\
\hline
UCIe-A (interposer) & 10 Tbps/mm & \textbf{1,200 Tbps} \\
\hline
\end{tabular}

\textbf{Blackwell 실제}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item Off-package BW: 23.6 Tbps (NVLink 5.0)
\item Shoreline density: 약 0.4 Tbps/mm
\item \textbf{UCIe-A 대비 25배 차이!}
\end{itemize}

\vspace{0.1cm}

\textbf{4. 왜 Wide I/O가 CPO에 최적인가?}

\textbf{거리가 문제가 아니므로}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item CPO에서 electrical path는 2-5mm (매우 짧음)
\item Wide I/O의 단점(짧은 거리)이 문제 없음
\item \textbf{장점만 취함}: 높은 bandwidth density + 낮은 전력
\end{itemize}

\textbf{SerDes의 불필요한 복잡도 제거}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item Serialization/Deserialization 회로 제거
\item Clock recovery 회로 단순화 (clock forwarding 사용)
\item Equalization 최소화 (신호 깨끗함)
\end{itemize}

\textbf{전력 효율 극대화}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item 저주파수 신호 → 낮은 전력
\item 간단한 회로 → 작은 면적
\item 예: UCIe-A는 224G SerDes 대비 전력 50-70\% 절감 추정
\end{itemize}

\vspace{0.1cm}

\textbf{5. 왜 Broadcom/NVIDIA는 SerDes를 고집하는가?}

\textbf{Hybrid 전략}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{In-rack copper}: 여전히 필요 (CPC)
  \begin{itemize}[itemsep=0pt]
  \item Copper는 1-2m 거리에서 SerDes 필요
  \item Wide I/O는 copper에 부적합
  \end{itemize}
\item \textbf{CPO/Optics}: Wide I/O 사용
\item \textbf{동일 silicon에 둘 다 지원}: 여러 tape-out 피하기
\end{itemize}

\textbf{점진적 migration}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item CPO 채택이 느릴 것으로 예상
\item 당분간 SerDes 기반 copper 계속 사용
\item Wide I/O 전용 chip은 niche market
\end{itemize}

\textbf{SerDes IP 활용}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item NVIDIA/Broadcom은 세계 최고 SerDes IP 보유
\item 기존 투자 활용
\end{itemize}

\vspace{0.1cm}

\textbf{6. 미래: Wide I/O의 승리?}

\textbf{Short term (2025-2027)}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item SerDes 기반 CPO가 주류
\item UCIe-S (substrate) 일부 사용
\end{itemize}

\textbf{Medium term (2027-2030)}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item UCIe-S 더 널리 사용
\item Interposer 기반 CPO 등장 시작
\end{itemize}

\textbf{Long term (2030+)}:
\begin{itemize}[itemsep=1pt,parsep=0pt,topsep=2pt]
\item \textbf{UCIe-A (interposer) 기반 CPO가 ultimate goal}
\item 10 Tbps/mm bandwidth density
\item SerDes는 copper-only application으로 제한
\end{itemize}

\end{tcolorbox}

물론 이것은 like-for-like 비교가 아닌데, 이러한 off-package PHY는 긴 거리를 구동하는 데 필요하기 때문이다. 
어떤 것이든 이것이 설명되고 있는 바로 그 point이다: CPO를 사용하면 signal이 electrically하게 긴 거리로 구동되지 않기 때문에 reach는 더 이상 고려 사항이 아니다. 
10 Tbit/s/mm의 bandwidth density에서 bottleneck은 더 이상 electrical interface에 있지 않고 link의 다른 부분에 있으며, 
즉 반대편의 fiber에서 얼마나 많은 bandwidth가 escape할 수 있는지에 있다.

이 제약에 도달하는 것은 오늘날의 현실에서 매우 먼 end state이며 OE는 host와 interposer를 공유해야 할 것이다. 
Interposer 자체에 CPO를 통합하는 것은 substrate에 OE를 안정적으로 통합하는 것보다 roadmap에서 훨씬 더 멀리 있다. 
Substrate의 PHY 성능은 물론 열등하며 UCIe-S는 약 1.8Tbit/s/mm의 shoreline density를 제공한다.
이것은 여전히 우리가 믿는 224G SerDes가 \textasciitilde{}0.4Tbit/s/mm에서 제공하는 것에 비해 상당한 향상이다.

그러나 Broadcom과 NVIDIA는 wide interface가 제공하는 이점에도 불구하고 roadmap에서 electrical SerDes를 고집하고 있다. 
주요 이유는 그들이 여전히 SerDes를 확장할 수 있다고 믿고 optics의 채택이 느리기 때문에 특히 copper를 위해 설계해야 한다고 믿기 때문이다. 
또한 hybrid co-packaged copper와 co-packaged optics 솔루션이 계속 유지될 가능성이 더 높아 보이며, 이는 그들이 둘 다를 최적화해야 함을 요구한다. 
이 접근 방식은 서로 다른 솔루션에 대한 여러 tape-out의 필요성을 제거하기 위해 취해진다.

\subsection{Link Resiliency}
Link resiliency와 reliability는 CPO 기술의 또 다른 매우 중요한 추진 요인이다. 
대규모 AI cluster에서 link downtime은 전체 cluster availability에 상당한 기여를 하며, link availability와 stability의 작은 개선조차도 infrastructure 투자에 대한 큰 수익을 제공한다.

오늘날 pluggable module을 사용하는 1M link에 접근하는 대규모 AI cluster에서는 하루에 수십 건의 link interruption이 발생할 수 있다. 
이 중 일부는 component failure 또는 hardware 품질의 결과로 인한 \textbf{hard} failure이며, 
많은 것들은 pluggable 기반 솔루션의 고유한 복잡성과 가변성에서 비롯되는 다양한 root cause의 결과로 인한 \textbf{soft} failure이다. 
Signal integrity 문제 및 variation, connector 및 wirebond 품질, component 및 pin의 contamination, noise injection 및 기타 transient effect를 포함하되 이에 국한되지 않는 긴 failure mode tail이 있다. 
Component failure와의 correlation은 거의 없다. link failure로 인해 반품되는 optical module의 80\%는 \textbf{no trouble found}이다.
CPO는 다음을 통해 대규모 AI network에서 high-speed signal path의 고유한 복잡성과 가변성을 크게 줄인다:

\begin{itemize}
  \item Optical interface의 component 수를 크게 줄인다. photonic level과 chip/package level 모두에서의 높은 수준의 통합은 중요한 high-speed assembly의 복잡성을 줄이고 system level에서 reliability와 yield를 향상시킨다. E/O interface의 수도 감소하여 각 interface에서 발생하는 power loss를 최소화한다

  \item Host ASIC (예: switch)과 optical engine 사이의 host electrical interface의 signal integrity를 크게 개선한다. insertion loss, reflection 및 기타 non-linear impairment는 매우 잘 정의되고 결정적인 design rule 및 manufacturing tolerance를 가진 first level package에 optical engine을 packaging함으로써 크게 감소한다

  \item Switch 전체에서 high-speed signal path의 port-to-port variation을 줄여 DSP signal processing, host 및 module equalization, host 및 module firmware 및 link optimization algorithm에 overhead와 복잡성을 추가한다. 모든 pluggable module 솔루션과 host SerDes는 복잡성과 failure point로 이어지는 port별 성능의 이러한 variation을 수용하도록 설계되어야 한다

\end{itemize}
Optical link provisioning의 human 요소를 제거한다. 
CPO switch 또는 optical engine은 공장에서 완전히 조립되고 테스트된 \textbf{`known-good'} 상태로 나오며 switch의 optics를 provision하기 위한 상당한 현장 작업이 필요하지 않으며, 
이는 설치 variation, 손상, contamination 및 system과 optical module 간의 compatibility 문제로 이어질 수 있다.
















\newpage{}
\section{Part 3: Bringing CPO to Market and deployment challenges}

\subsection{CPO optical engine manufacturing considerations and go to market}
CPO는 아직 광범위한 채택에 상응하는 수량으로 제조되지 않았다. 
Broadcom은 Bailly 및 Humboldt switch를 포함하는 CPO를 특징으로 하는 production system을 출하한 유일한 vendor였지만, 이제 NVIDIA도 합류하고 있다. 
이것들은 매우 낮은 volume으로 출하되었다. 
CPO는 많은 새로운 manufacturing process와 상당한 manufacturability 과제를 도입한다. 
당연히 supply chain의 미성숙과 reliability에 대한 데이터 부족을 고려할 때, 고객들도 기술 채택에 뛰어드는 것을 꺼린다.

CPO가 견인력을 얻으려면 업계 leader가 이러한 제품을 출하하는 데 투자하여 supply chain이 scalable한 manufacturing 및 testing process를 개발하도록 추진해야 한다. 
NVIDIA가 뛰어들고 있으며 그들의 의도는 supply chain을 준비시키고, 문제를 식별하고 해결하며, 우리가 생각하기에 \textbf{killer} application이 될 것을 위해 datacenter operator를 준비시키는 것이다.
Scale-up networking. CPO와 관련하여 집중해야 할 몇 가지 핵심 component와 고려사항이 있으며, 다음의 모든 것이 성능과 manufacturability에 영향을 미친다. 


\begin{enumerate}
  \item Host and Optical Engine packaging

  \item Fibers and Fiber coupling

  \item Laser Sources and Wavelength Multiplexing

  \item Modulator Type

\end{enumerate}

\subsection{Host and Optical Engine Packaging}

\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: CPO Packaging의 핵심 구성요소}]

\textbf{PIC (Photonic Integrated Circuit)}: Photodetector와 modulator를 포함하는 optical component가 집적된 회로. Silicon photonics process를 통해 제조되며, 빛의 생성, 변조, 전송을 담당한다.
\vspace{2mm}

\textbf{EIC (Electric Integrated Circuit)}: Driver와 Transimpedance Amplifier (TIA)를 포함하는 electrical 회로. CMOS process로 제조되며, electrical signal을 optical signal로 변환하거나 그 역을 수행한다.
\vspace{2mm}

\textbf{Monolithic Integration}: PIC와 EIC를 동일한 silicon wafer에 제조하는 방식. Parasitic과 latency가 가장 낮지만, 약 35nm 미만의 geometry에서 한계를 가진다.
\vspace{2mm}

\textbf{Heterogeneous Integration}: SiPho process로 PIC를 제조하고 advanced packaging을 통해 CMOS wafer의 EIC와 통합하는 방식. 3D integration을 통해 최고의 성능 제공.
\vspace{2mm}

\textbf{Hybrid Bonding}: Die-to-wafer 또는 wafer-to-wafer bonding 기술로, bump 없이 직접 연결하여 trace 길이를 최소화하고 parasitic을 감소시킨다. TSMC SoIC가 대표적.
\vspace{2mm}

\textbf{FAU (Fiber Array Unit)}: 여러 optical fiber를 정밀하게 정렬하여 optical engine에 연결하는 unit. Coupling efficiency와 manufacturing yield에 중요한 영향을 미친다.

\end{tcolorbox}

이름에서 알 수 있듯이 \textbf{Co-packaged optics}는 근본적으로 packaging 및 assembly 과제이다.
optical engine은 optical 및 electrical component를 모두 가지고 있다. 
Photodetector와 modulator는 \textbf{Photonic Integrated Circuit (PIC)}에 포함된 optical component이다. 
Driver와 Transimpedance Amplifier는 \textbf{Electric Integrated Circuit (EIC)}에 포함된 electrical 회로이다. 
OE가 작동하려면 PIC와 EIC가 통합되어야 한다. 이 PIC--EIC 통합을 달성하기 위해 여러 packaging 방법이 존재한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/6500f472-56b6-4b66-826a-64bb60e09f94_1024x465.jpg}
\caption{Source: ID TechEx}
\end{figure}

\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: Parasitic과 신호 무결성}]

\textbf{Parasitic}: 회로 설계 시 의도하지 않았지만 물리적 구조로 인해 필연적으로 발생하는 원치 않는 전기적 특성. Trace, via, interconnect 등의 물리적 구조에서 발생하며, 신호의 품질과 전력 효율을 저하시킨다. 거리가 멀수록, 구조가 복잡할수록 parasitic이 증가한다.

\vspace{2mm}
\textbf{Parasitic Capacitance}: 인접한 도체 사이에 의도치 않게 형성되는 capacitance. 신호 전환 시 충전/방전에 추가 전력이 소모되며, signal integrity를 저하시키고 switching speed를 늦춘다. CPO에서 PIC-EIC 간 거리가 가까울수록 parasitic capacitance가 감소한다.
\vspace{2mm}

\textbf{Trace Length (배선 길이)}: 신호가 이동하는 physical path의 길이. Trace가 길수록 parasitic capacitance, inductance, resistance가 모두 증가하여 signal integrity와 전력 효율이 저하된다. 3D integration과 hybrid bonding은 trace 길이를 최소화하여 parasitic을 감소시키는 핵심 기술이다.

\end{tcolorbox}

Optical engine은 PIC와 EIC를 동일한 silicon wafer에 제조함으로써 monolithic일 수 있다. 
Monolithic integration은 parasitic, latency 및 전력 측면에서 가장 우아한 접근 방식이다. 
이것이 Ayar Labs가 2세대 TeraPHY chiplet에 대해 취한 접근 방식이다 (하지만 그들의 next-gen chiplet은 TSMC COUPE로 pivot한다). 
GlobalFoundries, Tower 및 Advanced Micro Foundry는 monolithic CMOS 및 SiPho process를 제공할 수 있는 foundry이다. 
그러나 photonics process는 전통적인 CMOS처럼 scale할 수 없기 때문에 monolithic process는 약 35nm 미만의 geometry에서 멈춘다. 
이는 특히 CPO 시스템에서 예상되는 더 높은 lane speed를 고려할 때 EIC의 capability를 제한한다. 
고유한 단순성과 우아함에도 불구하고 이는 monolithic integration을 scaling에 대한 dealbreaker로 만든다. 
이것이 Ayar Labs도 추가 scaling을 허용하기 위해 heterogeneously integrated OE로 roadmap을 이동시키고 있는 이유이다.

Heterogenous integration은 SiPho process를 사용하여 PIC를 제조하고 advanced packaging을 통해 CMOS wafer의 EIC와 통합하는 것을 포함하는 주류 접근 방식이 되고 있다. 
다양한 packaging 솔루션이 존재하며, 더 advanced한 packaging 솔루션이 더 높은 성능을 제공한다. 
그 중에서 3D integration은 최고의 bandwidth와 energy efficiency를 제공한다. 
EIC와 PIC 통신에 있어서 큰 문제는 성능을 저하시키는 parasitic이다.
Trace 길이를 줄이면 parasitic이 크게 감소하고 따라서 coupling efficiency가 증가한다.
3D integration은 bandwidth와 전력 관점에서 CPO의 성능 목표에 도달하는 유일한 방법이다.

\subsection{TSMC COUPE is emerging as the integration option of choice}

\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: Advanced Packaging 및 Silicon Photonics 기술}]

\textbf{SiPho (Silicon Photonics)}: Silicon 기반 재료를 사용하여 photonic component를 제조하는 기술. 기존 CMOS 제조 인프라를 활용할 수 있어 대량 생산이 가능하며, waveguide, modulator, photodetector 등 optical component를 silicon wafer에 집적한다. SOI (Silicon-on-Insulator) wafer를 사용하여 빛의 전파와 제어를 구현한다.

\vspace{2mm}

\textbf{Die-to-Wafer Hybrid Bonding}: 개별 die를 전체 wafer에 직접 bonding하는 advanced packaging 기술. Micro-bump나 solder bump 없이 copper와 dielectric을 직접 접합하는 bumpless 방식으로, trace 길이를 극도로 단축하여 parasitic을 최소화한다. TSMC SoIC가 대표적이며, monolithic integration 다음으로 가장 우수한 electrical 특성을 제공한다.

\vspace{2mm}

\textbf{Wafer-to-Wafer Bonding}: 전체 wafer를 다른 전체 wafer에 bonding하는 방식. Die-to-wafer bonding보다 공정이 단순하지만, yield 측면에서 불리할 수 있다. 한 wafer의 defect가 bonding된 전체 wafer pair에 영향을 미치기 때문이다.

\vspace{2mm}

\textbf{Bump vs Bumpless}: 전통적 bump 기반 연결은 solder ball이나 copper pillar를 통해 chip을 연결하며, 수십 마이크론의 pitch를 가진다. Bumpless (hybrid bonding)는 bump 없이 직접 copper-to-copper 및 dielectric-to-dielectric bonding을 수행하며, 수 마이크론 pitch로 훨씬 높은 interconnect density와 낮은 parasitic을 달성한다. Iso-power에서 23배 이상의 bandwidth density 향상을 제공한다.

\vspace{2mm}

\textbf{SOI (Silicon-on-Insulator)}: Silicon layer 아래에 insulator (산화물) layer가 있는 wafer 구조. Silicon photonics에서 광신호의 confinement를 향상시켜 더 효율적인 waveguide와 optical component 제작을 가능하게 한다. TSMC COUPE는 N65 SOI node를 PIC 제조에 사용한다.

\end{tcolorbox}
\vspace{2mm}

\begin{tcolorbox}[breakable,colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: TSMC COUPE - CPO Ecosystem의 새로운 표준}]

TSMC가 역사적으로 silicon photonics에서 제한적 존재감을 가졌음에도 불구하고 \textbf{COUPE (COmpact Universal Photonic Engine)}가 업계 표준으로 급부상하고 있다. 이는 세 가지 핵심 요인에 기인한다:

\vspace{2mm}
\textbf{1. 기술적 우위}: TSMC는 유일하게 합리적 scale에서 die-to-wafer hybrid bonding을 성공적으로 입증한 foundry이다. SoIC (System-on-Integrated-Chips) bumpless interface는 monolithic 통합 다음으로 가장 짧은 trace 길이를 제공하며, iso-power에서 bump 기반 통합보다 23배 이상의 bandwidth density를 달성한다.
\vspace{2mm}

\vspace{2mm}
\textbf{2. 통합 ecosystem}: COUPE는 단순한 manufacturing process가 아닌 완전한 솔루션이다. EIC (N7 node), PIC (SOI N65 node), 3D stacking, optical I/O design, 완전한 PDK 및 EDA workflow까지 포함한다. 이는 설계자가 fragmented supply chain을 관리할 필요 없이 end-to-end 솔루션을 얻을 수 있음을 의미한다.
\vspace{2mm}

\vspace{2mm}
\textbf{3. Vendor lock-in 효과}: COUPE를 채택하면 사실상 TSMC 제조 PIC를 사용하기로 commit하는 것이다. TSMC는 다른 foundry의 SiPho wafer를 package하지 않기 때문이다. 이는 TSMC가 value chain의 더 많은 부분을 포착할 수 있게 한다.
\vspace{2mm}

결과적으로 NVIDIA, Broadcom, Ayar Labs 등 주요 player들이 기존 solution에서 COUPE로 pivot하고 있다. 특히 Broadcom은 SPIL FOWLP 접근 방식이 parasitic capacitance로 인해 lane당 100G를 넘어 scaling할 수 없다는 것을 인식하고 COUPE로 전환했다. 이는 단순히 하나의 기술 옵션이 아닌, CPO 산업의 \textbf{de facto standard}가 되어가고 있음을 시사한다.

\end{tcolorbox}

TSMC는 fabless giant와 startup 모두를 위한 차세대 OE의 foundry 파트너로 선택되어 앞서 나가고 있다. 
CPO endpoint를 특징으로 하는 첫 번째 high-volume 제품은 \textbf{COmpact Universal Photonic Engine (COUPE)} 이름으로 소개되고 있다. 
여기에는 EIC 및 PIC의 제조와 TSMC의 COUPE 솔루션에 따른 heterogeneous integration이 포함된다. 
NVIDIA는 GTC 2025에서 COUPE optical engine을 자랑스럽게 전시했으며, 이것들이 출하될 첫 번째 COUPE 제품이 될 것이다. 
Broadcom도 다른 supply chain 파트너와 함께 기존 세대의 OE를 가지고 있음에도 불구하고 미래 roadmap을 위해 COUPE를 채택하고 있다. 
앞서 언급한 것처럼 이전에 monolithic optical engine을 위해 Global Foundries의 Fotonix platform에 의존했던 Ayar Labs도 이제 roadmap에 COUPE를 가지고 있다.

전통적인 CMOS logic에서의 지배력과 달리 TSMC는 이전에 silicon photonics에서 제한적인 존재감을 가졌으며, 
여기서 Global Foundries와 Tower Semi가 선호되는 foundry 파트너였다. 
그러나 최근 몇 년 동안 TSMC는 photonic capability와 관련하여 빠르게 따라잡고 있다. 
TSMC는 또한 EIC component를 위한 leading edge CMOS logic에서의 의심할 여지 없는 강점과 leading packaging capability를 가져온다.
TSMC는 합리적인 scale에서 die-to-wafer hybrid bonding capability를 성공적으로 입증한 유일한 foundry이며, volume으로 다양한 AMD hybrid bonded chip을 출하했다. hybrid bonding은 PIC와 EIC를 bonding하는 더 성능이 좋은 접근 방식이지만, 상당히 높은 비용이 수반된다. Intel은 유사한 capability를 개발하기 위해 노력하고 있지만 이 기술을 개척하는 데 상당한 과제에 직면했다.

전반적으로 TSMC는 이전에 더 약했던 standalone SiPho capability에도 불구하고 이제 CPO에서 매우 핵심적인 player가 되었다. 
다른 주요 player와 마찬가지로 TSMC는 가능한 한 많은 value chain을 포착하는 것을 목표로 한다. 
TSMC의 COUPE 솔루션을 채택함으로써 고객은 사실상 TSMC 제조 PIC를 사용하기로 commit하는 것인데, 
TSMC는 다른 foundry의 SiPho wafer를 package하지 않기 때문이다. 
많은 CPO 중심 기업들은 실제로 향후 몇 년 동안 TSMC의 COUPE를 go to market 솔루션의 일부로 만드는 쪽으로 결정적으로 pivot했다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/86edb5e2-23f9-426c-acde-0a965fe506ae_1024x576.jpg}
\caption{Source: TSMC}
\end{figure}
\textbf{Die fabrication}: TSMC는 die fabrication을 위한 포괄적인 솔루션 suite를 제공한다. 
EIC는 high-speed optical modulator driver와 TIA를 통합하는 N7 node에서 제조된다. 
또한 wavelength stabilization과 같은 기능을 가능하게 하는 heater controller를 통합한다. 
반면 PIC는 SOI N65 node에서 제조되며, TSMC는 photonic circuit design, photonic layout design 및 verification, 
그리고 photonic circuit의 simulation 및 modeling (RF, noise 및 multi-wavelength와 같은 측면을 다룸)에 대한 광범위한 지원을 제공한다.

EIC와 PIC는 TSMC-SoIC-bond process를 사용하여 bonding된다. 
앞서 언급했듯이 더 긴 trace 길이는 더 많은 parasitic을 의미하며, 이는 성능을 저하시킨다. 
TSMC의 SoIC는 monolithic하지 않으면서 가능한 가장 짧은 trace 길이를 제공하는 bumpless interface이므로 EIC와 PIC를 heterogeneously하게 통합하는 가장 성능이 좋은 가능한 방법이다. 
아래에 표시된 것처럼 iso-power에서 SoIC 기반 OE는 bump로 통합된 OE보다 23배 이상의 bandwidth density를 제공한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/34ef9a17-7602-4097-8a5c-bc53cc8515f8_1024x471.jpg}
\caption{Source: TSMC}
\end{figure}
COUPE는 전체 optical engine design 및 integration process를 지원한다. optical I/O의 경우, wafer 또는 chip level에서 micro-lens의 통합을 가능하게 하는 µLens design을 지원하며, mirror, µLens, grating coupler (GC) 및 reflector를 다루는 optical I/O path simulation을 지원한다. 3D stacking의 경우, 3D floorplanning, SoIC-X/TDV/C4 bump layout implementation, interface physical checking, 그리고 high-frequency channel model extraction 및 simulation을 지원한다. seamless한 개발을 보장하기 위해 회사는 COUPE design 및 verification을 위한 완전한 PDK 및 EDA workflow를 제공하여 designer가 기술을 효율적으로 구현할 수 있도록 한다.

\textbf{Coupling}: 나중에 더 자세히 설명하겠지만 두 가지 주요 coupling 방법이 있다: \textbf{Grating Coupling (GC)}과 \textbf{Edge Coupling (EC)}. 
COUPE는 GC와 EC 모두에 대해 PIC bumpless stacking 구조에 하나의 공통 EIC를 사용한다. 그러나 COUPE-GC 구조는 Silicon lens (Si lens)와 MR (metal reflector)를 독특하게 사용하는 반면, COUPE-EC는 고유하게 EC facet (fiber로 EC를 종료하기 위한)를 가질 것이다. GC의 경우 Si lens는 770µm silicon carrier (Si-carrier)에 설계되고 MR은 optical 성능에 필요한 최적화 dielectric layer와 함께 GC 바로 아래에 배치된다. 그런 다음 Si-carrier는 CoW (chip-on-wafer) wafer에 WoW (wafer-on-wafer) bonding된다.

\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: Optical Coupling 방식과 구성요소}]

\textbf{Grating Coupling (GC)}: Optical fiber와 PIC 사이에서 빛을 수직 방향으로 결합하는 방식. Waveguide 표면에 주기적인 grating 구조를 만들어 빛의 방향을 90도 전환한다. Wafer level testing이 용이하고 정렬 tolerance가 높아 manufacturing이 상대적으로 쉽지만, edge coupling 대비 coupling efficiency가 낮고 wavelength dependency가 크다. TSMC COUPE-GC는 silicon lens와 metal reflector를 사용하여 efficiency를 향상시킨다.

\vspace{2mm}

\textbf{Edge Coupling (EC)}: Optical fiber와 PIC를 chip의 edge에서 수평 방향으로 직접 결합하는 방식. Fiber와 waveguide의 mode profile을 matching시켜 빛을 전달한다. Grating coupling 대비 높은 coupling efficiency (일반적으로 1-2 dB loss)와 넓은 wavelength range를 제공하지만, sub-micron 수준의 정밀한 alignment가 필요하고 wafer level testing이 어렵다. High-performance application에서 선호된다.

\vspace{2mm}

\textbf{Silicon Lens (Si lens)}: Grating coupling에서 빛을 집속(focusing)시키는 silicon 재질의 micro-lens. COUPE-GC에서는 770µm silicon carrier에 설계되어, fiber에서 나오는 빛을 grating coupler로 효율적으로 전달하거나 그 역 방향으로 빛을 집속시킨다. Silicon의 높은 refractive index를 활용하여 compact한 크기로 효과적인 focusing을 구현한다.

\vspace{2mm}

\textbf{Metal Reflector (MR)}: Grating coupler 아래에 배치되는 금속 반사층. Grating에서 아래쪽으로 회절된 빛을 위쪽으로 반사시켜 fiber 방향으로 보냄으로써 coupling efficiency를 크게 향상시킨다. 최적화된 dielectric layer와 함께 사용되어 optical 성능을 극대화한다. MR 없이는 빛의 상당 부분이 substrate로 손실된다.

\vspace{2mm}

\textbf{EC Facet}: Edge coupling을 위해 chip의 edge를 정밀하게 가공한 평면. Optical fiber를 직접 연결하기 위해 매우 매끄럽고 수직인 표면이 필요하다. Dicing이나 cleaving을 통해 제작되며, surface roughness와 verticality가 coupling efficiency에 직접적인 영향을 미친다. COUPE-EC는 이러한 고품질 facet을 제공한다.

\vspace{2mm}

\textbf{Coupling Efficiency}: Optical fiber와 PIC 사이에서 전달되는 빛의 비율. Mode mismatch, alignment error, surface quality 등에 의해 손실이 발생한다. 일반적으로 dB 단위로 표현되며, 낮을수록 좋다 (예: 1 dB loss = 79\% efficiency). CPO 시스템에서는 수십 개의 channel이 있으므로 각 channel의 coupling efficiency가 전체 시스템 성능에 중요하다.

\vspace{2mm}

\textbf{CoW (Chip-on-Wafer)}: 개별 die (chip)를 전체 wafer에 bonding하는 공정. Known-good die만 선택하여 bonding할 수 있어 yield가 향상된다. COUPE에서 Si-carrier를 PIC/EIC stack에 bonding할 때 사용된다.

\vspace{2mm}

\textbf{WoW (Wafer-on-Wafer)}: 전체 wafer를 다른 전체 wafer에 bonding하는 공정. Die-to-wafer보다 throughput이 높지만, 한 wafer의 defect가 전체 bonding pair에 영향을 주므로 두 wafer 모두 높은 yield를 가져야 경제적이다.

\vspace{2mm}

\textbf{Insertion Loss}: Optical signal이 component나 interface를 통과할 때 발생하는 power 손실. Coupling, connector, waveguide 등 각 단계에서 발생하며, dB 단위로 측정된다. CPO 시스템에서는 FAU, coupling interface 등 여러 단계의 insertion loss를 최소화하는 것이 중요하다. 높은 insertion loss는 signal quality 저하와 power penalty를 초래한다.

\end{tcolorbox}

\vspace{2mm}

\begin{tcolorbox}[breakable,colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: Optical Coupling - Fiber와 Chip을 연결하는 핵심 기술}]

\textbf{Coupling의 개념}

CPO 시스템에서 "coupling"은 optical fiber (빛이 장거리를 이동하는 매체)와 PIC의 waveguide (chip 내에서 빛이 이동하는 경로)를 광학적으로 연결하는 과정을 의미한다. 이는 전기 신호에서 connector나 PCB trace가 칩을 연결하는 것과 유사하지만, 빛의 파동 특성 때문에 훨씬 더 정밀하고 복잡하다. Fiber의 mode (빛의 분포 패턴)와 waveguide의 mode가 다르기 때문에, 이 변환 과정에서 불가피하게 일부 빛이 손실된다. CPO의 성능과 신뢰성은 이 coupling이 얼마나 효율적으로 이루어지는지에 크게 좌우된다.

\vspace{2mm}




\textbf{Grating Coupling (GC) 동작 과정}

\begin{enumerate}
\item \textbf{광신호 입력}: Optical fiber에서 나온 빛이 FAU (Fiber Array Unit)를 통해 정렬되어 Si lens로 입사된다.

\item \textbf{집속 (Focusing)}: 770µm silicon carrier에 형성된 Si lens가 diverging하는 빛을 focusing하여 grating coupler로 집중시킨다. Silicon의 높은 refractive index (약 3.5)를 활용하여 compact한 크기로 강력한 focusing을 달성한다.

\item \textbf{방향 전환 및 mode 변환}: Grating coupler (주기적 구조를 가진 waveguide 표면)가 수직으로 입사된 빛을 수평 방향으로 회절시키면서 동시에 fiber mode를 waveguide mode로 변환한다. 이 과정에서 일부 빛은 위아래로 분산된다.

\item \textbf{반사를 통한 효율 향상}: Grating 아래에 배치된 Metal Reflector (MR)가 아래쪽으로 회절된 빛을 다시 위로 반사시켜 waveguide로 보낸다. 최적화된 dielectric layer가 interference pattern을 조절하여 efficiency를 극대화한다. MR 없이는 30-50\%의 빛이 substrate로 손실될 수 있다.

\item \textbf{Waveguide 전달}: 변환된 빛이 PIC의 silicon waveguide를 통해 modulator나 photodetector 등 optical component로 전달된다.

\item \textbf{조립}: Si-carrier는 CoW 방식으로 PIC/EIC stack에 배치된 후, WoW bonding으로 최종 고정된다.
\end{enumerate}

\textbf{GC의 장점}: Wafer 표면 어디서든 접근 가능, wafer-level testing 용이, alignment tolerance 높음 (수 마이크론), 대량 생산에 유리. \textbf{단점}: 일반적으로 3-5 dB insertion loss, wavelength에 민감 (약 40nm bandwidth), polarization dependent.

\vspace{2mm}

\textbf{Edge Coupling (EC) 동작 과정}

\begin{enumerate}
\item \textbf{광신호 입력}: Optical fiber가 chip의 edge에 정밀하게 정렬된다. Sub-micron (1 마이크론 미만) 수준의 alignment 정확도가 필요하다.

\item \textbf{직접 mode 변환}: Fiber에서 나온 빛이 EC facet (정밀 연마된 chip edge)를 통해 waveguide로 직접 입사된다. Spot size converter (점진적으로 폭이 변하는 waveguide 구조)가 fiber의 큰 mode (약 10 µm)를 waveguide의 작은 mode (약 0.5 µm)로 단열적(adiabatic)으로 변환한다.

\item \textbf{Waveguide 전달}: 변환된 빛이 바로 PIC의 waveguide를 통해 optical component로 전달된다.
\end{enumerate}

\textbf{EC의 장점}: 낮은 insertion loss (일반적으로 1-2 dB), 넓은 wavelength range (100nm 이상), polarization 제어 가능. \textbf{단점}: Sub-micron alignment 필요, wafer-level testing 불가, facet quality에 민감, chip edge에만 배치 가능하여 I/O density 제한.

\end{tcolorbox}





\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/ee18cf78-ed61-4ca1-baad-f672bcae88b9_4089x1425.jpg}
\caption{Source: TSMC}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/73634536-0ac6-485b-9323-0de1bdbb8ce2_2992x1231.jpg}
\caption{Source: TSMC}
\end{figure}
\textbf{Fiber Attach Unit (FAU)}: FAU는 COUPE의 optical path에 따라 co-design되어야 한다. FAU의 목적은 Si lens에서 optical fiber로 빛을 낮은 insertion loss로 coupling하는 것이다. I/O 수가 증가함에 따라 manufacturing 난이도가 증가하지만, 산업이 특정 표준을 준수하면 개발 시간과 비용이 감소한다. 전반적으로 각 component는 최상의 optical 성능을 달성하기 위해 최적화된 design이 필요하다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/43a2a500-e7f0-4846-8cd1-62e86e91a3e0_3843x1248.jpg}
\caption{Source: TSMC}
\end{figure}
\textbf{Product roadmap}: COUPE의 첫 번째 iteration은 substrate에 있는 optical engine이 될 것이며 궁극적인 목표는 interposer에 OE를 배치할 수 있는 것이다. 
Interposer는 훨씬 더 많은 I/O density를 제공하므로 OE와 ASIC PHY 사이에 더 큰 bandwidth를 가능하게 하며, 
개별 OE가 각각 최대 12.8Tbit/s의 bandwidth를 가질 가능성이 있어 약 4 Tbit/s/mm로 환산된다. 
Interposer를 통합하는 데 있어 과제는 OE를 수용하기 위해 interposer 크기 (package substrate보다 비쌈)를 확장하는 것이다.

\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: Advanced Packaging 기술 - FOWLP와 TMV}]

\textbf{FOWLP (Fan-Out Wafer-Level Packaging)}: Die를 재배치하여 원래 wafer 크기보다 더 큰 I/O area를 제공하는 advanced packaging 기술. Die들을 temporary carrier에 face-down으로 배치한 후 epoxy mold compound로 encapsulation하고, RDL (Redistribution Layer)을 형성하여 I/O를 "fan-out" (확장)시킨다. 전통적인 substrate나 interposer 없이 wafer-level에서 처리할 수 있어 cost-effective하지만, vertical interconnect 길이가 길어져 high-speed application에서 한계가 있다.

\vspace{2mm}

\textbf{RDL (Redistribution Layer)}: FOWLP에서 die의 I/O pad를 재배치하여 더 넓은 영역으로 확장하는 금속 배선층. 일반적으로 copper와 dielectric layer로 구성되며, fine-pitch die pad를 coarser-pitch BGA (Ball Grid Array)로 연결한다. Multiple RDL layer를 사용하여 복잡한 routing을 구현할 수 있다.

\vspace{2mm}

\textbf{TMV (Through-Mold Via)}: FOWLP에서 mold compound를 관통하여 die와 RDL을 수직으로 연결하는 via. Copper로 채워지며, die의 top surface에서 RDL로 signal을 전달한다. TMV는 일반적으로 수십에서 수백 마이크론의 길이를 가지며, 이는 다음과 같은 문제를 야기한다:

\begin{itemize}
\item \textbf{Parasitic Capacitance}: TMV의 긴 길이와 mold compound와의 interface가 상당한 parasitic capacitance를 생성한다. Via 길이가 길수록, via와 주변 mold/RDL 사이의 capacitance가 증가한다.

\item \textbf{Scalability 한계}: 이러한 parasitic 요소로 lane당 100Gbps를 넘어서는 고속 신호 전송을 어렵게 만든다. Signal이 EIC에서 PIC로 (또는 그 반대로) 전달될 때 TMV를 통과해야 하므로, 각 lane마다 이 parasitic penalty를 겪는다.
\end{itemize}

\vspace{2mm}

\textbf{FOWLP vs Hybrid Bonding (COUPE)}: 

\begin{itemize}
\item \textbf{Interconnect 길이}: FOWLP의 TMV는 수십-수백 µm인 반면, hybrid bonding은 수 µm 이하로 10-100배 짧다.

\item \textbf{Parasitic}: FOWLP는 TMV와 RDL routing으로 인해 높은 parasitic을 가지며, COUPE의 bumpless hybrid bonding은 극도로 낮은 parasitic을 제공한다.

\item \textbf{Bandwidth Density}: Hybrid bonding은 micro-bump pitch (약 40 µm)보다 훨씬 조밀한 pitch (수 µm)를 달성하여 iso-power에서 23배 이상의 bandwidth density를 제공한다.

\item \textbf{Lane Speed Scalability}: FOWLP는 parasitic 제약으로 lane당 약 100Gbps에서 한계에 도달하지만, COUPE는 200Gbps 이상으로 확장 가능하다.

\item \textbf{Cost}: FOWLP는 초기에는 더 cost-effective하지만, 성능 제약으로 인해 high-end CPO application에서는 경쟁력을 잃는다.
\end{itemize}

\end{tcolorbox}

이것이 Broadcom이 SPIL에 의해 개발된 \textbf{Fan-Out Wafer-Level Packaging (FOWLP)} 접근 방식을 사용하여 여러 세대의 CPO를 iterate했음에도 불구하고 CPO 솔루션을 위해 TSMC COUPE로 전환하고 있는 이유이다. 
특히 Broadcom은 미래 switch 및 customer accelerator roadmap을 위해 COUPE에 commit했다. 
우리가 이해하기로는 FOWLP 접근 방식은 과도한 parasitic capacitance로 인해 lane당 100G를 넘어 scaling할 수 없는데, 
electrical signal이 EIC에 도달하기 위해 \textbf{Through-Mold Via (TMV)}를 통과해야 하기 때문이다. 
경쟁력 있는 roadmap을 유지하기 위해 Broadcom은 우수한 성능과 scalability를 제공하는 COUPE로 전환해야 한다. 
이는 TSMC의 기술적 우위를 강조하며, 역사적으로 더 약하다고 여겨졌던 optics domain에서도 승리를 확보할 수 있게 한다.

\begin{tcolorbox}[breakable,colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: FOWLP의 100Gbps Scaling 한계 - TMV Parasitic의 물리적 제약}]

\textbf{문제의 근본 원인: TMV의 구조적 한계}

FOWLP 기반 CPO에서 optical signal과 electrical signal 간 변환은 다음 경로를 거친다:

\begin{enumerate}
\item PIC에서 photodetector가 optical signal을 electrical signal로 변환 (또는 modulator가 그 역을 수행)
\item Electrical signal이 PIC에서 RDL로 이동하기 위해 \textbf{TMV를 수직으로 통과} (수십-수백 µm)
\item RDL을 통해 horizontal routing (추가 parasitic)
\item 다시 TMV를 통해 EIC (Driver 또는 TIA)로 전달
\end{enumerate}

이 architecture에서 TMV는 불가피한 bottleneck이 된다.

\vspace{2mm}

\textbf{TMV Parasitic이 고속 신호에 미치는 영향}

\textbf{1. Parasitic Capacitance의 영향}:
\begin{itemize}
\item TMV는 cylindrical conductor로, via와 주변 mold compound 및 ground plane 사이에 capacitance가 형성된다.
\item 일반적인 TMV (직경 40-60 µm, 길이 100-200 µm)의 parasitic capacitance는 약 50-100 fF (femtofarad) 수준이다.
\item 이 capacitance는 RC time constant를 증가시켜 signal rise/fall time을 늦춘다: \(\tau = RC\)
\item 100Gbps에서 bit period는 10 ps (picosecond)이다. Parasitic이 크면 signal이 한 bit period 내에 완전히 switching되지 못한다.
\end{itemize}

\textbf{2. Power Consumption 증가}:
\begin{itemize}
\item High-speed switching에서 power는 주로 capacitance 충전/방전에 소모된다: \(P = C \cdot V^2 \cdot f\)
\item Parasitic capacitance가 크면 동일한 data rate에서 더 많은 power가 필요하다.
\item Lane당 100Gbps를 넘어서면, TMV parasitic을 driving하는 데 필요한 power가 급격히 증가하여 CPO의 energy efficiency 목표를 달성할 수 없다.
\end{itemize}

\textbf{3. Signal Integrity 저하}:
\begin{itemize}
\item TMV의 parasitic inductance와 capacitance가 결합되어 resonance를 형성하고 signal ringing을 유발한다.
\item 긴 TMV path는 impedance mismatch를 일으켜 reflection을 증가시킨다.
\item 200Gbps 이상의 속도에서는 eye diagram이 닫혀 reliable한 data detection이 불가능해진다.
\end{itemize}

\vspace{2mm}

\textbf{COUPE Hybrid Bonding의 해결책}

TSMC COUPE는 이 문제를 근본적으로 해결한다:
\begin{itemize}
\item \textbf{극도로 짧은 interconnect}: Hybrid bonding은 수 µm 길이로 TMV 대비 10-100배 짧다.
\item \textbf{Parasitic 감소}: 짧은 길이와 작은 pitch로 parasitic capacitance가 수 fF 수준으로 감소한다.
\item \textbf{Bandwidth density}: Bumpless interface가 높은 interconnect density를 제공하여 병렬 lane 수를 늘릴 수 있다.
\item \textbf{Lane speed scalability}: 낮은 parasitic으로 lane당 200Gbps 이상으로 확장 가능하다.
\end{itemize}

이것이 Broadcom이 SPIL FOWLP에서 검증된 supply chain을 가지고 있음에도 불구하고, 미래 roadmap을 위해 TSMC COUPE로 전환을 결정한 핵심 이유이다. 단순한 incremental improvement가 아닌, 근본적인 physical limitation을 극복하기 위한 architecture 전환이다.

\end{tcolorbox}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/52efaf06-fa1a-4c3d-95b3-c4510f59128c_1312x738.jpg}
\caption{Source: Broadcom}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/d77912f3-8fea-4219-9960-ac4ff1ae63b0_1302x738.png}
\caption{Source: Broadcom}
\end{figure}

\subsection{Packaging the OEs with the host}

OE 자체는 substrate에 배치되며, 그 후 substrate는 host package에 flip-chip bonding된다. 
OE를 co-packaging하기 위해서는 많은 package 면적이 필요하다. 
이는 배치되는 위치에 따라 package substrate 또는 interposer를 상당히 확대해야 함을 필요로 한다.
NVIDIA의 Spectrum-X Photonics switch ASIC package의 경우 substrate는 110mm x 110mm를 측정할 것이다. 
참고로 이것은 그 자체로 매우 큰 chip인 70mm x 76mm의 Blackwell package와 비교된다.

추가로 substrate에 더 많은 element를 부착하는 것은 yield 과제를 제시한다. 
다시 Spectrum-X의 경우 36개의 known good OE가 먼저 substrate에 flip-chip bonding되어야 한다. 
CoWoS assembly를 완료하기 위한 \textbf{on Substrate} 단계를 위해 interposer module을 bonding하기 전에.

마찬가지로 interposer의 경우 훨씬 더 큰 interposer를 제조해야 하는 필요성은 비용이 많이 들며 훨씬 더 많은 element를 bonding해야 하므로 yield 과제를 제시한다. 
추가로 이러한 과제는 \textbf{interposer/substrate 크기가 확장됨에 따라 더욱 두드러지는 warpage 문제로 인해 악화된다.}


\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: Chip Bonding 기술의 이해}]

  \textbf{Flip-Chip Bonding}: Chip을 face-down (활성면이 아래를 향함) 방식으로 substrate나 interposer에 직접 연결하는 packaging 기술. Chip의 I/O pad에 형성된 bump (돌기)를 통해 전기적 연결과 기계적 고정을 동시에 달성한다. "Flip"이라는 이름은 chip이 뒤집혀서 활성면이 substrate를 향하는 것에서 유래한다. 전통적인 wire bonding 대비 짧은 interconnect 길이, 높은 I/O density, 우수한 전기적 성능을 제공한다.
  
  \vspace{2mm}
  
  \textbf{Wire Bonding vs Flip-Chip Bonding}:
  
  \begin{itemize}
  \item \textbf{Wire Bonding}: Chip을 face-up으로 배치하고 얇은 금속 wire (보통 gold 또는 aluminum)를 사용하여 chip pad와 substrate를 연결. 성숙한 기술이고 저렴하지만, wire 길이가 수 mm로 길어 parasitic inductance가 크고, I/O density가 제한적이며, high-speed application에 부적합하다.
  
  \item \textbf{Flip-Chip Bonding}: Wire가 필요 없고, bump를 통한 직접 연결로 interconnect 길이가 수십-수백 µm로 짧아 parasitic이 훨씬 낮다. 높은 I/O density (수천 개 이상의 connection 가능), 양면 cooling 가능, 우수한 전기적/thermal 성능을 제공한다.
  \end{itemize}
  
  \textbf{Bump 종류}:
  
  \begin{itemize}
  \item \textbf{Solder Bump}: 전통적인 방식으로 lead-tin 또는 lead-free solder (예: SnAg, SAC)로 형성된 bump. Reflow process로 melting되어 bonding을 형성한다. Pitch는 일반적으로 150-200 µm이다.
  
  \item \textbf{Copper Pillar}: Copper pillar에 thin solder cap을 가진 구조. Solder bump보다 높은 aspect ratio를 가져 finer pitch (40-100 µm) 달성이 가능하고, electromigration resistance가 우수하며, thermal/electrical performance가 더 좋다. Advanced packaging에서 주로 사용된다.
  
  \item \textbf{Micro-bump}: Advanced 3D packaging (예: die stacking)에서 사용되는 매우 작은 bump. Pitch가 20-40 µm로 매우 조밀하며, copper pillar 기반이 일반적이다.
  \end{itemize}
  
  \textbf{Flip-Chip vs Hybrid Bonding}:
  
  \begin{itemize}
  \item \textbf{Flip-Chip (Bump 기반)}: Bump를 통한 연결로 pitch가 20-200 µm 범위. Bump height만큼 gap이 존재하며 underfill material로 채운다. Parasitic은 hybrid bonding보다 크지만 wire bonding보다는 훨씬 낮다.
  
  \item \textbf{Hybrid Bonding (Bumpless)}: Bump 없이 copper-to-copper와 dielectric-to-dielectric을 직접 bonding. Pitch가 수 µm로 극도로 조밀하고, parasitic이 최소화되며, iso-power에서 23배 이상의 bandwidth density를 제공한다. 하지만 공정이 복잡하고 비용이 높다.
  \end{itemize}
  
  \vspace{2mm}
  
  \textbf{CPO에서의 역할}: CPO 시스템에서 optical engine (OE)은 일반적으로 substrate나 interposer에 flip-chip bonding으로 부착된다. 이는 OE와 substrate 사이에 수천 개의 electrical connection (power, ground, control signal 등)을 제공한다. 그 후 substrate가 host ASIC package에 다시 flip-chip bonding된다. 한편, OE 내부의 PIC-EIC 간 연결은 더 높은 성능을 위해 hybrid bonding (TSMC SoIC)을 사용한다.
  
  \end{tcolorbox}


\subsection{FAUs and Fiber Coupling}

\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: Fiber Coupling과 Waveguide 기초}]

\textbf{Optical Fiber}: 빛을 장거리로 전송하는 얇고 유연한 유리 또는 plastic 가닥. Core (빛이 전파되는 중심부)와 cladding (빛을 core에 가두는 외부층)으로 구성된다. Total internal reflection 원리를 이용하여 빛이 fiber 내부에 갇혀 전송된다. Single-mode fiber (코어 직경 약 9 µm)는 하나의 광학 mode만 전파하여 장거리 고속 통신에 사용되고, multi-mode fiber (코어 직경 50-62.5 µm)는 여러 mode를 전파하여 단거리 통신에 사용된다.

\vspace{2mm}

\textbf{Waveguide}: Chip 내부에서 빛을 전송하는 구조물. Silicon photonics에서는 silicon을 core로, silicon dioxide (SiO₂)를 cladding으로 사용하여 waveguide를 형성한다. Silicon의 높은 refractive index (약 3.5)와 SiO₂의 낮은 refractive index (약 1.45) 차이로 빛을 silicon 영역에 confine한다. 일반적으로 폭이 0.3-0.5 µm, 높이가 0.2-0.3 µm인 매우 작은 구조로, optical fiber보다 훨씬 compact하다.

\vspace{2mm}

\textbf{Optical Mode}: Waveguide나 fiber 내에서 전파되는 빛의 electric field 분포 패턴. 각 mode는 특정한 spatial distribution과 propagation constant를 가진다. Mode의 크기와 형태는 waveguide/fiber의 geometry와 refractive index profile에 의해 결정된다. Single-mode waveguide는 하나의 fundamental mode만 지원하고, multi-mode는 여러 higher-order mode를 지원한다. Coupling 과정에서 fiber mode (큰 size)와 waveguide mode (작은 size) 간 mismatch가 주요 손실 원인이다.

\vspace{2mm}

\textbf{Fiber Coupling}: Optical fiber와 on-chip waveguide 사이에서 빛을 효율적으로 전달하는 과정. 두 구조 간 mode size mismatch (fiber: 약 10 µm, waveguide: 약 0.5 µm), numerical aperture 차이, alignment error 등으로 인해 불가피하게 일부 빛이 손실된다. 정밀한 alignment (sub-micron 수준)와 mode matching 구조 (lens, taper, grating)가 필요하다. CPO 시스템에서는 수십-수백 개의 fiber를 동시에 coupling해야 하므로, manufacturing yield와 reliability가 중요한 과제이다.

\vspace{2mm}

\textbf{FAU (Fiber Array Unit)}: 여러 optical fiber를 정밀하게 정렬하여 하나의 array로 만든 unit. Fiber들을 정확한 pitch (간격)로 고정하고, OE의 waveguide array와 정렬할 수 있도록 한다. V-groove array나 fiber holder를 사용하여 fiber의 위치를 고정하며, active alignment (빛을 통과시키면서 최적 위치 찾기) 또는 passive alignment (mechanical feature로 정렬) 방식으로 OE에 부착된다.

\vspace{2mm}

\textbf{Fiber Pitch}: Fiber 간의 중심 간 거리. 현재 표준은 127 µm (mm당 최대 8개 fiber)이며, 업계는 더 높은 fiber density를 위해 80 µm pitch를 개발 중이다. 또한 multicore fiber (하나의 fiber에 여러 core)를 통해 effective density를 더욱 증가시키는 연구가 진행 중이다. Pitch가 작을수록 동일한 area에서 더 많은 I/O를 달성할 수 있어 CPO의 bandwidth density가 향상된다.

\vspace{2mm}

\textbf{Waveguide Taper}: Waveguide의 폭이 점진적으로 변하는 구조. Edge coupling에서 fiber의 큰 mode size를 chip waveguide의 작은 mode size로 adiabatic하게 변환하는 데 사용된다. 급격한 크기 변화는 reflection과 scattering을 유발하지만, 점진적인 taper (일반적으로 수백 µm 길이)는 손실을 최소화하며 mode 변환을 수행한다. 잘 설계된 taper는 coupling efficiency를 크게 향상시킨다.

\vspace{2mm}

\textbf{Refractive Index}: 빛이 물질을 통과하는 속도를 나타내는 무차원 수. 진공에서 빛의 속도를 물질 내에서의 속도로 나눈 값이다. Silicon의 refractive index는 약 3.5, SiO₂는 약 1.45로, 이 차이가 waveguide에서 빛을 confine하는 원리가 된다. Refractive index 차이가 클수록 더 tight한 confinement가 가능하여 더 작고 compact한 waveguide를 만들 수 있다.

\end{tcolorbox}

Fiber는 데이터를 전송하기 위해 OE에서 나온다. 하나의 optical lane은 2개의 fiber 또는 하나의 fiber pair (transmit plus receive)로 구성된다. 
\textbf{Fiber coupling} (원활하고 효율적인 빛 전송을 위해 fiber를 on-chip waveguide와 정밀하게 정렬하는 것)은 CPO에서 중요하고 도전적인 단계이며, 
\vspace{2mm}

\textbf{Fiber Array Unit (FAU)}는 그 과정을 지원하기 위해 CPO에서 널리 사용된다. 
이를 수행하는 두 가지 주요 방법이 있는데, 즉 \textbf{Edge Coupling (EC)}과 \textbf{Grating Coupling (GC)}이다.

\subsubsection{Edge Coupling (EC)}
Edge coupling은 fiber를 chip의 edge를 따라 정렬한다. 
아래 이미지에서 빛 beam이 edge coupler에 정확하게 들어가도록 보장하기 위해 fiber 끝이 chip의 polished edge와 정밀하게 정렬되어야 함을 볼 수 있다. 
Fiber 끝의 microlens는 빛을 chip 쪽으로 집중하고 방향을 지정하여 waveguide로의 진입을 이끈다. 
Waveguide taper는 점진적으로 넓어져 reflection과 scattering을 줄여 coupling efficiency를 보장하는 smooth mode transition을 허용한다. 
그러한 lens와 taper가 없으면 fiber facet과 waveguide facet 사이의 interface에서 상당한 optical loss가 발생할 것이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/758e5ed1-01f8-4b79-8d31-3b58a64bfe28_1024x761.jpg}
\caption{Source: Ansys}
\end{figure}
Edge coupling은 낮은 coupling loss, 광범위한 wavelength와 작동하는 능력, 그리고 polarization insensitivity로 인해 선호된다. 그러나 다음의 몇 가지 단점도 있다:

\begin{enumerate}
  \item Fabrication process가 더 복잡하며 undercut과 deep etching이 필요하다.

  \item 1D 구조이기 때문에 fiber density가 제한될 수 있다.

  \item Die stacking과 호환되지 않는다. (TSV가 thinning을 필요로 하기 때문에)

  \item Form factor의 mechanics reliability, mechanical stress, warpage 및 fiber handling에 대한 과제가 있다.

  \item 더 낮은 thermal reliability를 제공한다.

  \item 일반적으로 ecosystem compatibility의 부족이 있다.

\end{enumerate}




\begin{tcolorbox}[breakable,colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: Edge Coupling의 기술적 한계와 제약}]

\textbf{1. Fabrication 복잡성: Undercut과 Deep Etching}

Edge coupling을 위해서는 chip edge에 고품질의 optical facet이 필요하다. 이를 달성하기 위한 fabrication 과정은 매우 까다롭다:

\begin{itemize}
\item \textbf{Undercut}: Mask 아래의 재료가 lateral 방향으로 식각되는 현상. Edge coupler를 만들 때 waveguide facet을 노출시키기 위해 의도적으로 사용되지만, 제어하기 어렵다. Isotropic etching에서 자주 발생하며, 과도한 undercut은 구조적 불안정성을 초래한다.

\item \textbf{Deep Etching}: Waveguide를 완전히 노출시키기 위해 수 마이크론 이상의 깊이로 수직 식각하는 공정. Reactive Ion Etching (RIE)이나 Deep Reactive Ion Etching (DRIE) 같은 anisotropic etching 기술이 필요하다. Sidewall roughness, etch angle 제어, aspect ratio dependent etching (ARDE) 등의 과제가 있으며, 이는 optical loss에 직접 영향을 미친다.

\item \textbf{Facet Quality}: Edge coupling의 efficiency는 facet의 smoothness와 verticality에 극도로 민감하다. Surface roughness가 수십 nm만 되어도 scattering loss가 크게 증가한다. Polishing이나 cleaving으로 facet quality를 개선할 수 있지만, 이는 추가 공정 단계를 요구한다.
\end{itemize}

이에 비해 grating coupling은 간단한 two-step etched process로 제조 가능하여 fabrication이 훨씬 용이하다.

\vspace{2mm}

\textbf{2. 1D 구조의 Fiber Density 제한}

Edge coupling은 본질적으로 \textbf{1차원(1D) 구조}이다:

\begin{itemize}
\item \textbf{제약}: Fiber는 chip의 edge를 따라 일렬로만 배치될 수 있다. 즉, chip의 perimeter (둘레)만 사용 가능하다.

\item \textbf{Density 계산}: Chip edge 길이가 L mm이고 fiber pitch가 127 µm라면, 최대 fiber 수는 약 L/0.127 개이다. 예를 들어, 10mm edge에는 약 79개 fiber만 배치 가능하다.

\item \textbf{Corner 문제}: Chip의 모서리(corner)에서는 fiber routing이 어려워 실제 사용 가능한 edge 길이는 더 짧아진다.

\item \textbf{Bidirectional 고려}: 각 fiber pair (Tx + Rx)는 2개의 위치를 차지하므로, 실제 optical lane 수는 fiber 수의 절반이다.
\end{itemize}


\vspace{2mm}

\textbf{4. Die Stacking 비호환성 - 근본적인 구조적 충돌}

먼저 핵심 개념들을 이해해보자:

\begin{itemize}
\item \textbf{Edge Coupling}: Fiber가 chip의 \textbf{옆면(edge)}에서 수평으로 들어간다. 마치 책의 옆면에서 무언가를 꽂는 것과 같다.

\item \textbf{Die Stacking}: 여러 개의 die를 레고 블록처럼 \textbf{위로 쌓는} 것이다. 

\item \textbf{TSV (Through-Silicon Via)}: 쌓인 die들을 전기적으로 연결하기 위해 die를 \textbf{수직으로 관통}하는 전선이다.

\item \textbf{Wafer Thinning}: Die를 쌓기 전에 각 die를 sandpaper로 갈듯이 뒷면을 \textbf{얇게 깎아내는} 공정이다 (약 775 µm → 50 µm 이하). 여러 층을 쌓아도 전체 높이가 너무 높아지지 않도록 하기 위함이다.
\end{itemize}

\textbf{왜 호환되지 않는가? - 간단한 비유}

책 3권을 쌓는다고 상상해보자:
\begin{enumerate}
\item \textbf{1층 책}: 책의 옆면(edge)에 fiber를 연결했다.
\item \textbf{2층 책을 올림}: 이제 1층 책의 옆면이 2층 책에 가려진다! Fiber를 연결할 수 없다.
\item \textbf{3층 책을 올림}: 2층 책의 옆면도 가려진다!
\item \textbf{결과}: 맨 위층 책의 옆면만 접근 가능하다. 나머지는 전부 가려진다.
\end{enumerate}
\vspace{2mm}

\textbf{5. Thinning이 추가로 만드는 문제}

Die stacking을 위해 wafer를 얇게 깎을 때 추가 문제가 발생한다:

\begin{itemize}
\item \textbf{Edge 손상}: Edge coupler는 chip 옆면의 매끈하고 정확한 표면(facet)에 의존한다. Backside를 sandpaper로 가는 것처럼 갈아내면, 그 mechanical stress와 진동이 chip 전체로 전달되어 edge 근처의 섬세한 waveguide 구조를 손상시킬 수 있다.

\item \textbf{얇은 chip은 깨지기 쉬움}: 50 µm로 얇아진 chip은 종이처럼 약하다. 취급 중에 edge가 쉽게 깨지거나 부스러질(chipping) 수 있다. Edge coupler는 손상된 edge에서는 작동하지 않는다.
\end{itemize}

\vspace{2mm}

\textbf{반면 Grating Coupling은 왜 문제없나?}

Grating coupling은 완전히 다른 구조이다:
\begin{itemize}
\item \textbf{Fiber가 위에서 들어온다}: 책의 옆면이 아니라 \textbf{표지(top surface)}를 통해 빛이 수직으로 들어온다.

\item \textbf{Die를 쌓아도 OK}: 책 3권을 쌓아도 각 책의 표지는 여전히 위를 향한다. 모든 층의 grating에 위에서 접근 가능하다.

\item \textbf{Thinning도 OK}: Backside (뒷면)를 깎지만, grating은 front side (앞면) 표면에 있으므로 전혀 영향받지 않는다.

\item \textbf{결론}: 3D integration과 완벽하게 호환된다.
\end{itemize}

\vspace{2mm}

\end{tcolorbox}



\textbf{Global Foundries (GFS)}는 올해 VLSI conference에서 signature 45nm Fotonix platform에서 32 channel과 127µm-pitch를 가능하게 하는 monolithically integrated SiN edge coupler를 시연했다.

\subsubsection{Grating Coupling (GC)}
\textbf{Grating Coupler (GC)}에서 빛은 위에서 들어오고 fiber는 grating 위의 작은 각도로 배치된다. 
빛이 grating에 도달하면 주기적 구조가 빛을 아래로 scatter하고 구부려 waveguide로 들어간다.

Grating/vertical coupling의 주요 이점은 여러 행의 fiber를 가질 수 있어 optical engine당 더 많은 fiber를 허용한다는 것이다. 
GC는 또한 substrate의 바닥에 배치될 필요가 없어 OE를 interposer에 배치하는 것이 가능하다. 
마지막으로 GC는 극도의 정밀도로 배치될 필요가 없으며 간단한 two-step etched process로 더 쉽게 제조될 수 있다.
GC의 단점은 single-polarization grating coupler가 제한된 범위의 wavelength에서만 작동하고 polarization에 매우 민감하다는 것이다.

NVIDIA는 여러 이점 때문에 GC를 선호했다.
2D density를 가능하게 하고, 더 작은 footprint를 제공하며, 제조가 더 쉽고, EC와 비교하여 더 간단한 wafer-level testing을 허용한다. 
그러나 회사는 또한 GC의 여러 단점을 알고 있다. 일반적으로 더 높은 optical loss를 도입하고 EC보다 더 좁은 optical bandwidth를 가진다 (후자는 일반적으로 더 넓은 spectral range를 수용할 수 있다).
TSMC도 COUPE platform에서 지원되는 GC에 대한 더 높은 선호도를 명확히 가지고 있다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/e2285602-3ce2-4370-9e31-85fea4e808c4_1024x583.jpg}
\caption{Source: Journal of Semiconductors}
\end{figure}

\subsection{Laser Type and Wavelength Division Multiplexing (WDM)}

\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: Laser와 Wavelength Multiplexing}]

\textbf{ELS (External Light Source)}: Laser가 fiber를 통해 optical engine에 연결된 별도의 module에 위치하는 방식. 
주로 OSFP와 같은 pluggable form factor 사용. 
Field servicing이 간단하지만 connector loss, fiber coupling loss, modulator inefficiency로 인해 높은 전력 소비 (laser와 TEC가 전력의 ~70\% 차지).
\vspace{2mm}

\textbf{CWDM (Coarse Wavelength Division Multiplexing)}: 일반적으로 20nm spacing으로 더 적은 channel을 전달하는 방식. Wider spacing으로 capacity가 제한됨.

\vspace{2mm}

\textbf{DWDM (Dense Wavelength Division Multiplexing)}: 매우 tight spacing (종종 <1nm)으로 많은 lane을 pack하는 방식. 40, 80 또는 100+ channel 수용 가능. 제한된 fiber pair를 최대화하는 데 핵심적.

\vspace{2mm}

\textbf{Lambda (λ)}: 개별 optical lane의 개별 wavelength를 지칭. 각 lambda는 하나의 modulator를 필요로 한다.

\vspace{2mm}

\textbf{DFB (Distributed FeedBack) Laser}: Single wavelength를 생성하는 laser type. Lumentum이 single high-power DFB와 DFB array를 제공.

\vspace{2mm}

\textbf{Quantum-dot Mode-locked Comb}: 여러 wavelength를 동시에 생성하는 laser 기술. Innolume이 개발 중.

\vspace{2mm}

\textbf{Pumped Nonlinear Resonant Comb}: 여러 wavelength를 생성하는 또 다른 접근법. Xscape, Enlightra, Iloomina가 개발 중.

\end{tcolorbox}

Laser를 CPO에 통합하는 두 가지 주요 방법이 있다.
첫 번째 접근 방식인 on-chip laser는 일반적으로 III‑V (InP) 재료를 silicon에 bonding하여 laser와 modulator를 동일한 photonic chip에 통합한다. 
On-chip laser는 design을 단순화하고 insertion loss를 줄이지만 몇 가지 과제가 있을 수 있다:

\begin{enumerate}
  \item Laser는 system 내에서 가장 failure가 발생하기 쉬운 component 중 하나로 알려져 있다 -- CPO engine에 통합되면 전체 chip을 다운시킬 것이기 때문에 failure는 높은 blast radius를 가질 것이다;

  \item Laser는 또한 열에 민감하며, co-packaged OE에 배치하면 system의 가장 뜨거운 부분인 host silicon에 매우 가까울 것이기 때문에 laser를 높은 열에 노출시켜 문제를 더욱 악화시킬 것이다;

  \item On-chip laser는 일반적으로 충분히 높은 power output을 제공하는 데 어려움을 겪는다.

\end{enumerate}
업계가 합의에 도달한 하나의 접근 방식은 \textbf{External Light Source (ELS)}를 사용하는 것이다. 
Laser는 fiber를 통해 optical engine에 연결된 별도의 module에 있다. 
종종 이 laser는 OSFP와 같은 pluggable form factor에 있다. 
이 설정은 laser failure의 상당히 일반적인 경우에 field servicing을 단순화한다.

ELS의 단점은 더 높은 전력 소비이다. 
아래 다이어그램에 표시된 것처럼 ELS 기반 system에서 output power는 connector loss, 
fiber coupling loss 및 modulator inefficiency와 같은 다양한 요인으로 인해 여러 단계에서 손실된다. 
따라서 이 system의 각 laser는 loss를 보상하고 안정적인 전송을 보장하기 위해 24.5 dBm의 optical power를 제공해야 한다. 
High-output laser는 더 많은 열을 생성하고 thermal stress 하에서 더 빨리 저하되며, 
laser와 thermo-electric cooler가 ELS 전력 소비의 \textasciitilde{}70\%를 차지한다. 
Laser design, packaging 및 optical path의 점진적인 개선이 도움이 되지만 laser의 높은 전력 요구사항 문제는 완전히 해결되지 않았다.

올해 VLSI conference에서 NVIDIA는 ecosystem 내의 여러 laser 파트너를 강조했다.
Single high-power DFB를 위한 Lumentum, DFB array를 위한 Ayar Labs, quantum-dot mode-locked comb를 위한 Innolume, 
그리고 pumped nonlinear resonant comb를 위한 Xscape, Enlightra 및 Iloomina.

NVIDIA는 또한 잠재적인 대안 laser 솔루션으로 \textbf{VCSEL array}를 탐색하는 것에 대해 논의했다. 
Fiber당 데이터 rate는 더 낮을 것이고 일부 thermal 문제가 있을 수 있지만, 
VCSEL은 전력 및 비용 효율성을 제공할 수 있으며 wide-and-slow application에 적합할 수 있다. 
그렇긴 하지만 우리는 이것이 NVIDIA에게 즉각적인 우선순위라고 보지 않는다.

\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: VCSEL - 대안적 Laser 기술}]

\textbf{VCSEL (Vertical-Cavity Surface-Emitting Laser)}: 빛이 wafer 표면에 수직으로 방출되는 반도체 laser. 전통적인 edge-emitting laser와 달리 cavity (공진기)가 수직으로 배치되어 빛이 chip의 top surface에서 나온다. 일반적으로 GaAs 또는 InP 기반으로 제조되며, 850nm, 940nm, 1310nm 등 다양한 wavelength를 지원한다.

\vspace{2mm}

\textbf{VCSEL의 구조적 특징}:
\begin{itemize}
\item \textbf{수직 cavity}: 두 개의 distributed Bragg reflector (DBR) mirror가 위아래에 있고, 그 사이에 active region이 있다. Cavity 길이가 매우 짧아 (수 µm) single longitudinal mode 동작이 가능하다.

\item \textbf{Surface emission}: 빛이 wafer surface로 방출되므로 wafer-level testing이 용이하고, 2D array로 제조할 수 있다.

\item \textbf{Circular beam}: 대칭적인 구조로 circular beam profile을 생성하여 fiber coupling이 edge-emitting laser보다 용이하다.
\end{itemize}

\vspace{2mm}

\textbf{Edge-Emitting Laser vs VCSEL}:

\begin{itemize}
\item \textbf{Edge-Emitting Laser} (DFB, DBR 등):
  \begin{itemize}
  \item Cavity가 수평으로 배치, 빛이 chip edge에서 방출
  \item 높은 출력 power (수십-수백 mW)
  \item 좁은 linewidth, single wavelength에 최적화
  \item 개별 die로 dicing 필요, testing이 복잡
  \item High-speed, long-distance communication에 적합
  \end{itemize}

\item \textbf{VCSEL}:
  \begin{itemize}
  \item Cavity가 수직, 빛이 surface에서 방출
  \item 상대적으로 낮은 출력 power (수 mW)
  \item Wafer-level testing 가능, 2D array 제조 용이
  \item 낮은 제조 비용, 높은 생산성
  \item Short-distance, cost-sensitive application에 적합
  \end{itemize}
\end{itemize}

\vspace{2mm}

\textbf{VCSEL Array}: 여러 개의 VCSEL을 2D grid 형태로 배열한 구조. 단일 wafer에서 수십-수백 개의 VCSEL을 동시에 제조할 수 있어 제조 비용이 크게 절감된다. 각 VCSEL은 독립적으로 modulation 가능하며, parallel optical link를 구현하는 데 이상적이다. Data center의 short-reach application (예: 100m 이내 multimode fiber)에서 널리 사용된다.

\vspace{2mm}

\textbf{CPO에서 VCSEL의 장단점}:

\textbf{장점}:
\begin{itemize}
\item \textbf{비용 효율}: Wafer-level 제조로 대량 생산 시 매우 저렴하다. Edge-emitting laser 대비 1/10 수준의 비용이 가능하다.
\item \textbf{전력 효율}: 낮은 threshold current와 낮은 operating voltage로 전력 소비가 적다.
\item \textbf{2D array}: 고밀도로 배치 가능하여 많은 수의 parallel channel을 구현할 수 있다.
\item \textbf{제조 용이}: 성숙한 제조 공정, 높은 yield, 간단한 packaging.
\end{itemize}

\textbf{단점}:
\begin{itemize}
\item \textbf{낮은 출력 power}: 일반적으로 수 mW로, long-distance나 high-loss system에 부적합하다.
\item \textbf{낮은 data rate}: 개별 VCSEL은 일반적으로 25-56 Gbps로 제한된다. Edge-emitting laser의 100+ Gbps에 비해 낮다.
\item \textbf{Thermal sensitivity}: 온도에 따라 wavelength가 drift하여 DWDM 같은 tight spacing에 어려움이 있다.
\item \textbf{Shorter reach}: Multimode fiber에 최적화되어 있어 장거리 transmission에 제한적이다.
\end{itemize}

\vspace{2mm}

\textbf{Wide-and-Slow Application}: VCSEL은 "많은 수의 낮은 속도 channel" 접근 방식에 적합하다. 예를 들어, 100개의 25 Gbps VCSEL로 2.5 Tbps를 달성하는 것이 10개의 250 Gbps edge-emitting laser보다 비용 효율적일 수 있다. 그러나 fiber count와 space 제약이 있는 CPO에서는 "fewer, faster lane"이 일반적으로 선호되므로, VCSEL은 특정 cost-optimized niche application에 제한될 것으로 예상된다.

\end{tcolorbox}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/355713bf-ab91-4b4a-99e7-68c4a57b9313_1024x647.png}
\caption{Source: CPO status, challenges, and solutions}
\end{figure}

\textbf{Wavelength Division Multiplexing (WDM)}은 여러 다른 wavelength 또는 lambda의 빛이 동일한 fiber strand를 통해 전송되는 것이다. 
WDM의 두 가지 일반적인 variant는 \textbf{Coarse Wavelength Division Multiplexing (CWDM)}과 \textbf{Dense Wavelength Division Multiplexing (DWDM)}이다. 
CWDM은 일반적으로 상대적으로 멀리 떨어진 (일반적으로 20 nm spacing) 더 적은 channel을 전달하는 반면, 
DWDM은 매우 tight spacing (종종 <1nm spacing)으로 많은 lane을 pack한다. 
CWDM의 wider channel spacing은 capacity를 제한하는 반면, DWDM의 narrower spacing은 40, 80 또는 심지어 100+ channel을 수용할 수 있다. 
WDM이 중요한 이유는 오늘날 제안된 대부분의 CPO 구현이 optical engine에 부착될 수 있는 fiber 수에 의해 제한되기 때문이다. 
제한된 fiber pair는 각 fiber pair가 최대화되어야 함을 의미한다.

\subsection{Modulator Types}







Laser가 PIC에 들어가면 electronic signal이 laser의 wavelength로 encoding되는 modulation 단계 (driver에 의해 구동됨)를 거친다. 
이 과정에 사용되는 세 가지 주요 modulator type은 \textbf{Mach-Zender Modulator (MZM)}, \textbf{Micro-Ring Modulator (MRM)} 및 \textbf{Electro-Absorption Modulator (EAM)}이다. 각 개별 lambda (개별 optical lane의 개별 wavelength)는 하나의 modulator를 필요로 한다.

\subsubsection{Mach-Zehnder Modulator (MZM)}
MZM은 continuous-wave optical signal을 두 개의 waveguide arm으로 분할하여 데이터를 encoding하며, 
이들의 refractive index는 applied voltage에 의해 변화한다. 
Arm이 재결합될 때 그들의 interference pattern이 signal의 intensity 또는 phase를 modulate한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/2df0e438-b3fb-47d4-ac77-2121524f303f_1024x469.png}
\caption{Source: Luceda Academy}
\end{figure}
MZM은 세 가지 중 구현하기 가장 쉽고 낮은 thermal sensitivity를 가지고 있어 정밀한 temperature control의 필요성을 줄인다. 
그들의 높은 linearity는 PAM4 및 coherent QAM (QAM은 HPC/AI workload에 적합하지 않지만)과 같은 advanced modulation format을 지원한다. 
MZM의 낮은 chirp는 higher-order modulation 및 long-distance transmission을 위한 signal integrity를 향상시킨다. 
MZM은 또한 channel당 더 높은 bandwidth를 가능하게 한다: lane당 200G가 작동하는 것으로 입증되었으며, 
non-coherent PAM modulation으로 lane당 400G가 가능할 것으로 믿어진다.

그러나 MZM의 단점은 다음과 같다:

\begin{itemize}
  \item \textbf{Millimeter scale로 측정되는 dimension을 가진 큰 form factor} (micron scale의 MRM과 비교), 두 개의 waveguide arm과 combining region이 필요하기 때문에 더 많은 chip 면적을 소비하고 OE PIC에 포함된 modulator (따라서 channel)의 density를 제한한다. MZM 크기는 \textasciitilde{}12,000mm2 정도이고, EAM은 약 250mm2 (5x50mm)이며 MRM은 25mm2에서 225mm2 사이 (diameter 5-15mm2)이다. 이것은 scaling을 제한할 수 있는 MZM의 한 가지 중요한 단점이다. 그러나 modulator 주변의 driver 및 optical/electrical control circuitry를 포함한 전체 PIC/EIC 조합의 크기를 고려하면 MZM의 크기 단점은 덜 두드러질 수 있다,

  \item \textbf{높은 전력 소비}, phase-shifting process가 상당한 에너지를 요구하기 때문이다. 또한 sub-voltage에서 작동하는 MRM보다 더 높은 bias condition (기본적으로 initiating voltage)을 가진다. 
  그러나 Nubis와 같은 기업은 MZM의 전력 단점을 개선하기 위해 영리한 design을 개발하려고 노력하고 있다.

\end{itemize}
startup ecosystem에서 Nubis는 scale-up CPO 솔루션에 주로 MZM을 활용하는 기업 중 하나이다. 
MZM은 큰 form factor와 제한된 수의 lambda로 인해 startup ecosystem에서 널리 선택되지 않는다.

\begin{tcolorbox}[breakable,colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: Mach-Zehnder Modulator - 빛의 간섭을 이용한 변조}]

\textbf{A. Modulation이란 무엇인가?}

CPO 시스템에서 modulator의 역할은 electrical digital signal (0과 1)을 optical signal (빛의 밝기 변화)로 변환하는 것이다. 
Laser는 일정한 밝기의 continuous-wave (CW) 빛을 제공하고, modulator는 이 빛을 켜고 끄거나 밝기를 조절하여 데이터를 encoding한다. 


\vspace{2mm}

\textbf{B. MZM의 기본 원리: 빛의 간섭 (Interference)}
\vspace{1mm}
MZM은 \textbf{빛의 간섭 현상}을 이용한다. 간단한 비유로 이해해보자:

\textbf{물결 비유}: 연못에 두 개의 돌을 동시에 던지면 두 물결이 만나는 곳에서:
\begin{itemize}
\item 두 물결의 봉우리가 만나면 → 더 큰 물결 (constructive interference)
\item 한 물결의 봉우리와 다른 물결의 골이 만나면 → 서로 상쇄 (destructive interference)
\end{itemize}

빛도 파동이므로 같은 원리가 적용된다!

\vspace{2mm}

\textbf{C. MZM의 동작 과정 - 단계별 설명}

\begin{enumerate}
\item \textbf{빛을 두 갈래로 나눔}: CW laser에서 나온 빛이 Y-junction (Y자 분기점)에서 두 개의 waveguide arm으로 50:50으로 분할된다.

\item \textbf{한쪽 arm의 phase를 조절}:
   \begin{itemize}
   \item 두 arm 중 하나 (또는 둘 다)에 전압을 가한다.
   \item 전압이 가해지면 silicon의 \textbf{refractive index}가 변한다.
   \item Refractive index가 변하면 그 arm을 통과하는 빛의 \textbf{속도가 느려지거나 빨라진다}.
   \item 속도가 변하면 빛의 \textbf{phase}가 shift된다.
   \end{itemize}

\item \textbf{두 빛을 다시 합침}:
   \begin{itemize}
   \item 두 arm의 빛이 다시 만나 결합된다.
   \item 이때 두 빛의 phase 차이에 따라 간섭이 발생한다.
   \end{itemize}

\item \textbf{간섭 결과로 밝기 결정}:
   \begin{itemize}
   \item \textbf{Phase 차이 = 0}: 두 빛이 같은 위상 → Constructive interference → 밝음 (Binary "1")
   \item \textbf{Phase 차이 = π (180도)}: 한 빛의 봉우리가 다른 빛의 골과 만남 → Destructive interference → 어두움 (Binary "0")
   \item \textbf{Phase 차이 = 중간값}: 부분적 간섭 → 중간 밝기 (PAM4 같은 multi-level modulation에 사용)
   \end{itemize}
\end{enumerate}

\vspace{2mm}

\textbf{D. 전압과 Phase의 관계}

Silicon photonics MZM은 \textbf{plasma dispersion effect}를 사용한다:
\begin{itemize}
\item 전압을 가하면 silicon에 free carrier (전자와 정공)가 주입되거나 제거되어, Free carrier density가 변하면 refractive index가 변한다.
\item 이 작은 변화도 수 millimeter 길이의 waveguide를 통과하면 충분한 phase shift를 만든다.
\item 일반적으로 π phase shift를 위해 수 volt의 voltage가 필요하다 (V$_\pi$ parameter).
\end{itemize}

\vspace{2mm}

\textbf{E. 왜 MZM이 "Mach-Zehnder"인가?}

Mach-Zehnder interferometer라는 19세기 물리학 실험 장치에서 이름을 따왔다. Ludwig Mach와 Ludwig Zehnder가 빛의 간섭을 연구하기 위해 개발한 구조와 동일한 원리를 사용한다.

\vspace{2mm}

\textbf{F. MZM의 장점:}

\begin{itemize}
\item \textbf{낮은 thermal sensitivity}: 두 arm이 나란히 있어 온도가 변해도 \textbf{비슷하게} 영향을 받는다. Phase \textbf{차이}만 중요하므로, 공통 영향은 상쇄된다.

\item \textbf{High linearity}: Phase shift가 voltage에 비례하므로, multi-level modulation (PAM4, QAM)에서 각 level을 정확히 제어할 수 있다.

\item \textbf{낮은 chirp}: Intensity만 변하고 wavelength는 거의 변하지 않는다. 이는 signal quality에 중요하다.

\item \textbf{고속 동작}: Simple structure로 200 Gbps 이상 달성 가능.
\end{itemize}

\vspace{2mm}

\textbf{G. MZM의 단점:}

\begin{itemize}
\item \textbf{큰 크기}: 두 개의 긴 arm이 필요하다 (각각 수 mm). Total footprint가 약 12,000 µm².

\item \textbf{높은 전력}: V$_\pi$가 크고(수 volt), capacitance도 커서 switching power가 높다.
\end{itemize}

\end{tcolorbox}




\subsubsection{Micro-Ring Modulators (MRMs)}
MRM은 하나 이상의 straight waveguide에 결합된 compact ring waveguide를 사용한다. 
Electrical signal은 ring의 refractive index를 변경하여 resonant wavelength를 이동시킨다. 
Resonance를 input light와 align하거나 misalign하도록 tuning함으로써 MRM은 optical signal의 intensity 또는 phase를 modulate하여 데이터를 encoding한다.

Light source는 input port에서 ring으로 전달된다. 
대부분의 wavelength의 빛에 대해 ring에 resonance가 없어 빛이 device를 통과하여 input port에서 through port로 전달된다. 
Wavelength가 resonance condition을 만족하면 빛은 ring에서 constructively하게 interfere하고 대신 drop port로 당겨진다. 
아래 normalized power graph에 표시된 것처럼 특정 wavelength의 빛은 drop port에서 transmission power의 sharp peak를 야기하고 through port에서 transmission의 해당 drop을 야기한다. 이 효과는 modulation에 사용될 수 있다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/11337158-6ce6-4419-8ba3-9ce78cc1a939_1291x990.jpg}
\caption{Source: \href{https://people.engr.tamu.edu/spalermo/ecen689_oi/lecture11_ee689_rrm_tx.pdf}{Sam Palermo, Texas A\&M University}}
\end{figure}
Optical Engine은 일반적으로 여러 MRM을 사용하며, 
이러한 각 ring은 다른 wavelength로 tuning될 수 있어 WDM을 달성하기 위해 추가 device set를 필요로 하는 것과 반대로 ring 자체를 사용하여 \textbf{Wavelength Division Multiplexing (WDM)}을 가능하게 한다.
MRM은 다음의 몇 가지 주요 이점을 가지고 있다:

\begin{itemize}
  \item 극도로 compact하며 (수십 micron scale), MZM보다 훨씬 더 높은 modulator density를 허용한다. MZM 크기는 \textasciitilde{}12,000mm2 정도이고, EAM은 약 250mm2 (5x50mm)이며 MRM은 25mm2에서 225mm2 사이 (diameter 5-15mm2)이다.

  \item Ring은 WDM application (8 또는 16 wavelength를 가진 DWDM 포함)에 매우 적합하며, built-in mux/demux 기능을 가지고 있다.

  \item MRM은 매우 에너지 효율적일 수 있다 (bit당 더 낮은 전력).

  \item 마지막으로 ring은 낮은 chirp를 가지며, 이는 signal 품질을 향상시킨다.

\end{itemize}
그러나 MRM은 또한 아래의 몇 가지 과제를 동반한다:

\begin{itemize}
  \item MRM은 MZM 및 EAM보다 10-100배 더 temperature-sensitive할 수 있으며, design 및 manufacturing이 어려운 매우 정밀한 control system을 필요로 한다.

  \item 그들은 non-linear하여 PAM4/6/8과 같은 higher-order modulation을 복잡하게 만든다.

  \item MRM의 sensitivity와 tight temperature control tolerance는 각 design이 정밀한 요구사항을 가지기 때문에 표준화를 어렵게 만들 수 있다.

\end{itemize}
솔루션 제공업체 중에서 NVIDIA는 MRM에 대한 명확한 선호를 가지고 있다. 
그들은 CPO system에서 MRM을 design하고 넣은 최초라고 주장한다. 
회사는 MRM의 핵심 이점이 전력 소비를 줄이는 데 도움이 되는 compact size와 낮은 driving voltage라고 믿는다. 
그러나 MRM 기술은 또한 control하기 어려운 것으로 알려져 있어 성공적인 구현을 위해 design precision이 중요하다. 이것은 실제로 NVIDIA의 강점이다.

Fabrication 측면에서 TSMC의 advanced CMOS expertise는 high-precision과 훌륭한 Q-factor로 MRM을 fabrication하는 데 매우 적합하다. 
추가로 Tower도 photonics node에 강력한 fabrication capability를 가져온다.
MRM은 구현하기 어렵지만 확실히 실행 가능하다. 
그들은 잠재적으로 MZM보다 더 높은 bandwidth density를 가능하게 할 수 있다. 
그래서 TSMC, NVIDIA 및 Ayar Labs, Lightmatter 및 Ranovus와 같은 많은 CPO 기업이 이 기술 roadmap에 집중한다.

\begin{tcolorbox}[breakable,colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: Micro-Ring Modulator - Resonance를 이용한 compact 변조}]

\textbf{A. MRM의 기본 구조}

MRM은 circular ring waveguide가 하나 이상의 straight bus waveguide 옆에 위치한 구조이다:

\begin{itemize}
\item \textbf{Ring waveguide}: 직경이 수-수십 µm인 원형 waveguide로 빛이 ring 내부를 순환할 수 있다.

\item \textbf{Bus waveguide}: Ring 옆을 지나가는 직선 waveguide로 Input port에서 through port로 연결되며, 때로는 drop port도 있다.

\item \textbf{Coupling gap}: Ring과 bus waveguide 사이의 간격 (수백 nm)이고, 이를 통해 evanescent field coupling이 발생하여 빛이 ring으로 전달될 수 있다.
\end{itemize}

MRM의 전체 footprint는 매우 작다: 직경 5-15 µm ring은 약 25-225 µm² 면적만 차지한다. 이는 MZM (약 12,000 µm²)보다 50-500배 작다.

\vspace{2mm}

\textbf{B. Resonance의 핵심 개념}

MRM의 동작은 \textbf{optical resonance}에 기반한다:

\begin{itemize}
\item \textbf{Resonance condition}: 빛의 wavelength가 ring의 circumference (둘레)의 정수배와 일치할 때 발생한다.
  \[
  m \lambda = n_{eff} \times 2\pi R
  \]
  여기서 $m$은 정수 (mode number), $\lambda$는 wavelength, $n_{eff}$는 effective refractive index, $R$은 ring radius이다.

\item \textbf{Resonance 시}: 빛이 ring을 여러 번 순환하면서 constructive interference가 발생한다. 각 round trip마다 빛이 같은 위상으로 돌아오면 빛의 intensity가 ring 내부에서 축적된다 (buildup).

\item \textbf{Off-resonance 시}: Wavelength가 맞지 않으면 각 round trip마다 phase가 맞지 않아 destructive interference가 발생하고, 빛은 ring에 들어가지 못하고 bus waveguide를 따라 그냥 통과한다.
\end{itemize}

\vspace{2mm}

\textbf{C. Q-factor (Quality Factor)}

Q-factor는 ring resonator의 성능을 나타내는 핵심 parameter이다:

\begin{itemize}
\item \textbf{정의}: Q = $\lambda_0 / \Delta\lambda$, 여기서 $\lambda_0$는 resonance wavelength, $\Delta\lambda$는 resonance peak의 폭 (full width at half maximum).

\item \textbf{의미}: 높은 Q는 sharp한 resonance peak를 의미한다. 빛이 ring 내부에서 많은 round trip을 하며 오래 머문다.

\item \textbf{TSMC의 expertise}: High-precision fabrication으로 uniform ring geometry와 smooth sidewall을 만들어 high Q-factor를 달성한다. 이것이 TSMC가 MRM에서 경쟁 우위를 갖는 이유이다.
\end{itemize}

\vspace{2mm}

\textbf{D. Modulation 원리 - Resonance Tuning}

MRM은 electrical signal로 resonance wavelength를 shift하여 modulation을 수행한다:

\begin{enumerate}
\item \textbf{전압 인가}: Ring에 인접한 전극에 voltage를 가한다.

\item \textbf{Refractive index 변화}: 
   \begin{itemize}
   \item MZM과 마찬가지로 plasma dispersion effect 사용
   \item Free carrier injection/depletion으로 $n_{eff}$ 변화
   \end{itemize}

\item \textbf{Resonance wavelength shift}:
   \begin{itemize}
   \item $\Delta \lambda = \lambda_0 \times (\Delta n_{eff} / n_{eff})$
   \end{itemize}

\item \textbf{Intensity modulation}:
   \begin{itemize}
   \item \textbf{"On" state (Binary "1")}: Voltage = 0, resonance가 input laser wavelength와 align 
   \item \textbf{"Off" state (Binary "0")}: Voltage applied, resonance shift → off-resonance
   \end{itemize}
\end{enumerate}

\vspace{2mm}

\textbf{F. WDM과의 완벽한 궁합}

\begin{itemize}
\item \textbf{Built-in wavelength selectivity}: 각 ring은 특정 wavelength에만 반응한다. 여러 ring을 array로 배치하면 각각 다른 wavelength를 처리한다.

\item \textbf{Multiplexing 자동 수행}: Separate mux/demux device 없이 ring 자체가 wavelength routing을 수행한다.
  \begin{itemize}
  \item Ring 1: $\lambda_1$ (예: 1550.0 nm) → Drop port 1
  \item Ring 2: $\lambda_2$ (예: 1550.8 nm) → Drop port 2
  \item ...
  \end{itemize}

\item \textbf{Compact array}: 수십 개의 ring을 작은 면적에 집적 가능. DWDM (40-80 channel)을 mm² 규모로 구현할 수 있다.
\end{itemize}

\vspace{2mm}

\textbf{G. 장점의 이유}

\begin{itemize}
\item \textbf{극도로 compact}: Ring 하나가 25-225 µm². Long waveguide arm이 필요 없다.

\item \textbf{낮은 전력}: Small modal volume → 적은 carrier 필요, 낮은 voltage로 충분한 shift ($<$ 1 V).

\item \textbf{WDM 최적화}: Inherent wavelength selectivity로 자연스럽게 mux/demux 기능 제공.

\item \textbf{낮은 chirp}: Absorption 변화가 아닌 resonance shift 사용.
\end{itemize}

\vspace{2mm}

\textbf{H. 단점의 이유}

\begin{itemize}
\item \textbf{극도로 높은 thermal sensitivity (10-100배)}:
  \begin{itemize}
  \item Resonance wavelength는 $n_{eff}$에 매우 민감하다: $\Delta \lambda / \Delta T \approx$ 0.1 nm/°C
  \item MZM은 두 arm의 phase \textbf{차이}만 중요하므로 공통 온도 변화는 상쇄되지만, MRM은 절대적인 resonance position이 중요하므로 온도 영향을 직접 받는다
  \end{itemize}

\item \textbf{정밀한 thermal control 필요}:
  \begin{itemize}
  \item Active thermal tuning: 각 ring에 heater를 배치하여 resonance를 실시간 조정
  \item Feedback control loop: Photodetector로 monitor하고 heater power 조절
  \item 이는 추가 전력 소비와 복잡한 control system을 요구한다
  \end{itemize}

\item \textbf{표준화 어려움}:
  \begin{itemize}
  \item 각 ring design은 정밀한 dimension (radius, gap, width)을 요구한다
  \item 제조 variation에 민감하여 각 device마다 개별 calibration 필요
  \item Vendor 간 interoperability 달성이 어렵다
  \end{itemize}
\end{itemize}

\vspace{2mm}

\textbf{I. NVIDIA와 TSMC가 MRM을 선택한 이유}

\begin{itemize}
\item \textbf{Scaling 우위}: CPO의 핵심은 제한된 space에 최대한 많은 lane을 집적하는 것이다. MRM의 50-500배 작은 footprint는 결정적 이점이다.

\item \textbf{WDM 필수성}: Fiber count 제약으로 각 fiber당 여러 wavelength를 전송해야 한다. MRM은 이를 위해 태어난 구조이다.

\item \textbf{전력 효율}: Data center scale에서 전력 소비는 critical하다. MRM의 낮은 driving voltage는 장기적으로 큰 절약을 의미한다.
\end{itemize}

\end{tcolorbox}





\subsubsection{Electro-Absorption Modulators (EAM)}
\href{https://people.engr.tamu.edu/spalermo/ecen689_oi/lecture10_ee689_eam_tx.pdf}{EAM은 signal을 modulate}하여 applied voltage에 기반하여 빛을 흡수하는 능력을 변경한다. 
더 구체적으로 EAM에 낮거나 voltage가 applied되지 않으면 device는 들어오는 laser 빛의 대부분을 통과시켜 transparent하거나 \textbf{open}하게 보이게 한다. 
더 높은 voltage가 applied되면 GeSi modulator의 band gap이 high C-band 범위 (1500nm 이상)를 커버하도록 이동하여 해당 wavelength에 대한 absorption coefficient를 증가시키고 nearby waveguide를 통과하는 optical signal을 attenuating \textbf{closing}한다. 
이것은 Franz-Keldysh effect로 알려져 있다. 
이 \textbf{open}과 \textbf{close} state 사이의 switching은 빛의 intensity를 modulate하여 optical signal에 데이터를 효과적으로 encoding한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/1406b09a-faa5-44a7-8387-4d7f6463a600_900x318.png}
\caption{Source: \href{https://people.engr.tamu.edu/spalermo/ecen689_oi/lecture10_ee689_eam_tx.pdf}{Texas A\&M University, Liu 2008, Helman 2005}}
\end{figure}
동일한 원리가 오늘날 modulation을 위해 \textbf{Electro-Absorption Modulated Laser (EML)}를 사용하는 transceiver에서 사용된다.
\textbf{Continuous Wave (CW)}, \textbf{Distributed FeedBack (DFB)} laser와 InP 기반 EAM이 함께 결합되어 하나의 lane을 modulate할 수 있는 단일 discrete \textbf{Electro-Absorption Modulated Laser (EML)}을 구축한다. 예를 들어 800G DR8 transceiver는 8개의 개별 fiber lane에 걸쳐 8개의 EML을 사용하며, 각각 PAM4 modulation (2 bits/signal)을 사용하고 \textasciitilde{}56 GBaud에서 signaling한다. GeSi 기반 modulator와 달리 InP modulator의 band gap은 모든 Datacom DR optics에서 사용되는 표준 wavelength인 O-band (1310nm)에 해당하여 높은 수준의 상호운용성을 허용한다.
InP modulator는 CPO에서 사용하기에 이상적이지 않은 몇 가지 단점을 가지고 있다. 
InP wafer는 작은 경향이 있으며 (3" 또는 6") 낮은 yield에 시달린다. 
두 요인 모두 8" 또는 12" process에서 구축될 수 있는 Silicon과 비교할 때 InP 기반 device의 unit cost를 증가시킨다. 
InP를 Silicon에 coupling하는 것도 GeSi를 다른 silicon device에 coupling하는 것보다 훨씬 더 어렵다.

EAM은 MRM 및 MZI와 비교하여 여러 이점을 가지고 있다:

\begin{itemize}
  \item 분명히 EAM과 MRM 모두 temperature variation에 대해 둘 다를 안정화시키기 위해 작동하는 control logic과 heater를 가지고 있지만, EAM은 근본적으로 temperature에 대한 sensitivity가 더 낮다. 
  MRM과 비교하여 EAM은 50°C 이상에서 훨씬 더 나은 thermal stability를 가지는 반면 MRM은 temperature에 매우 민감하다. 
  MRM의 일반적인 stability인 70-90 pm/C는 2°C variation이 resonance를 0.14nm만큼 이동시킨다는 것을 의미하며, 
  이는 MRM 성능이 붕괴되는 0.1nm resonance shift를 훨씬 넘는다. 
  \\
  이와 대조적으로 EAM은 최대 35°C의 순간 temperature shift를 견딜 수 있다. 
  이 tolerance는 특히 Celestial AI의 접근 방식에서 중요한데, 그들의 EAM modulator가 수백 watt의 전력을 dissipate하는 high-XPU power compute engine 아래의 interposer 내에 위치하기 때문이다. 
  EAM은 또한 약 80°C의 높은 ambient temperature 범위를 견딜 수 있으며, 이는 XPU 옆에 있고 그 아래에 있지 않은 chiplet application에 적용될 수 있다.

  \item MZI와 비교하여 EAM은 크기가 훨씬 작고 전력을 덜 소비하는데, MZI의 상대적으로 큰 크기가 high voltage swing을 필요로 하여 0-5V의 swing을 달성하기 위해 SerDes를 증폭하기 때문이다. Mach Zender Modulator (MZM)는 \textasciitilde{}12,000mm2 정도이고, EAM은 약 250mm2 (5x50mm)이며 MRM은 25mm2에서 225mm2 사이 (diameter 5-15mm2)이다. MZI는 또한 그러한 큰 device를 desired bias로 유지하는 데 필요한 heater에 대해 더 많은 전력 사용을 필요로 한다.

\end{itemize}
반면에 CPO를 위해 GeSi EAM을 사용하는 데 몇 가지 단점이 있다:

\begin{itemize}
  \item MRM 및 MZI와 같은 Silicon 또는 Silicon Nitride에 구축된 물리적 modulator 구조는 GeSi 기반 device보다 훨씬 더 큰 endurance와 reliability를 가진 것으로 인식되어 왔다. 실제로 많은 이들이 Germanium 기반 device를 다루고 통합하는 어려움을 고려할 때 GeSi 기반 device의 reliability에 대해 걱정하지만, Celestial은 본질적으로 Photodetector의 역인 GeSi 기반 EAM은 오늘날 transceiver에서 photodetector의 ubiquity를 고려할 때 reliability와 관련하여 알려진 양이라고 주장한다.

  \item GeSi modulator의 band edge는 자연적으로 C-band (즉, 1530nm-1565nm)에 있다. 이것을 O-band (즉, 1260-1360nm)로 이동시키기 위해 quantum well을 design하는 것은 매우 어려운 engineering 문제이다. 이는 GeSi 기반 EAM이 book-ended CPO system의 일부를 형성할 가능성이 높으며 open chiplet 기반 ecosystem에 참여하는 데 쉽게 사용될 수 없음을 의미한다.

  \item C-band laser source 주변의 laser ecosystem을 구축하는 것은 O-band CW laser source 주변의 잘 개발된 ecosystem을 사용하는 것과 비교할 때 scale의 diseconomy를 가질 수 있다. 대부분의 datacom laser는 O-band를 위해 구축되지만 Celestial은 상당한 volume의 1577nm XGS-PON laser가 제조된다고 지적한다. 이것들은 일반적으로 consumer fiber to the home 및 business connectivity application에 사용된다.

  \item SiGe EAM은 MRM과 MZI 모두에 대한 3-5dB와 비교하여 약 4-5dB의 insertion loss를 가진다. MRM은 다른 wavelength를 직접 multiplex하는 데 사용될 수 있는 반면, EAM은 CWDM 또는 DWDM을 구현하기 위해 별도의 multiplexer를 필요로 하여 잠재적 loss budget에 약간 추가한다.

\end{itemize}
전반적으로 EAM은 현재 CPO 구현에서 널리 사용되지 않으며, Celestial AI가 이 접근 방식을 적극적으로 추구하는 몇 안 되는 기업 중 하나로 두드러진다.

\begin{tcolorbox}[breakable,colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: Electro-Absorption Modulator - 흡수를 이용한 변조}]

\textbf{A. EAM의 기본 원리}

EAM은 MZM (간섭)이나 MRM (공진)과 완전히 다른 메커니즘인 \textbf{흡수 (absorption)}를 이용한다:

\begin{itemize}
\item \textbf{핵심 아이디어}: Semiconductor material의 빛 흡수 능력을 전압으로 제어한다.
  \begin{itemize}
  \item 전압이 없을 때 → Material이 transparent (투명) → 빛이 통과 → "Open" state (Binary "1")
  \item 전압이 있을 때 → Material이 absorbing (흡수) → 빛이 차단 → "Closed" state (Binary "0")
  \end{itemize}

\item \textbf{물리적 메커니즘}: Franz-Keldysh effect 또는 Quantum-Confined Stark Effect (QCSE)를 사용한다.
\end{itemize}

\vspace{2mm}

\textbf{B. Band Gap과 Absorption의 관계}

Semiconductor의 광 흡수는 band gap에 의해 결정된다:

\begin{itemize}
\item \textbf{Band gap (에너지 띠 간격)}: Valence band와 conduction band 사이의 에너지 차이.

\item \textbf{Absorption rule}: 
  \begin{itemize}
  \item Photon energy $> $ Band gap → 흡수됨 (전자가 valence에서 conduction으로 excite)
  \item Photon energy $< $ Band gap → 투과됨 (흡수할 수 없음)
  \end{itemize}

\item \textbf{Wavelength와의 관계}: 
  \[
  E_{photon} = \frac{hc}{\lambda}
  \]
  짧은 wavelength (높은 energy) → 흡수 확률 높음 \\
  긴 wavelength (낮은 energy) → 흡수 확률 낮음
\end{itemize}

\vspace{2mm}

\textbf{C. Franz-Keldysh Effect - 전압으로 Band Gap 조절}

Franz-Keldysh effect는 electric field가 semiconductor의 band structure를 변형시키는 현상이다:

\begin{enumerate}
\item \textbf{전압 없음 (0 V)}:
   \begin{itemize}
   \item Band gap이 원래 값 (예: GeSi의 경우 약 0.8 eV = 1550 nm에 해당)
   \item 1550 nm 빛은 band gap보다 약간 낮은 energy → 거의 흡수되지 않음
   \item Material이 transparent → 빛이 통과
   \end{itemize}

\item \textbf{전압 인가 (수 volt)}:
   \begin{itemize}
   \item Strong electric field가 band edge를 "기울인다" (tilt)
   \item Effective band gap이 감소하여 1550 nm 빛의 photon energy가 band gap보다 커져 빛이 흡수되어 차단됨
   \end{itemize}

\item \textbf{핵심}: Electric field로 band gap을 조절하여 특정 wavelength에 대한 absorption을 on/off.
\end{enumerate}

\vspace{2mm}

\textbf{D. GeSi EAM의 구조}

Silicon photonics에서 사용되는 EAM은 일반적으로 Germanium-Silicon (GeSi) 합금 기반이다:

\begin{itemize}
\item \textbf{Active region}: GeSi layer (수백 nm 두께)가 waveguide 옆이나 위에 배치된다.

\item \textbf{전극 구조}: p-i-n diode 또는 p-n junction 형태로 전압을 가한다.

\item \textbf{Waveguide coupling}: 빛이 GeSi region을 통과하며, absorption이 on/off되면서 intensity가 modulate된다.

\end{itemize}

\vspace{2mm}

\textbf{E. C-band vs O-band - Band Gap의 제약}

GeSi EAM의 가장 큰 제약은 natural band gap이 C-band에 맞춰져 있다는 것이다:

\begin{itemize}
\item \textbf{GeSi natural band gap}: 약 0.8 eV → 1550 nm 근처 (C-band: 1530-1565 nm)

\item \textbf{O-band 문제}: O-band (1260-1360 nm)는 더 짧은 wavelength (높은 energy)이다.

\item \textbf{결과}: GeSi EAM은 C-band에 lock-in된다. 이는 ecosystem compatibility 문제를 야기한다.
\end{itemize}

\vspace{2mm}

\textbf{F. InP EAM - O-band의 대안}

InP (Indium Phosphide) 기반 EAM은 O-band에 자연스럽게 맞는다:

\begin{itemize}
\item \textbf{InP band gap}: 약 0.95 eV → 1310 nm 근처 (O-band)

\item \textbf{EML (Electro-Absorption Modulated Laser)}: CW DFB laser + InP EAM을 하나의 package로 통합한 것. Datacom transceiver (예: 800G DR8)에서 널리 사용된다.

\item \textbf{장점}: O-band ecosystem 활용 가능, 성숙한 기술, 높은 interoperability.

\item \textbf{단점 (CPO에 부적합)}:
  \begin{itemize}
  \item 작은 wafer size (3", 6") → 높은 unit cost
  \item InP를 silicon에 coupling하기 어려움
  \item Silicon photonics PIC에 통합하기 복잡
  \end{itemize}
\end{itemize}

\vspace{2mm}

\textbf{G. 장점의 이유}

\begin{itemize}
\item \textbf{Thermal robustness}:
  \begin{itemize}
  \item Absorption은 band gap에 의해 결정되고, band gap은 온도에 상대적으로 덜 민감하다. 반면 MRM은 Resonance position이 온도에 극도로 민감 (0.1 nm/°C)
  \item EAM: 35°C instant shift 견딤, 80°C ambient 견딤을 허용하여 XPU 아래 interposer에 배치 가능 (수백 watt dissipation)
  \end{itemize}

\item \textbf{MZM 대비 작고 낮은 전력}:
  \begin{itemize}
  \item Size: 250 µm² vs 12,000 µm²
  \item MZM의 큰 크기 → 높은 voltage swing (0-5V) → high power consumption
  \item EAM은 compact하여 낮은 voltage와 전력으로 동작
  \end{itemize}
\end{itemize}

\vspace{2mm}

\textbf{H. 단점의 이유}

\begin{itemize}
\item \textbf{GeSi reliability 우려}: Germanium은 silicon보다 mechanical stability와 endurance가 낮다

\item \textbf{C-band lock-in}: 대부분의 datacom은 O-band 사용하나 GeSi EAM은 C-band만 가능 → O-band chiplet ecosystem에 참여 불가

\item \textbf{높은 insertion loss (4-5 dB)}: Absorption mechanism 자체가 빛을 소모하기에 MRM/MZM (3-5 dB)보다 약간 높음

\item \textbf{별도 multiplexer 필요}: MRM은 Built-in wavelength selectivity로 자체 mux/demux가 가능하나 
EAM은 Wavelength selectivity 없어 CWDM/DWDM 위해 별도 component 필요

\end{itemize}

\vspace{2mm}

\textbf{I. Celestial AI의 전략적 선택}

\begin{itemize}
\item \textbf{Application의 특수성}: Optical chiplet을 XPU 아래 interposer에 직접 배치한다.
  \begin{itemize}
  \item Extreme thermal environment (수백 watt dissipation)
  \item MRM은 이 환경에서 작동 불가
  \item EAM의 35°C instant thermal tolerance가 결정적
  \end{itemize}

\item \textbf{Niche positioning}: High-bandwidth compute chiplet interconnect에 특화.
  \begin{itemize}
  \item Network switch (NVIDIA)와는 다른 use case
  \item Thermal tolerance가 scalability보다 중요
  \end{itemize}
\end{itemize}

\end{tcolorbox}

\vspace{2mm}



   
\begin{tcolorbox}[breakable,colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: Modulator 선택 - 성능과 Scalability의 Trade-off}]

  \vspace{2mm}
  
  \textbf{산업적 함의}: NVIDIA와 TSMC의 MRM 선택은 단순한 기술 선호가 아니다. MRM은 구현 난이도가 높지만, TSMC의 advanced CMOS expertise와 결합하여 high-precision, 훌륭한 Q-factor 달성이 가능하다. 이는 결국 \textbf{MRM의 potential bandwidth density 우위}가 CPO scaling의 핵심이 될 것임을 시사한다. 반면 EAM의 thermal advantage는 특정 niche application (co-packaged compute chiplet)에서 차별화 요소가 될 수 있다.
  
\end{tcolorbox}
  


\subsection{OE roadmap -- scaling OEs}
오늘날 사용 가능한 Optical Engine은 일반적으로 1.6T에서 3.2T 사이의 aggregate bandwidth를 제공한다. NVIDIA의 Quantum CPO는 1.6T engine을 포함하며, Spectrum용 3.2T 버전이 계획되어 있다. Broadcom은 Bailly용 6.4T OE를 선보였지만 form factor가 매우 크고 (NVIDIA의 2-3배 너비) 두 개의 FAU를 필요로 하므로 bandwidth density는 NVIDIA의 제공과 유사할 수 있다. Marvell의 6.4T OE도 동일한 경우로 2개의 FAU를 필요로 하므로 큰 footprint를 차지한다. Marvell의 OE는 우리가 아는 한 곧 production system에 들어가지도 않는다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/688c123b-18fb-4010-a366-b97e38b350f0_978x418.png}
\caption{Source: SemiAnalysis}
\end{figure}
논의한 바와 같이 NVIDIA의 Spectrum-X photonics switch의 3.2T OE 구현은 long reach SerDes에 의해 구동되는 pluggable보다 더 많은 shoreline bandwidth density를 제공하지 않는다. 다시 말해 optical engine density는 설득력 있는 성능 이점을 제공하고 고객 채택을 추진하기 위해 여러 번 scale해야 한다. 이는 host silicon과 OE EIC 사이의 electrical interface를 scaling하는 것뿐만 아니라 fiber에서 나오는 bandwidth 양을 scaling하는 것을 의미한다.

하지만 만약 우리가 free hand로 차세대 interconnect를 design할 수 있다면 이 세대와 그 이후를 위해 더 큰 bandwidth를 가능하게 하는 접근 방식은 무엇일까?

\subsection{Key approaches for scaling bandwidth}

Co-packaged optical engine에서 bandwidth를 scaling하기 위한 주요 접근 방식을 논의해보자:

\begin{enumerate}
  \item Electrical SerDes 기반 PHY를 계속 사용: long reach SerDes 대신 \textbf{Short Reach (SR)} SerDes를 사용하여 더 간단한 design 구현, 감소된 면적 및 더 낮은 전력을 활용한다. 이것은 결국 runway가 부족한 electrical interface에서 SerDes speed에 의해 여전히 제한될 것이다. 여기서 아이디어는 silicon designer가 I/O를 재설계할 필요가 없도록 interim 솔루션을 사용하는 것이다. 추가로 electrical SerDes를 사용하면 동일한 silicon으로 기존 pluggable optics 및/또는 copper를 사용할 수 있는 유연성을 제공한다.

  \item NRZ modulation과 함께 56G와 같은 더 낮은 Baud rate에서 작동하는 UCIe와 같은 wide I/O PHY를 사용한다. 이것은 optical engine의 EIC에 덜 요구되며 낮은 속도에서 parasitic이 덜 문제가 되므로 \href{https://semianalysis.com/2024/02/09/hybrid-bonding-process-flow-advanced/#besi-ev-group-amat-tel-asmpt-set-shibaura-suss-microtec}{expensive hybrid bonding}의 필요성을 제거할 수도 있다. 그러나 낮은 signal rate를 사용하면 optical engine을 떠나는 fiber의 수가 더 빨리 bottleneck이 될 수 있다. wavelength-division multiplexing은 각 fiber가 parallel로 여러 data stream을 전달할 수 있게 함으로써 이를 해결하는 데 도움이 된다.

  \item UCIe와 같은 wide I/O PHY를 사용한 다음 EIC가 더 적은 수의 optical fiber lane으로 serialize하도록 한다. 각 optical lane의 속도를 최대화하기 위해 PAM4 modulation과 함께 high Baud rate를 계속 사용하고, WDM scheme을 사용하여 필요한 경우 여러 wavelength를 추가하여 bandwidth를 더욱 증가시키기 위해 fiber pair당 여러 lambda를 허용한다.

\end{enumerate}
Electrical 측면이 해결되면 다음 과제는 fiber를 통해 얼마나 많은 escape bandwidth를 전달할 수 있는가이다. 
전체 fiber bandwidth는 세 가지 주요 요소에 따라 달라진다: 1) fiber의 수 (optical lane을 정의함) 2) lane당 속도, 그리고 3) fiber당 wavelength 수 -- 각각은 scaling을 위한 vector를 나타낸다.

최근 업계는 개념을 두 가지 주요 접근 방식으로 나누었다: \textbf{Fast and Narrow vs Slow and Wide}. 
\textbf{Fast and Narrow}는 FAU당 더 적은 수의 fiber (기껏해야 높은 두 자릿수)와 각 fiber pair에서 빠른 link를 구상하는 반면, \textbf{Slow and Wide}는 훨씬 더 많은 수의 fiber pair (훨씬 더 finer pitch를 가진)와 개별 fiber pair당 훨씬 더 느린 bandwidth의 아이디어에 기반한다.

\begin{enumerate}
  \item \textbf{More fiber pairs}: Fiber density는 fiber pitch에 의해 제한되며 단일 FAU 내의 전체 fiber 수는 yield가 문제가 되기 전에 manufacturing 가능한 것에 의해 제한된다. 현재 fiber의 최소 pitch는 127 micron (µm)이며, mm당 최대 8개의 fiber를 의미한다. 업계는 일정 면적이 수용할 수 있는 fiber 수를 더욱 scaling하기 위해 80-µm pitch와 multicore fiber를 향해 작업하고 있다. 그러나 더 많은 fiber를 부착하는 것은 manufacturability 과제를 가져온다: 
  
 A) 여전히 많은 manual process를 포함하는 fiber를 정렬하는 것은 yield loss에 취약하며, FAU yield는 정렬해야 하는 각 successive fiber와 함께 저하된다. Ficontec과 같은 회사가 제공하는 automation tool이 있지만 여전히 낮은 throughput에 시달린다. 
  
 B) Coupling 선택도 중요하다: Edge coupling은 fiber array를 단일 row로 제한하는 반면, grating coupling은 여러 row를 지원할 수 있다. 현재 우리가 본 가장 sizeable한 fiber array는 36 fiber를 가진 Nubis의 2D FAU이다.

  \item \textbf{Speed per lane}: Lane speed에 영향을 미칠 수 있는 두 가지 dimension이 있다: 
  
 A) \textbf{Baud rate}: 초당 전송되는 symbol 수를 정의한다; 오늘날의 advanced system은 100 Gbaud에서 작동하는 반면, 업계는 200 Gbaud를 추진하고 있다. 그러나 더 높은 baud rate는 modulator가 더 높은 frequency에서 switch하도록 더 높은 요구사항을 부과한다; 다양한 type 중에서 MZM은 이 metric에서 가장 capable하며 200 Gbaud를 달성하는 상대적으로 명확한 경로를 가지고 있다. 
  
 B) \textbf{Modulation}: Symbol당 전달되는 bit 수를 정의한다. NRZ (symbol당 1 bit) 및 PAM4 (4개의 다른 amplitude를 통해 symbol당 2 bit)는 오늘날 널리 채택되고 있다. 연구는 PAM6 (\textasciitilde{}2.6 bits per symbol) 및 PAM8 (symbol당 3 bit)로 확장되고 있다. higher order modulation scheme은 여러 amplitude level 외에도 빛의 다른 phase를 사용하여 signaling함으로써 접근할 수 있다. DP-16QAM은 각각 4개의 다른 amplitude, 4개의 다른 phase를 가진 두 개의 orthogonal plane을 가능하게 하여 총 256개의 가능한 signal -- signal당 8 bit를 전달한다.

  \item \textbf{Wavelength Division Multiplexing (WDM)}: Optical fiber는 여러 wavelength의 빛을 동시에 전달할 수 있다. 예를 들어 각각 200Gbit/s로 데이터를 전달하는 8 wavelength를 가진 fiber는 1.6 Tbit/s의 aggregate capacity를 전송할 수 있다. 오늘날 상업적으로 사용 가능한 DWDM 솔루션은 일반적으로 8-lambda 또는 16-lambda configuration을 제공한다. 연구자들은 또한 lambda 수를 증가시키기 위해 broad-spectrum, band multiplexing 및 interlaying 기술을 탐색하고 있다. wavelength 수를 scaling하는 데 있어 한 가지 주요 과제는 여러 lane의 빛을 안정적이고 효율적으로 생성할 수 있는 reliable laser source를 개발하는 것이다. Ayar Labs의 Supernova light source는 16 wavelength가 가능한 laser를 가지고 있다 (laser는 Sivers가 공급). Scintil의 wafer-scale InP laser는 마찬가지로 최대 16 wavelength를 제공하며, Xscape Photonics는 최대 64 wavelength를 가진 tuneable comb laser를 개발하기 위해 노력하고 있다. modulator 중에서 MRM은 여러 wavelength를 처리하는 데 가장 적합하며 built in multiplexing (mux) 및 demultiplexing (demux) 기능을 가지고 있다.

\end{enumerate}
아래 표는 optical engine을 12.8T 이상으로 scaling하기 위한 여러 접근 방식을 설명한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/f8406576-5382-4acf-a463-2809401af721_2406x992.png}
\caption{Source: SemiAnalysis}
\end{figure}

\subsection{CPO adoption pace and deployment challenges}
NVIDIA의 첫 번째 CPO 제품은 2H 2025에 사용 가능한 InfiniBand CPO와 H2 2026에 사용 가능한 Ethernet CPO switch를 갖춘 backend scale-out switch용이 될 것이다. 
우리는 이 초기 단계가 주로 market test와 supply chain 성숙도를 높이기 위한 preparatory phase 역할을 할 것이라고 생각한다. 
우리는 2026년 total shipment volume이 10-15K unit 범위가 될 것으로 예상한다.

배포가 더 나아가고 더 빠르게 진행되어 진정으로 ubiquitous하게 되려면 CPO 채택에 대한 더 설득력 있는 rationale이 있어야 할 것이다. 
두 가지 가능성은 CPO 채택으로 인한 극적인 total cost of ownership 이점이거나, switch ASIC에서 switch box의 front panel로 signal을 구동하는 데 필요한 electrical LR SerDes가 hard speed 또는 range wall에 부딪히는 경우일 수 있다.
TCO 이점을 상쇄하는 것은 datacenter operator가 CPO 기반 system 배포에 대해 싫어하는 두 가지 주요 사항이다: interoperability의 부족과 serviceability 과제.

CPO의 과제는 package 외부와 전체 system으로 확장된다. 
Fiber 관리, front plate density, external laser는 모두 과제인 필수 부분이다. CPO를 가능하게 하려면 chip 회사가 고객이 배포할 수 있는 end-to-end 솔루션을 제공해야 한다. 이것은 특히 성능을 확장하기 위한 system design에 집중하고 있는 NVIDIA에서 우리가 보고 있는 추세의 연속이다.

\subsection{Proprietary solutions vs standards}







\begin{tcolorbox}[breakable,colback=blue!10!white,colframe=blue!40!black,title=\textbf{핵심 인사이트: CPO의 근본적 딜레마 - Interoperability vs Innovation Speed}]

CPO는 pluggable optics의 성공 model과 근본적으로 다른 paradigm을 요구하며, 이는 업계에 심각한 딜레마를 제시한다.

\vspace{2mm}
\textbf{Pluggable의 성공 공식}:
- Electrical: OIF가 표준 관리

- Optical: IEEE 802.3 PMD layer가 modulation, lane speed, wavelength 등 정의

- Mechanical: MSA가 form factor와 multi-vendor interoperability 보장

→ 결과: 완벽한 plug-and-play, multi-vendor ecosystem, 경쟁적 가격

\vspace{2mm}
\textbf{CPO의 현실}:
CPO는 단순히 optics를 package 내부로 이동시킨 것이 아니라 전체 \textbf{system-level 문제}가 된다:

- Fiber management가 chassis 내부로 이동

- Front plate density 최적화 필요

- Modulator architecture 선택이 wavelength band 결정 (O-band vs C-band)

- External laser module 통합

- FAU coupling precision과 thermal management

\vspace{2mm}
이러한 복잡성 때문에 NVIDIA와 같은 vendor는 \textbf{end-to-end 솔루션}을 제공해야 한다. 고객이 여러 vendor의 component를 mix-and-match할 수 없다.

\vspace{2mm}
\textbf{산업적 함의}: 역설적이게도, CPO의 광범위한 채택은 초기에 proprietary solution의 성공에 달려 있을 수 있다. NVIDIA와 같은 major vendor가 volume을 창출하고 supply chain을 성숙시켜야만, 그 다음에 표준화와 interoperability 논의가 의미를 가질 것이다. Pluggable이 즉시 표준화된 것이 아니라 GBIC → SFP → SFP+ → QSFP → QSFP-DD로 진화했듯이, CPO도 유사한 경로를 밟을 가능성이 높다.

\end{tcolorbox}

CPO 채택에 대한 한 가지 주요 과제는 잘 확립되고 매우 상호운용 가능한 pluggable optics model에 대한 업계의 뿌리 깊은 의존을 극복하면서 interoperability를 달성하는 것이다.
Interoperability의 세 가지 주요 flavor가 있다: (1) electrical, (2) optical, 그리고 (3) mechanical 
Pluggable의 경우 interoperability는 다음과 같은 특성이 있다:

\begin{enumerate}
  \item 일반적으로 \textbf{Optical Internetworking Forum (OIF)}에 의해 처리된다,

  \item 일반적으로 IEEE (때로는 OIF)에 의해 처리된다. IEEE는 Ethernet \textbf{Physical Medium Dependent (PMD)} layer를 정의하는 IEEE 802.3 standard를 통해 중심 역할을 한다. 이러한 specification은 modulation format, lane speed, lane count, reach 및 media type, 그리고 optical signal을 위한 wavelength와 같은 주요 parameter를 다룬다. 이러한 표준화된 PMD를 준수함으로써 다른 vendor의 transceiver는 상호 교환 가능하게 작동하여 multi-vendor ecosystem 전체에서 진정한 plug-and-play compatibility를 보장한다,

  \item 그리고 일반적으로 \textbf{Multi-Source Agreement (MSA)}에 의해 처리된다. MSA는 specialized 솔루션을 정의하고 공식 IEEE standard 외부에서 multi-vendor interoperability를 보장한다.

\end{enumerate}
OIF, IEEE standard 및 MSA의 조합을 통해 pluggable transceiver는 광범위한 interoperability와 강력한 multi-vendor ecosystem을 달성한다. CPO의 경우:

\begin{enumerate}
  \item CPO module이 electrically하게 compliant한 것이 critical한데, 그렇지 않으면 state-of-the-art SerDes와 통신할 수 없기 때문이다.

  \item Optical에 대한 compatibility는 유용한데, cluster의 다른 곳에서 standard pluggable과 호환될 수 있기 때문이다.

  \item CPO가 \textbf{wild west} 단계에 있으며, 일부 솔루션과 architectural decision이 완전히 proprietary form factor로 이어지고 있다는 것을 이해하는 것이 중요하다. 이것이 새로운 OIF high-density interconnect 노력 (CPX paradigm과 같은)이 해결하려고 하는 것이다.

\end{enumerate}
(1)+(2)+(3)가 충족되면 CPO는 운영상 pluggable과 매우 유사해질 수 있으며, 이는 광범위한 채택을 가능하게 하는 데 도움이 될 것이다.

그러나 현재 CPO는 아직 pluggable만큼 standard를 수용하지 않고 있으며 optical transceiver가 할 수 있는 정도로 interoperability를 보장할 수 없다. 과제의 일부는 vendor가 box maker에게 silicon만을 판매하는 것이 아니라 system level 솔루션을 추진하고 있다는 것이다. 이는 CPO의 과제가 package 외부와 전체 system으로 확장되기 때문이다. fiber 관리, front plate density, modulator architecture 및 external laser는 모두 과제인 필수 부분이다. CPO 채택을 bootstrap하기 위해 NVIDIA와 같은 기업은 end-to-end 솔루션을 제공하는 것으로 시작해야 한다.

이를 위한 한 가지 접근 방식은 co-packaged OE가 wavelength, speed 및 modulation에 대한 Ethernet standard 또는 MSA와 align하는 photonic component---laser, modulator 및 photodiode와 같은---와 함께 표준화된 fiber interface를 따르는 component level에서 표준화된 솔루션을 채택하는 것일 수 있다. 이는 진정한 interoperability를 가능하게 하여 고객이 한 vendor에서 모든 장비를 조달할 필요 없이 다양한 vendor의 제품을 mix and match할 수 있게 한다. OE가 socketable하다면 operator는 failure의 경우 쉽게 교체할 수 있다. 그러한 표준화는 또한 고객을 위한 비용을 낮출 수 있는 더 경쟁적이고 강력한 multi-vendor 시장을 만들 것이다. 또한 고객이 한 vendor에 얽매이지 않고 lead time, 가격 또는 고유한 기능을 기반으로 다양한 vendor를 선택할 수 있게 한다.



\begin{tcolorbox}[breakable,colback=blue!5!white,colframe=blue!75!black,title=\textbf{기술 용어: Interoperability - 상호운용성의 이해}]

  \textbf{Interoperability (상호운용성)}: 서로 다른 vendor나 manufacturer가 생산한 component, system, device들이 표준화된 interface와 protocol을 통해 함께 작동할 수 있는 능력. IT 산업에서 multi-vendor ecosystem과 경쟁적 시장을 가능하게 하는 핵심 원칙이다.
  
  \vspace{2mm}
  
  \textbf{A. Interoperability의 세 가지 차원}
  \vspace{2mm}

  \textbf{1. Electrical Interoperability}:
  \begin{itemize}
  \item Signal voltage level, timing, protocol이 표준화되어 있어 다른 vendor의 chip들이 전기적으로 통신 가능
  \item 예: PCIe, Ethernet PHY layer - Intel chip과 AMD chip이 동일한 electrical interface로 통신
  \end{itemize}
  
  \textbf{2. Optical Interoperability}:
  \begin{itemize}
  \item Wavelength, modulation format, encoding scheme이 표준화
  \item 예: IEEE 802.3 PMD (Physical Medium Dependent) layer가 100GBASE-SR4의 850nm wavelength, NRZ modulation 정의
  \end{itemize}
  
  \textbf{3. Mechanical Interoperability}:
  \begin{itemize}
  \item Physical form factor, connector type, pinout, thermal spec이 표준화
  \item 예: QSFP-DD MSA (Multi-Source Agreement)가 module 크기, cage design, heat dissipation 정의
  \end{itemize}
    
  \textbf{B. Pluggable Optics(SFP, QSFP, OSFP)에서의 성공 사례}
    
  \begin{itemize}
  \item \textbf{Plug-and-play}: 고객이 Finisar, Lumentum, Innolight 등 어느 vendor의 transceiver든 구매하여 Cisco, Arista, Juniper 등 어느 switch에든 꽂을 수 있다.
  
  \item \textbf{Hot-swap}: System 전원을 끄지 않고 transceiver 교체 가능.
  
  \item \textbf{Multi-vendor sourcing}: 고객이 가격, 품질, 공급 안정성을 기준으로 여러 vendor 중 선택 가능. 이는 경쟁을 촉진하고 가격을 낮춘다.
  
  \item \textbf{표준화된 ecosystem}: 누구나 MSA spec을 따라 product를 만들 수 있어 innovation과 competition 활성화.
  \end{itemize}
  
  \textbf{C. CPO에서의 Interoperability 도전}
  
  \vspace{2mm}
  
  \textbf{System-level integration}:
  \begin{itemize}
  \item Optical engine이 package 내부에 permanently bonded
  \item OE architecture (MRM/MZM/EAM)가 wavelength band (O/C-band) 결정
  \item Modulator 선택이 laser source와 tightly coupled
  \item FAU design이 coupling method (GC/EC)와 연결
  \item Chassis 내부 fiber routing, thermal management, power distribution이 모두 상호 의존적
  \end{itemize}
  
  \textbf{결과}: Component 하나를 변경하면 전체 system re-design 필요. 고객이 "NVIDIA OE + Broadcom laser + Lumentum FAU"처럼 mix-and-match 불가능.
  
  \vspace{2mm}
  
  \textbf{D. 역사적 교훈: Pluggable의 진화}
  
  \vspace{2mm}

  Pluggable도 처음부터 완전히 표준화되지 않았다:
  \begin{itemize}
  \item GBIC (1990s) → 각 vendor 독자 구현
  \item SFP (2000s) → MSA로 mechanical 표준화 시작
  \item SFP+, QSFP (2010s) → IEEE 802.3이 optical spec 추가
  \item QSFP-DD (2020s) → 완전한 interoperability 달성
  \end{itemize}
  
  이 과정은 약 20년 소요되었고, 초기에는 proprietary solution이 시장을 개척했다.
  
  \vspace{2mm}
  
  \textbf{E. CPO의 가능한 경로}
  
  \begin{enumerate}
  \item \textbf{Phase 1 (현재-2027)}: Proprietary solution 주도
     - NVIDIA Spectrum-X, Broadcom Bailly는 각자의 closed ecosystem
     - Early adopter (hyperscaler)가 single vendor와 긴밀히 협력
     - Volume 증가로 supply chain 성숙
  
  \item \textbf{Phase 2 (2028-2030)}: De facto standard 등장
     - 시장 리더의 architecture가 reference가 됨
     - Follower들이 compatible solution 개발
     - Partial interoperability (예: 동일 wavelength band끼리)
  
  \item \textbf{Phase 3 (2030+)}: Full standardization
     - OIF CPX 같은 formal standard 완성
     - Multi-vendor ecosystem 형성
     - Commoditization 시작
  \end{enumerate}
  
  \textbf{핵심 통찰}: Interoperability는 CPO 채택의 필수 조건이 아니다. 오히려 초기에는 proprietary solution의 성공이 long-term interoperability의 foundation을 만들 수 있다. NVIDIA 같은 vendor가 volume을 만들어야 그 다음 standardization이 의미를 갖는다.
  
  \end{tcolorbox}
  




\subsection{Serviceability and reliability}

CPO가 광범위하게 채택되기 전에 해결되어야 하는 문제는 fiber coupling에서 나온다. 
Pluggable optical module에서 silicon photonics device에 fiber를 연결하는 것은 상대적으로 간단한 반면, 
CPO를 위해 optical engine에 fiber를 coupling하는 것은 훨씬 더 어렵다. 
CPO system에서 fiber는 chip의 매우 작은 waveguide (종종 width와 height 모두 submicron)로 빛을 couple하기 위해 sub-micron accuracy로 정밀하게 정렬되어야 하는 반면, pluggable은 pre-aligned, 표준화된 connector를 사용하기 때문에 덜 번거롭다. 또한 CPO 기반 system에서 fiber coupling은 cramped하고 thermally active한 switch chassis 내에서 발생하는 반면, pluggable의 경우 fiber coupling이 device의 main package에서 멀리 떨어진 곳에서 발생한다.

Copper 기반 electrical communication은 optical과 비교할 때 열등한 insertion loss와 signal integrity로 고통받지만, copper는 일반적으로 다른 방식으로 reliable하다. optical device는 본질적으로 temperature sensitive하다. 
Operating temperature의 변화는 laser wavelength를 변경하고, component efficiency를 감소시키며, 
reliability에 부정적인 영향을 미칠 수 있으며, 종종 specialized temperature control 또는 calibration mechanism을 필요로 한다. 
추가로 photonic component는 또한 시간이 지남에 따라 자연스럽게 저하되며, 
aging, contamination 또는 mechanical stress로 인해 점진적으로 optical efficiency를 잃어 장기 reliability를 더욱 어렵게 만든다. 
Dust, humidity 및 mechanical disturbance와 같은 environmental factor는 copper 기반 electrical link보다 더 취약한 optical system에 불균형적으로 영향을 미친다.

이러한 environmental 및 operational 과제는 물리적 fiber 자체로 직접 확장된다. 
Fiber 성능은 특히 bending과 같은 물리적 disturbance에 매우 민감하며, 이는 optical insertion loss를 증가시킬 뿐만 아니라 breakage와 failure를 가속화한다. 
여러 fiber array를 포함하는 chassis에서(일반적으로 engine당 2개, connector용 하나와 light source용 ELS의 하나), 각 array는 엄격한 topological 고려사항과 함께 routing되어야 한다. 
모든 개별 fiber link는 faceplate에서 각 optical engine까지의 거리 variation과 인접한 array에 의해 부과된 routing constraint를 고려하여 고유한 길이를 필요로 한다.

아래 Quantum-X CPO switch의 close-up 이미지에서 OE에서 나오는 fiber ribbon이 fiber를 적절하게 관리하기 위해 fiber cassette를 통해 routing되어야 함을 볼 수 있다. FAU는 교체가 필요한 breakage를 provision하기 위해 detachable하다. 그러나 switch 내에서 fiber의 더 복잡한 routing은 fiber/FAU 교체가 face plate의 front에서 단순히 hot swap하는 고장난 pluggable transceiver를 교체하는 것보다 훨씬 더 부담스럽다는 것을 의미한다. CPO switch의 경우 engineer는 box/chassis 내부로 들어가 고장난 FAU를 제거한 다음 cassette를 통해 새 FAU를 적절히 재부착해야 한다. 이것은 다른 fiber를 방해하지 않고 수행되어야 한다. NVIDIA가 CPO의 reliability 이점을 강조해 왔지만, serviceability는 더 길게 논의할 가치가 있는 또 다른 요소이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/4c4bc227-338d-431e-8342-6ca63e6cf631_1024x516.jpg}
\caption{Source: NVIDIA}
\end{figure}











\newpage{}

\section{Part 4: CPO Products of Today and Tomorrow}
이 part에서 우리는 오늘날 시장에 있거나 곧 시장에 출시될 CPO 제품을 소개하는 것으로 시작하여 NVIDIA와 Broadcom의 portfolio로 시작하고 다양한 CPO 중심 기업의 offering을 설명하는 것으로 이동할 것이다. \textbf{Intel CPO, MediaTek의 CPO 작업,} \textbf{Ayar Labs, Nubis, Celestial AI, Lightmatter, Xscape Photonics, Ranovus 및 Scintil}을 다루며, 각 제공업체의 솔루션을 상세히 설명하고 각 기업의 접근 방식에 대한 중요한 장단점을 평가한다. 마지막으로 Optical Engine 및 External Laser Source와 같은 주요 CPO component의 manufacturing, testing 및 assembly를 다루는 supply chain을 논의하기 위해 다시 돌아올 것이다.

\subsection{NVIDIA CPO}
GTC 2025에서 NVIDIA는 scale-out network를 위한 첫 번째 CPO 기반 switch를 선보였다. 세 가지 다른 CPO 기반 switch가 발표되었다. 우리는 각각을 차례로 살펴볼 것이지만, 먼저 모든 중요한 specification을 집계하는 깔끔한 표를 제시한다:

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/3d2d3acc-88bc-43ac-a366-43ccb9e17757_1028x730.png}
\caption{Source: SemiAnalysis}
\end{figure}

\subsubsection{Quantum-X Photonics}
2H 2025까지 시장에 나올 첫 번째 CPO switch는 Quantum X800-Q3450이 될 것이다. 144개의 Physical MPO port를 특징으로 하며 이는 800G의 144 logical port 또는 1.6T의 72 logical port를 가능하게 하여 115.2T의 aggregate bandwidth를 제공한다. spaghetti monster와의 유사성은 저자들의 배를 고프게 한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/8bf88c10-2a58-426b-8cf6-0efd4fbac46a_1024x518.jpg}
\caption{Source: NVIDIA}
\end{figure}
Quantum X800-Q3450은 multi-plane configuration에서 각각 28.8 Tbit/s bandwidth를 가진 4개의 Quantum-X800 ASIC chip을 사용하여 이 높은 radix와 높은 aggregate bandwidth를 달성한다. 이 multi-plane configuration에서 각 physical port는 4개의 switch ASIC 각각에 연결되어 4개의 다른 switch ASIC를 통해 모든 4개의 200G lane에 걸쳐 데이터를 spraying하여 모든 physical port가 다른 port와 통신할 수 있게 한다.

3-layer network의 maximum cluster size와 관련하여 이는 이론적으로 200G logical port size를 가진 28.8T switch box의 4배를 사용하는 것과 동일한 최종 결과를 제공한다.
둘 다 746,496 GPU의 maximum cluster size를 허용한다. 
차이점은 X800-Q3400 switch를 사용할 때 shuffle이 switch box 내부에서 깔끔하게 발생하는 반면, discrete 28.8T switch box로 동일한 network를 설정하려면 훨씬 더 많은 수의 destination으로 가는 훨씬 더 많은 개별 fiber cable이 필요하다는 것이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/0d78b0cb-7092-4124-acfe-6003ce26aa6a_1014x386.png}
\caption{Source: SemiAnalysis}
\end{figure}
Quantum-X800-Q3450의 각 ASIC는 6개의 detachable optical sub-assembly로 둘러싸여 있으며, 각 sub-assembly는 3개의 optical engine을 수용한다. 
각 optical engine은 1.6 Tbit/s의 bandwidth를 제공하여 ASIC당 총 18개의 optical engine과 ASIC당 28.8 Tbit/s의 aggregate optical bandwidth를 생성한다. 
이러한 sub-assembly는 detachable하므로 purist는 이것을 엄격히 \textbf{CPO}가 아니라 기술적으로 \textbf{NPO}로 간주할 수 있다. 
detachable OE와 관련된 약간의 extra signal loss가 있지만, 실제로 우리는 이것이 성능에 상당한 영향을 미치지 않을 것이라고 믿는다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/c2654220-e852-4fb3-b247-d5615bba8a11_1024x516.jpg}
\caption{Source: NVIDIA}
\end{figure}
각 engine은 8개의 electrical 및 optical channel로 작동하며, 
electrical 측면에서 200G PAM4 SerDes에 의해 구동되고 optical 측면에서 8개의 \textbf{Micro-Ring Modulator (MRM)}가 modulator당 200G를 달성하기 위해 PAM4 modulation을 사용한다. 
이 design 선택은 발표의 큰 takeaway 중 하나였다. 즉, NVIDIA와 TSMC가 production에서 200G MRM을 출하할 수 있다는 것. 
이것은 오늘날 가장 빠른 MZM과 일치하며 MRM이 NRZ modulation으로 제한된다는 업계 notion을 반증한다. 
이 milestone에 도달한 것은 NVIDIA의 매우 인상적인 engineering 성과이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/a9ca5427-41f9-470b-a6b7-9741318da128_1024x507.jpg}
\caption{Source: NVIDIA}
\end{figure}
각 optical engine은 mature N65 process node에 구축된 \textbf{Photonic Integrated Circuit (PIC)}와 advanced N6 node에서 fabrication된 \textbf{Electronic Integrated Circuit (EIC)}를 통합한다. 
PIC는 modulator, waveguide 및 detector와 같은 optical component를 포함하기 때문에 older node를 활용한다 (Scaling으로 이점을 얻지 못하고 종종 더 큰 geometry에서 더 나은 성능을 발휘하는 device)
이와 대조적으로 EIC는 advanced node가 가능하게 하는 더 높은 transistor density와 향상된 power efficiency로부터 상당한 이점을 얻는 driver, TIA 및 control logic을 포함한다. 
이 두 die는 그런 다음 TSMC의 COUPE platform을 사용하여 hybrid bonding되어 photonic과 electronic domain 사이에 ultra-short, high-bandwidth interconnect를 가능하게 한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/5757b5de-38e7-4890-a72d-68e5842e9b6b_1024x519.jpg}
\caption{Source: NVIDIA}
\end{figure}
Quantum-X800-Q3450의 ASIC 위에 2개의 copper cold plate가 각 switch ASIC에서 열을 효율적으로 dissipate하는 closed-loop liquid cooling system의 일부로 위치한다. 
Cold plate에 연결된 black tubing은 coolant fluid를 순환시켜 thermal stability를 유지하는 데 도움을 준다. 
이 cooling system은 ASIC뿐만 아니라 인접한 temperature-sensitive co-packaged optics의 thermal stability를 유지하는 데 필수적이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/551ef71b-ffdc-4595-a845-0998b288987b_1024x517.jpg}
\caption{Source: NVIDIA}
\end{figure}

\subsubsection{Spectrum-X Photonics}
Spectrum-X Photonics는 2026년 후반에 출시될 예정이며, 102.4T aggregate bandwidth를 가진 X800-Q3450 CPO switch의 Ethernet Spectrum-X variant, 102.4T aggregate bandwidth를 제공하는 Spectrum 6810, 
그리고 4개의 discrete Spectrum-6 \textbf {Multi-Chip Module (MCM)}을 사용하여 409.6T aggregate bandwidth를 제공하는 더 큰 cousin Spectrum 6800의 두 가지 별도 switch configuration 제품이 출시될 것이다.

Quantum X800-Q3450 CPO switch는 multi-plane configuration에서 physical에 연결된 4개의 discrete switch package를 활용하며, 
각 switch package는 required SerDes 및 기타 electrical component와 함께 28.8T switch ASIC를 포함하는 monolithic die이다. 
이와 대조적으로 Spectrum-X Photonics switch silicon은 중앙에 훨씬 더 큰 reticle size 102.4T switch ASIC가 있고 각 측면에 2개씩 8개의 224G SerDes I/O chiplet로 둘러싸인 MCM이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/3a9875aa-e356-4cd2-b3c0-48bf42a37882_1024x566.jpg}
\caption{Source: NVIDIA}
\end{figure}
각 Spectrum-X photonics multi-chip module switch package는 단일 102.4T switch package에 36개의 optical engine을 가질 것이다. 
이 package는 각각 200G의 16 optical lane을 가진 NVIDIA의 2세대 optical engine을 3.2T bandwidth로 사용할 것이다. 
32개의 optical engine만 active하며, OE가 실패할 경우를 대비한 redundancy 목적으로 추가 4개가 있다는 점에 유의하라. 
이는 OE가 substrate에 soldering되어 쉽게 교체할 수 없기 때문이다.

각 I/O chiplet은 64 SerDes lane으로 구성된 총 12.8T의 unidirectional bandwidth를 제공하며 각각 4개의 OE와 interface한다. 
이것이 Spectrum-X가 SerDes를 위한 훨씬 더 많은 shoreline과 면적으로 Quantum-X Photonics보다 훨씬 더 많은 aggregate bandwidth를 제공할 수 있게 하는 것이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/ee32beb9-466d-4d09-a93b-9f794a92d111_1024x899.jpg}
\caption{Source: SemiAnalysis}
\end{figure}
Spectrum-X 6810 Switch Box는 위의 switch package 1개 unit을 사용하여 102.4T의 aggregate bandwidth를 제공한다. 
더 큰 Spectrum-X 6800 Switch Box SKU는 multi-plane configuration에서 external physical port에도 연결된 위의 Spectrum-X switch package 4개를 활용하여 달성된 409.6T의 aggregate bandwidth를 가진 high-density chassis이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/e9451cff-2100-4514-962c-67794025c755_1346x760.jpg}
\caption{Source: NVIDIA}
\end{figure}
4개의 ASIC 115.2T Quantum X800-Q3450과 마찬가지로 Spectrum-X 6800은 각 port를 모든 4개의 ASIC에 물리적으로 연결하기 위해 internal breakout을 사용한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/8608ea19-5780-407b-aa8f-bc6ac1575ff6_1003x386.png}
\caption{Source: SemiAnalysis}
\end{figure}

\subsection{The Broadcom CPO Switch Portfolio}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/c83d20f5-3927-46a0-9797-c3d8a6e15eb4_1960x1270.jpg}
\caption{Source: SemiAnalysis}
\end{figure}
Broadcom은 실제 CPO 지원 system을 제공한 최초 기업 중 하나이며 따라서 CPO의 leader로 간주된다. 
Broadcom의 1세대 CPO device인 Humboldt는 주로 proof of concept 역할을 했다.
\textbf{TH4-Humboldt}로 불리는 이것은 총 capacity를 전통적인 electrical connection과 CPO 사이에 균등하게 나누는 25.6Tbit/s Ethernet switch이다. 
그 중 12.8Tbit/s는 각각 100 Gbit/s의 32 lane을 제공하는 4개의 3.2 Tbit/s optical engine에 의해 처리된다. 
이러한 copper와 optics의 hybrid design에는 몇 가지 두드러진 use case가 있다. 
한 시나리오에서 \textbf{Top-of-Rack (ToR)} switch는 nearby server로의 short-distance copper connection을 위해 electrical interface에 의존하는 반면, optical port는 다음 switching tier로 uplink한다.
다른 시나리오에서 aggregation layer에서 electrical port는 rack 내의 다양한 switch를 interconnect하고, optical link는 해당 layer 위 또는 아래의 switching tier로 확장된다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/cfbbea34-30f8-4ba8-a8a0-d1765d3b54c7_1024x571.jpg}
\caption{Source: Broadcom}
\end{figure}
이 design에서 Broadcom은 \textbf{Silicon Germanium (SiGe)} EIC를 사용했지만 다음 세대 (즉, Bailly)에서는 \textbf{CMOS}로 전환했다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/8ac5c9f5-2304-4cb0-b871-43f438424dcb_1024x573.jpg}
\caption{Source: Broadcom}
\end{figure}
Broadcom의 2세대 CPO device인 Bailly는 half-optical predecessor와 달리 전적으로 optical I/O에 의존하는 51.2 Tbit/s Ethernet switch이다. 
각각 100 Gbit/s의 64 lane을 제공하는 8개의 6.4Tbit/s optical engine으로 구성된다. 
또 다른 주목할 만한 변화는 SiGe EIC를 사용하는 대신 이제 7nm CMOS EIC를 사용한다는 것이다. 
CMOS EIC로 이동하면 추가 control logic이 있는 더 복잡하고 통합된 design이 가능해졌으며, 이는 차례로 더 높은 lane count를 가능하게 했다 (이전 32 lane에서 새로운 optical engine에서 64 lane까지 scaling)

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/ebce9581-dcb9-4654-899b-81bcdcb5f0a3_1024x578.jpg}
\caption{Source: Broadcom}
\end{figure}
1세대에서 2세대로의 또 다른 주목할 만한 shift는 TSV process에서 \textbf{Fan-Out Wafer-Level Packaging (FOWLP)}으로의 전환이다. 
이 design에서 EIC는 PIC까지 signal을 routing하기 위해 textbf{Through-Mold Via (TMV)}를 활용하는 반면 copper pillar bump는 substrate에 연결한다. 
FOWLP를 채택하는 주요 이유는 mobile handset 시장에서 이미 입증되었고 OSAT에 의해 널리 지원되어 기술에 더 큰 scalability를 제공하기 때문이다. 
ASE/SPIL이 이 FOWLP process의 OSAT 파트너였다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/d6854e20-b645-410a-bba0-a6c3edd00026_1024x572.jpg}
\caption{Source: Broadcom}
\end{figure}
Broadcom은 Hot Chips 2024에서 하나의 logic die, 두 개의 HBM stack 및 SerDes tile과 함께 package에 6.4 Tbit/s optical engine을 통합하는 experimental design을 공개했다. 
그들은 substrate의 east 및 west edge에 HBM을 배치하는 fan-out 접근 방식을 제안하여 동일한 package에 2개의 optical engine을 위한 공간을 허용했다. 
CoWoS-S에서 CoWoS-L로 이동하면 edge에서 100 mm를 초과하는 substrate로 이동한다. 
따라서 최대 4개의 optical engine을 수용하고 51.2 Tbit/s의 bandwidth를 달성할 수 있을 것이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/d9d20033-2dfd-4e49-944a-7ba61d60042b_1382x762.png}
\caption{Source: Broadcom}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/4f98a480-c54d-4603-8563-f9e0fd8a7fa9_1404x784.png}
\caption{Source: Broadcom}
\end{figure}
올해 Broadcom은 16개의 6.4T OE를 통합하는 Tomahawk 6 기반 Davisson CPO switch를 출시하고 있다. 
Switch ASIC는 TSMC의 N3 process node를 사용하여 fabrication되며 package당 102.4 Tbit/s의 bandwidth를 제공한다. 
Broadcom은 box assembly를 위해 Micas 및 Celestica와 같은 \textbf{Contract Manufacturer (CM)}를 사용한다. 
추가로 NTT Corp (Japan)은 Broadcom의 TH6 bare die를 구매하고 Broadcom에서 조달하지 않은 proprietary OE 및 optical 솔루션을 사용하여 자체 CPO system을 구축하고 있는 것으로 알려져 있다. 
이 접근 방식은 TH6 기반 CPO system에 대한 잠재적 business 기회를 확대하고 더 open한 vendor ecosystem을 장려한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/73c18781-37ad-4019-ae51-37dbd121e778_3982x2067.jpg}
\caption{Source: SemiAnalysis}
\end{figure}
Scale-up fabric에서 CPO에 대한 더 큰 가치를 보기 때문에 Broadcom이 제공하는 첫 번째 mass-produced CPO system은 고객의 AI ASIC에 있을 것이라고 믿는다. 
CPO에 대한 Broadcom의 경험은 중기적으로 ASIC roadmap에서 CPO를 보는 고객에게 매력적인 design 파트너로 만든다. 
우리는 이것이 OpenAI가 Broadcom을 선택하게 만든 주요 요인이었다고 이해한다. 
흥미롭게도 Broadcom의 가장 큰 ASIC 고객인 Google은 datacenter에 CPO를 배포하는 것을 가장 주저하는 hyperscaler이다. 
Google의 infrastructure philosophy는 절대 성능보다 reliability에 더 중점을 두며, CPO의 reliability는 그들에게 deal breaker이다. 
우리는 Google이 곧 CPO를 채택할 것으로 기대하지 않는다.

Broadcom CPO endpoint의 미래 세대도 TSMC의 COUPE platform으로 이동하고 있다.
COUPE가 제공하는 기능이 bandwidth scaling에 대한 경로를 제공한다는 명확한 signal이며, 이것은 OE를 package하는 방식의 변화일 뿐만 아니라 Broadcom의 이전 세대가 edge coupling과 MZM을 사용했다는 것이다. 
이 두 가지 선택은 구현 관점에서 더 간단했지만 위에서 논의한 것처럼 덜 scalable하기도 했다. 
COUPE는 grating coupling과 MRM에 bias되어 있으며 이는 기존 접근 방식과 극적인 변화이다. 
Broadcom이 가장 많은 CPO 경험을 가지고 있음에도 불구하고 이러한 technical approach의 변화는 Broadcom이 기술의 일부 측면에서 본질적으로 새로 시작해야 함을 의미한다. 
문제는 TSMC가 Broadcom을 위해 design을 더 쉽게 만들기 위해 얼마나 많은 도움을 제공할 수 있는가이다.

\subsection{Intel's CPO Roadmap}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/c8c4cc24-f4d2-4b56-9a78-1d05e8df788c_1817x1383.jpg}
\caption{Source: Intel}
\end{figure}
Intel은 올해 Intel Foundry Direct Connect에서 CPO roadmap을 공개하고 4단계 개발 roadmap을 설명했다

\textbf{2023}: Intel은 optical integration의 precursor로 advanced electrical package-to-package I/O connectivity에 대한 concept을 설명했다. 이 milestone은 multi-die system을 지원하기 위해 chip package 간 (전통적인 PCB trace를 우회하는) high-bandwidth, short-reach electrical link를 가능하게 하는 데 초점을 맞췄다. 나중에 optical channel로 augment될 수 있는 package-level I/O infrastructure를 확립함으로써 photonics를 통합하기 위한 stage를 설정했다.

\textbf{2024}: Intel은 direct fiber attach를 특징으로 하는 1세대 CPO 솔루션을 시연했다. 
이 접근 방식에서 optical engine chiplet은 external connector 없이 optical fiber에 직접 coupled되어 link를 단순화한다. 
OFC 2024에서 Intel은 concept Xeon CPU와 co-package된 4 Tbit/s (bi-directional) textbf{Optical Compute Interconnect (OCI)} chiplet을 선보이며 single-mode fiber link를 통해 error-free 데이터를 실행하고 각각 32 Gbit/s에서 64 lane을 제공했다. optical interface는 \textasciitilde{}5 pJ/bit에서 뛰어난 efficiency를 달성했다.

\textbf{2025}: Intel의 2세대 CPO 솔루션은 permanent fiber pigtail 대신 detachable optical package connector를 통합한다. 
Intel은 embedded 3D waveguide와 on-package photonics를 standard fiber connector와 interface하는 mechanical alignment feature를 포함하는 package 측면에 slot되는 glass optical bridge를 개발했다. 이 optical package connector design은 modular assembly를 가능하게 하여 더 connectorized되고 serviceable한 form factor로의 전환을 표시한다.

\textbf{2027}: Intel은 3D-integrated photonics의 breakthrough를 목표로 하고 있다 (Vertical expanded beam coupling을 사용하여 photonic component를 vertically stacking). 
이 구상된 3세대 design에서 optical I/O는 short free-space 또는 in-glass optical path를 통해 die layer 사이 (예를 들어 photonic interposer layer와 logic die 사이)를 vertically하게 routing될 것이다. package를 통해 빛을 vertically하게 coupling함으로써 Intel은 electrical bottleneck를 더욱 줄이고 10년 후반에 ultra-high bandwidth chiplet fabric을 가능하게 하는 것을 목표로 한다.

\subsection{MediaTek CPO plans}
Custom ASIC design house인 MediaTek은 design platform에 CPO capability를 통합하기 위해 노력하고 있다. 
그들은 custom accelerator와 seamless하게 작동할 수 있는 PIC/EIC design을 제공하는 것을 목표로 한다. 
그들은 200G-per-lane 세대에서 \textbf{Near-Packaged Copper (NPC)}가 fiber pitch >900 µm로 효과적인 솔루션이 될 수 있다고 생각한다.
데이터 rate가 200-300G 범위로 증가함에 따라 더 dense한 pitch >400 µm를 가진 CPC가 더 선호될 수 있다. 
그러나 속도가 400G-per-lane 이상에 도달하면 \textasciitilde{}130 µm의 더욱 dense한 fiber pitch와 더 compact한 interconnect IP를 가진 CPO architecture로 이동하는 것이 필요할 것이다.

\subsection{CPO Focused Companies}
NVIDIA, Broadcom 및 Marvell이 자체 proprietary 솔루션을 만들면서 자체 경로를 개척하고 있는 동안, 여러 CPO 중심 기업은 또 다른 접근 방식 set를 탐색하고 있다. 
이러한 기업의 질문은 주요 switch silicon 및 GPU/ASIC provider와 어떻게 경쟁할 것인가이다. 
특히 이러한 incumbent 대부분이 이미 proprietary 솔루션을 발표하거나 시연했기 때문이다. 
AMD는 예외로 남아 있다. 그들은 어떤 offering도 선보이지 않았지만 내부적으로 photonic IP를 개발하고 있는 것으로 알려져 있다.

Ayar Labs, Lightmatter, Celestial AI, Nubis 및 Ranovus와 같은 OE chiplet provider의 경우 과제는 established player를 overtake하고 통합하기에 충분히 설득력 있는 솔루션을 제공하는 것이다. 
Ayar Labs, Celestial AI 및 Ranovus는 완전한 "bookended" system을 공급하며, 
이는 고객이 완전한 end-to-end 솔루션을 채택해야 함을 의미한다. 
반면 Nubis는 구현을 streamline하고 채택을 더 간단하게 만드는 것을 목표로 더 open하고 standards-based 솔루션에 집중하고 있다. 
다른 한편으로 일부 기업의 제품 roadmap의 중요한 부분을 형성하는 더 radical한 접근 방식이 있다 
(Lightmatter의 optical interposer와 Celestial AI의 photonic bridge) 
이러한 솔루션은 전체 잠재력을 실현하기 위해 package와 host silicon design 모두의 근본적인 재고를 필요로 한다. 
그러나 이러한 접근 방식에는 elevated cost와 상당한 불확실성, 특히 CMOS 기반 silicon 및 high-volume manufacturing과의 seamless한 통합과 관련하여 수반된다.

이러한 각 기업의 architecture와 go to market plan을 통한 tour를 시작해보자.

\subsubsection{Ayar Labs}
Ayar Labs의 제품은 XPU, switch ASIC 또는 memory에 package될 수 있는 TeraPHY optical engine chiplet이다. 
1세대 TeraPHY는 단지 10W의 전력을 사용하면서 2Tbit/s의 uni-directional bandwidth를 제공할 수 있다. 
2세대 TeraPHY는 4 Tbit/s의 unidirectional bandwidth를 제공한다. 
이것은 세계 최초의 UCIe optical retimer chiplet으로 host signal을 optically하게 전송하기 위해 chiplet 내에서 E/O conversion을 수행한다. 
UCIe의 선택은 host chip에 쉽게 구현될 수 있는 표준화된 interface를 가지고 있기 때문에 고객에게 매력적으로 만들어야 한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/ab58856e-1c1b-43e6-9f32-825b26e3a02e_1024x563.jpg}
\caption{Source: Ayar Labs}
\end{figure}
Ayar Labs는 TeraPHY의 처음 두 세대를 GlobalFoundries의 45 nm process에서 electronic과 silicon photonics를 모두 통합하는 monolithic 솔루션으로 제조했으며, 
3세대 TeraPHY는 대신 TSMC COUPE를 채택한다. 
Ring modulator, waveguide 및 control circuitry의 이러한 긴밀한 통합은 electrical loss를 줄이는 데 도움이 된다. 
그러나 처음 두 세대에서 사용된 mature monolithic node는 EIC의 성능을 제약하며 이것이 TeraPHY의 처음 몇 세대가 낮은 modulation rate를 사용한 이유이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/36973977-f4e4-4d06-a2a4-4a6b44bec6b1_1024x525.jpg}
\caption{Source: Ayar Labs}
\end{figure}
4 Tbit/s unidirectional 2세대 code-named \textbf{Eagle}에서 TeraPHY는 각각 MRM에 의해 modulate된 32 Gbit/s NRZ x 16-wavelength architecture로 구동되는 8개의 512 Gbit/s I/O port를 통합한다. 
SuperNova라고 불리는 external laser source는 Swedish 회사 Sivers가 공급한다. 
Laser는 DWDM을 사용하여 16 lambda ("color")를 하나의 fiber로 결합한다. 
그런 다음 각 port는 transmit (Tx) 및 receive (Rx)를 위해 하나의 single-mode fiber pair를 사용하며, 
이는 각 4T chiplet이 총 24 fiber에 연결됨을 의미한다 (Rx/Tx용 16개와 laser input용 8개). 
회사는 packaging process에서 edge coupling (EC)을 사용하지만 grating coupling (GC)도 지원할 수 있다.

Chiplet당 bandwidth를 scaling하기 위해 회사는 fiber density (현재 chiplet당 24개)가 connector 기술이 발전함에 따라향후 몇 년 동안 realistically하게 두 배가 될 수 있다고 언급했다. 
추가로 port/fiber당 bandwidth도 per-wavelength 데이터 rate를 증가시켜 두 배가 될 수 있으며, 가까운 미래 roadmap에서 전체 4배 bandwidth 확장에 기여한다.

SuperNova laser는 \textbf{Multi-Source Agreement (MSA)} compliant하여 다른 CW-WDM standard optical component와 상호운용할 수 있다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/8cf45cbd-6901-4ccc-9965-70363c7efb03_844x588.jpg}
\caption{Souce: Ayar Labs}
\end{figure}
Ayar의 3세대 TeraPHY는 TSMC COUPE를 사용하는 것으로 pivot하며 각 optical engine당 13.5 Tbit/s uni-directional 이상을 제공할 수 있으며, 
8개의 optical engine이 아래 \textasciitilde{}108Tbit/s의 total package scale-up bandwidth를 제공한다. 이 \textasciitilde{}13.5+Tbit/s는 PAM4 Modulation을 사용하여 lambda당 \textasciitilde{}200Gbit/s의 bandwidth를 사용하여 달성된다.

Ayar Labs가 정확한 port architecture (즉, DWDM wavelength 수, FAU당 fiber 등)를 공개하지 않았지만, 
bi-directional optical link의 사용은 Tx와 Rx를 위해 최대 \textasciitilde{}64 fiber strand가 필요하고 external laser source에 연결하기 위해 최대 수십 개가 더 필요함을 의미한다. 
그러나 Ayar의 전략은 항상 WDM에 집중해 왔으며, 이는 FAU당 total fiber count가 총 32개 정도로 낮을 수 있음을 의미한다. 
처음 두 세대와 마찬가지로 3세대 TeraPHY는 optical chiplet이 작게 유지되면서 미래 bandwidth scaling을 위한 vector로 CWDM 또는 DWDM을 가능하게 하기 위해 Microring Modulator를 계속 사용한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/8ea656d5-1940-4070-ae86-2e9ad796c63d_3624x2067.jpg}
\caption{Source: Ayar Labs, Alchip}
\end{figure}
Ayar Labs는 또한 Alchip 및 GUC와 협력하여 chiplet을 Alchip 및 GUC의 XPU 솔루션에 통합할 수 있도록 했다. 
위의 예는 2개의 reticle size compute die와 8개의 TeraPHY optical engine을 가진 XPU를 보여주며, 
이는 최대 108 Tbit/s uni-directional의 bandwidth를 가능하게 할 수 있다.

Hot Chips 2025에서 \href{https://www.youtube.com/watch?v=mZXsIfLKXrM}{Ayar Labs는 결과를 공유했다}.
Slow thermal cycling link test에서 약 5C/min의 rate로 4시간 이상의 thermal cycling을 보여주며 전체에 걸쳐 강력한 link BER을 입증했다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/362d0b77-974d-435d-acca-af5d8464b34c_3030x1684.jpg}
\caption{Source: \href{https://www.youtube.com/watch?v=mZXsIfLKXrM}{Ayar Labs}}
\end{figure}
그러나 temperature의 빠른 변화에 대한 MRM의 resilience를 연구하는 것은 긴 기간 동안 넓은 temperature 범위에 걸쳐 link의 stability를 입증하는 것만큼 중요하다. 
동일한 Hot Chips 강연에서 Ayar는 실제로 0에서 500W step을 수행할 수 있는 on-package ASIC를 갖는 대신 laser wavelength를 sweeping하여 fast temperature ramp를 emulate하기로 선택한 방법을 설명했다. 
Control circuit은 ring resonance가 drift하는지 감지한다. 
이는 incoming laser가 wavelength를 변경하거나 ring temperature의 변화로 인해 발생할 수 있으므로 temperature의 동등한 변화에 해당하는 rate로 laser wavelength를 sweep한다. 
예를 들어 20nm/s sweep은 0.2 second에 걸쳐 64C 변화를 simulate하여 320 C/s에 해당한다. 이 연구는 temperature 변화의 800C/s까지 bit error가 없음을 보여주었다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/8b6eb82b-e964-4837-9694-27230f0ace2c_3030x1684.jpg}
\caption{Source: Ayar Labs}
\end{figure}
Ayar Labs는 GlobalFoundries, Intel Capital, NVIDIA, AMD, TSMC, Lockheed Martin, Applied Materials 및 Downing을 포함한 광범위한 strategic backer를 보유하고 있다.

\subsubsection{Nubis}
Nubis는 최근 2025년 10월 \href{https://www.ciena.com/about/newsroom/press-releases/ciena-to-acquire-nubis-communications-to-expand-its-inside-the-data-center-strategy-and-further-address-growing-ai-workloads}{Ciena에 의해 인수}되었다. Ayar와 유사하게 Nubis는 고객 host silicon과 통합하기 위한 optical engine chiplet을 제공하지만 single wavelength connection에 중점을 둔다. Nubis는 interoperability -- protocol과 mechanical (즉, pluggable) 모두 -- 에 집중해 왔으며 이는 그들의 기술 선택을 결정했다. 
Nubis는 또한 일반적으로 I/O wall을 해결하기 위한 더 광범위한 mission을 가지고 있으며, 그들의 솔루션은 optics와 copper를 모두 포함한다.

기존 optical engine 제품은 Vesta 100 1.6T NPX optical engine이다. 이것은 100G의 16 lane으로 1.6T의 bi-directional bandwidth를 제공하는 socketable module이다. module은 6x7mm의 footprint를 가지고 있다. Nubis는 다른 기업과 달리 modulator의 interoperability, reliability 및 maturity로 인해 주로 MZM을 사용하고 있다. 다른 주요 design 선택은 Nubis가 대부분의 ASIC developer가 이러한 기술을 계속 활용할 것이라고 믿기 때문에 IEEE/OIF standard-compliant electrical interface와 호환되도록 design되었다는 것이다.

Nubis의 주요 차별화 point는 fiber를 couple하는 방법이다. Nubis는 PIC의 표면을 couple하며, 특히 fiber를 routing하고 정렬하는 데 도움이 되는 얇은 glass piece를 사용한다. optical fiber가 chip의 edge에 연결되는 edge coupling과 달리 Nubis의 2D fiber array 접근 방식은 silicon photonics die의 top에서 optical fiber를 연결하는 것을 포함한다.
아래 다이어그램을 보면: PIC (bottom의 green)는 modulator, photo detector 및 waveguide를 포함하며 top에 EIC가 mounted되어 있다. red pole은 optical fiber이고, optical fiber를 포함하는 block은 fiber holder로 사용되는 glass block (FAU)이다. FAU는 정확한 fiber positioning을 보장하기 위해 block의 top에 laser-drilled hole을 가지고 있다. 2D fiber array를 사용함으로써 그들은 36개의 optical fiber (transmit용 16개, receive용 16개, laser용 4개)를 PIC에 연결하고 더 적은 fiber에 더 많은 lambda를 얻기 위해 WDM의 필요성을 피할 수 있다. 이것은 Nubis FAU를 현재 shipping 중인 가장 dense한 것 중 하나로 만든다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/7c707240-0f0c-44f7-b7d9-e3c4257d9b4d_2559x2067.png}
\caption{Source: Nubis}
\end{figure}
2D fiber array는 TSMC와 같은 기업의 roadmap에 있고 vertical coupling의 주요 이점이지만, Nubis 외에는 아무도 아직 이것을 shipping하지 않으며 이것이 그들을 구별하지만, 다른 기업들은 나중에 2D array로 이동할 계획이다.
Optical fiber는 위로 올라가고 Sumitomo Electric이 개발한 \textbf{FlexBeamGuidE}라고 하는 특수 optical fiber를 사용하여 90-degree angle로 구부러지면서 높은 reliability와 낮은 loss를 나타낼 수 있어 sideways로 구부러진다.

Edge coupling이 아닌 2D array를 사용하는 또 다른 이점은 연결할 수 있는 fiber 수에 의해 물리적으로 덜 제한된다는 것이다. 아래 다이어그램에서 볼 수 있듯이 Nubis의 2D Fiber array 구조를 사용하면 package가 허용하는 경우 여러 row의 optical engine을 ASIC 주위에 배치하여 bandwidth density를 증가시킬 수 있다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/b3f5c8ef-15d8-45cf-886e-25aef3d4d648_1024x521.jpg}
\caption{Source: Nubis}
\end{figure}
2025년 4월 Nubis는 0.5Tbps/mm의 unidirectional beachfront density (electrical host interface density와 일치)를 가진 16 x 200G per lane silicon photonics IC인 차세대 PIC의 availability를 발표했다. 추가로 Nubis는 Samtec Si-Fly HD Co-Packaged copper connector와 snap-in 호환 가능한 32x 200G (6.4T) optical module을 sample할 Samtec과의 partnership을 발표했다. 대안 CPO 접근 방식과 비교하여 이 접근 방식은 공통 copper 및 optical footprint를 가능하게 한다; 이것은 또한 시간이 지남에 따라 CPO를 배포하기 위한 open pluggable ecosystem을 만들 수 있다.

마지막으로 copper에서 Nubis는 OFC에서 200G over copper의 reach를 several meter로 확장할 수 있는 \textbf{Active Copper Cable (ACC)}용 linear redriver chip인 Nitro를 발표하고 시연했다. 이것은 Nitro linear redriver를 기반으로 ACC를 구축할 Amphenol과의 partnership으로 수행된다.

\subsubsection{Celestial AI}
Celestial AI는 AI scale-up network를 위한 optical interconnect 솔루션을 전문으로 하는 IP, 제품 및 system 회사이다. 
회사 기술의 주요 목표는 photonic device (modulator, PD, waveguide 등)를 외부 세계와의 interface (FAU를 가진 GC)와 결합된 interposer에 구축하는 것이다. 
아래 다이어그램은 Celestial AI가 \textbf{Photonic Fabric TM (PF)}라고 부르는 photonics 기반 interconnect 솔루션 suite의 좋은 표현이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/c70caf3d-f7a6-4e7d-82a0-40501a95e55e_4380x2541.jpg}
\caption{Source: Celestial AI}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/4d2d54f5-9d19-4c0c-b260-2a422a372467_4380x2508.jpg}
\caption{Source: Celestial AI}
\end{figure}
PF Chiplet은 \textbf{Universal Chiplet Interconnect Express (UCIe)} 및 MAX PHY와 같은 die-to-die interface를 통합하는 TSMC 5nm chiplet으로 XPU-to-XPU, XPU-to-Switch 및 XPU-to-Memory connectivity를 가능하게 한다. 고객이 XPU와 함께 co-package할 수 있으며, electrical SerDes interface를 기반으로 하는 CPO 제품보다 더 높은 bandwidth density와 더 낮은 전력 소비를 제공한다. Celestial AI는 특정 D2D interface 및 protocol을 수용하기 위해 customer별로 이러한 chiplet을 개발한다. 1세대 PF Chiplet은 16 Tbit/s의 bandwidth를 지원하는 반면, 2세대는 64 Tbit/s를 제공할 것이다.

Optical chiplet은 전통적인 copper trace와 비교하여 강력한 전력 이점을 제공한다. 
224G의 linear SerDes를 가진 전통적인 copper cable은 \textasciitilde{}5 pJ/bit를 필요로 한다. 
2개의 end가 필요하므로 총 전력 소비는 \textasciitilde{}10 pJ/bit이다. 
Celestial AI의 솔루션은 전체 electrical-optical-electrical link에 단지 \textasciitilde{}2.5 pj/bit만 필요하다 (plus external laser를 위한 \textasciitilde{}0.7 pJ/bit).

다음으로 Photonic Fabric TM \textbf{Optical Multichip Interconnect Bridge TM (OMIB TM)}은 본질적으로 CoWoS-L style 또는 EMIB style packaging 솔루션이다. 
Interposer의 embedded bridge에 직접 photonics를 추가하여 bridge가 consumption point로 직접 데이터를 이동할 수 있도록 한다. 
Beachfront constraint에 의해 제한되지 않기 때문에 PF Chiplet보다 더 높은 전체 chip bandwidth를 제공한다.

metal interconnect를 가진 전통적인 interposer 또는 substrate에서 chip의 center에 I/O를 배치하는 것은 비실용적인데, high-density signal congestion으로 인한 과도한 routing 복잡성과 심각한 crosstalk 문제를 만들기 때문이다. 
그러나 OMIB optical interposer를 사용하면 Celestial AI는 interposer를 ASIC 바로 아래에 배치하여 shoreline limitation을 우회하고 최소한의 crosstalk로 더 빠르고 효율적인 데이터 이동을 가능하게 할 수 있다.

Optical interposer는 I/O를 chip의 어디에나 배치할 수 있는데, \textbf{optical} waveguide는 거리에 걸쳐 무시할 수 있는 signal degradation을 경험하여 우리가 알고 있는 shoreline의 전통적인 제약을 제거하기 때문이다. 또한 crosstalk를 제거하는데, 다른 waveguide의 빛 signal은 densely packed copper trace의 electrical signal처럼 interfere하지 않기 때문이다. 왜냐하면 cladding의 외부에 작은 evanescent field만 있는 waveguide core 내에 매우 confined되어 있기 때문이다. 이러한 I/O design 및 배치의 ground-up 재설계는 optics가 제공하는 잠재력을 완전히 활용한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/d96ba1c1-81fc-42a2-8628-a2f8010117f6_1622x876.jpg}
\caption{Source: Celestial AI}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/6d4a954c-8753-42ad-86f2-a1248eabdbd4_4380x2499.jpg}
\caption{Source: Celestial AI, Marvell}
\end{figure}
Optical interposer의 아이디어 또는 advanced package를 통한 optical signal channeling은 둘 다 logic chip 아래로 optical signal을 routing하여 shoreline constraint를 피한다는 점에서 Lightmatter의 솔루션과 몇 가지 유사성을 가지지만 몇 가지 주요 차이점이 있다. 
Celestial AI는 silicon bridge (CoWoS-L silicon bridge처럼 생각)와 유사한 photonic bridge를 채택하는 반면 Lightmatter는 여러 개별 chip 아래에 위치하는 큰 multi-reticle photonic interposer를 사용한다. 
Lightmatter의 concept는 scope에서 더 야심적이다. 
M1000 3D Photonic Superchip에서 4,000 mm2 interposer size를 목표로 하는 동시에 interposer 내에서 optical circuit switching을 지원하고 매우 높은 114 Tbit/s의 total aggregate bandwidth를 목표로 한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/57501d2f-ac77-461c-90ad-dcb0336bc51e_4380x2484.jpg}
\caption{Source: Celestial AI, Marvell}
\end{figure}
마지막으로 Celestial AI는 각 ASIC당 7.2T scale-up bandwidth에서 16개의 ASIC를 연결할 수 있는 115.2T total bandwidth를 가진 TSMC 5nm에 구축된 in-network memory를 가진 high-bandwidth, low-latency scale-up fabric인 \textbf{Photonic Fabric TM Memory Appliance (PFMA)}를 제공한다. 
주목할 만하게도 PFMA는 chip의 center에 위치한 on-die optical I/O를 가진 세계 최초의 silicon device로 memory controller를 위한 희소한 perimeter physical I/O를 남긴다. 
이것은 PFMA를 KVCache offloading을 위한 host CPU memory와 storage 사이의 \textbf{warm memory tier}로 위치시킨다.

Celestial AI 기술의 주요 차별화 요소는 \textbf{Electro Absorption Modulator (EAM)}의 사용이다. 
이 문서의 Part 3은 EAM이 어떻게 작동하는지 더 자세히 설명하고 EAM과 관련된 이점과 trade-off를 논의한다. 
EAM의 장단점을 이해하는 것이 Celestial의 go to market을 이해하는 데 핵심이기 때문에 여기서 이 논의의 대부분을 반복한다.

EAM은 MRM 및 MZI와 비교하여 아래의 여러 이점을 가지고 있다:

\begin{itemize}
  \item 분명히 -- EAM과 MRM 모두 temperature variation에 대해 둘 다를 안정화시키기 위해 작동하는 control logic과 heater를 가지고 있지만, EAM은 근본적으로 temperature에 대한 sensitivity가 더 낮다. MRM과 비교하여 EAM은 50°C 이상에서 훨씬 더 나은 thermal stability를 가지는 반면 MRM은 temperature에 매우 민감하다. MRM의 일반적인 stability인 70-90 pm/C는 2°C variation이 resonance를 0.14nm만큼 이동시킨다는 것을 의미하며, 이는 MRM 성능이 붕괴되는 0.1nm resonance shift를 훨씬 넘는다. 이와 대조적으로 EAM은 최대 35°C의 순간 temperature shift를 견딜 수 있다. 이 tolerance는 특히 Celestial AI의 접근 방식에서 중요한데, 그들의 EAM modulator가 수백 watt의 전력을 dissipate하는 high-XPU power compute engine 아래의 interposer 내에 위치하기 때문이다. EAM은 또한 약 80°C의 높은 ambient temperature 범위를 견딜 수 있으며, 이는 XPU 옆에 있고 그 아래에 있지 않은 chiplet application에 적용될 수 있다.

  \item MZI와 비교하여 EAM은 크기가 훨씬 작고 전력을 덜 소비하는데, MZI의 상대적으로 큰 크기가 high voltage swing을 필요로 하여 0-5V의 swing을 달성하기 위해 SerDes를 증폭하기 때문이다. Mach Zender Modulator (MZM)는 \textasciitilde{}12,000mm2 정도이고, EAM은 약 250mm2 (5x50mm)이며 MRM은 25mm2에서 225mm2 사이 (diameter 5-15mm2)이다. MZI는 또한 그러한 큰 device를 desired bias로 유지하는 데 필요한 heater에 대해 더 많은 전력 사용을 필요로 한다.

\end{itemize}
반면에 CPO를 위해 GeSi EAM을 사용하는 데 몇 가지 단점이 있다:

\begin{itemize}
  \item MRM 및 MZI와 같은 Silicon 또는 Silicon Nitride에 구축된 물리적 modulator 구조는 GeSi 기반 device보다 훨씬 더 큰 endurance와 reliability를 가진 것으로 인식되어 왔다. 실제로 많은 이들이 Germanium 기반 device를 다루고 통합하는 어려움을 고려할 때 GeSi 기반 device의 reliability에 대해 걱정하지만, Celestial은 본질적으로 Photodetector의 역인 GeSi 기반 EAM은 오늘날 transceiver에서 photodetector의 ubiquity를 고려할 때 reliability와 관련하여 알려진 양이라고 주장한다.

  \item GeSi modulator의 band edge는 자연적으로 C-band (즉, 1530nm-1565nm)에 있다. 이것을 O-band (즉, 1260-1360nm)로 이동시키기 위해 quantum well을 design하는 것은 매우 어려운 engineering 문제이다. 이는 GeSi 기반 EAM이 book-ended CPO system의 일부를 형성할 가능성이 높으며 open chiplet 기반 ecosystem에 참여하는 데 쉽게 사용될 수 없음을 의미한다.

  \item C-band laser source 주변의 laser ecosystem을 구축하는 것은 O-band CW laser source 주변의 잘 개발된 ecosystem을 사용하는 것과 비교할 때 scale의 diseconomy를 가질 수 있다. 대부분의 datacom laser는 O-band를 위해 구축되지만 Celestial은 상당한 volume의 1577nm XGS-PON laser가 제조된다고 지적한다. 이것들은 일반적으로 consumer fiber to the home 및 business connectivity application에 사용된다.

  \item SiGe EAM은 MRM과 MZI 모두에 대한 3-5dB와 비교하여 약 4-5dB의 insertion loss를 가진다. MRM은 다른 wavelength를 직접 multiplex하는 데 사용될 수 있는 반면, EAM은 CWDM 또는 DWDM을 구현하기 위해 별도의 multiplexer를 필요로 하여 잠재적 loss budget에 약간 추가한다.

\end{itemize}
전반적으로 Celestial AI는 custom link를 혁신하기 위해 노력해 왔다. 
그들은 어떤 gearbox component에도 의존하지 않으며 더 나은 latency와 power efficiency를 제공하고 다양한 type의 protocol에 adaptive하다. 
앞서 언급했듯이 Celestial AI는 주로 modulation을 위해 EAM을 사용하는 유일한 주요 player이다. 
한 가지 주요 implication은 그들이 EAM design을 foundry에 통합하는 데 앞으로 일부 작업이 있을 것이라는 것인 반면, 
다른 CPO 기업은 MRM 및 관련 heater가 이미 PDK의 일부인 TSMC COUPE에 의존할 수 있다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/ecb3b556-cc14-4576-b521-a5e4d6a9f122_1999x1114.jpg}
\caption{Source: Celestial AI}
\end{figure}
단기적으로 Celestial AI는 chiplet 출시에 대한 야심찬 timeline에 commit하고 있다. 
Marvell은 transaction summary에서 Marvell의 Fiscal Year 2028 말 (즉, F1/28)인 2028년 1월 말 Celestial로부터의 estimated revenue run rate가 \$500 million에 도달할 것으로 예상된다고 발표했다. 
Barclay's Global Technology Conference에서 그들은 이 run rate가 Calendar Year 2028 말까지 (CY28의 대부분이 2029년 1월에 끝나는 Marvell의 FY에 해당.
즉, F1/29) \$1B로 두 배가 될 것으로 예상된다고 추가했으며, 이는 제품이 commercial viability를 달성하기까지 지금부터 2027년 말까지 2년 기간을 의미한다.

Deal term의 일부로 Celestial AI의 equity holder에 대한 추가 \$2.25B의 payout은 회사가 2029년 1월 (Marvell의 Fiscal Year 2029의 말, 즉 F1/29)까지 최소 \$2.0 billion의 cumulative revenue를 달성하는 것에 달려 있다. 
전체 payout에 대한 첫 번째 milestone은 payout의 1/3를 위해 2029년 1월까지 \$500M의 cumulative revenue를 달성하는 것이다. 
F1/29를 exit하며 예상되는 \$1B revenue run rate는 earn-out 금액의 절반이다.
Celestial이 \$2B earn-out target을 달성하기 위해 order book에 추가 고객을 추가해야 함을 의미한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/4989be41-b872-4506-bd5a-37142db93c95_1454x810.png}
\caption{Source: Celestial AI, Marvell}
\end{figure}
Celestial AI 인수와 관련하여 Marvell은 2025년 12월 2일 \href{https://www.sec.gov/Archives/edgar/data/1835632/000119312525305271/d81371d8k.htm}{8-K report}를 제출하여 2030년 12월 31일까지 \$87.0029의 exercise price로 Amazon warrant를 발행했다. 
이러한 warrant는 \textbf{2030년 12월 31일까지 간접적으로 또는 직접적으로 Amazon의 Photonic Fabric 제품 구매를 기반으로}vest하며, 
이것이 2027년 후반에 ramp하기 시작하는 AWS의 Trainium이 target 제품임을 강력히 시사한다. 
Marvell의 Industry Analyst Day에서 Celestial AI는 주요 hyperscaler가 해당 hyperscaler의 차세대 processor에서 volume production으로 이동할 advanced AI system을 위한 optical interconnectivity를 위해 그들을 선택한 방법을 논의했다. 
이것은 transaction summary의 earn-out timing 및 product revenue guidance와 함께 Celestial AI가 Trainium 4 내에 솔루션을 배포하는 것을 목표로 하고 있음을 시사한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/e6b68504-b727-4c88-af5b-34e6916d5eb1_840x277.jpg}
\caption{Source: Marvell SEC Filings}
\end{figure}

chiplet에 연결될 16Tbit/s Photonics Link를 중심으로 oriented될 시장에 나올 첫 번째 scale-up 솔루션에 대해 더 자세히 설명하여 Celestial AI에 대한 논의를 마무리하자. 
FAU는 grating coupler를 통해 channel waveguide에 연결된다. 
Scale-up switch ASIC (아마도 Marvell의 115.2T scale-up ASIC)은 Photonics link와 PF chiplet을 통해 XPU에 optically하게 연결될 것이다. 
Celestial은 초기 go-to-market revenue의 대부분이 chiplet에 의해 기여될 것으로 예상하지만, 
system 회사로 자신을 포지셔닝하며 이 첫 번째 scale-up networking 솔루션 이후 시장에 나올 수 있는 여러 optical 기반 memory expansion 솔루션을 제안했다.

optics를 사용하여 여러 switch layer를 통해 scale-up world size를 증가시키는 것은 새로운 concept이 아니지만, 물론 아직 productized되는 것에 가까이 가지 못했다. 
그러한 concept는 GB200의 NVL576 concept를 mirror하는 topology를 가질 수 있으며, 
여기서 2개의 switch layer가 있고 각 switch layer는 OSFP transceiver module 및 optical fiber를 통해 다른 layer에 연결된다. 
여러 switch layer를 사용하는 Celestial AI의 접근 방식은 유사하지만 실제 transceiver의 사용을 skip한다.

그러나 NVL576 concept와의 가장 큰 차이점은 scale-up ASIC가 router와 memory endpoint 모두로 double할 수 있는 반면 
NVSwitch는 GPU 간 high-bandwidth link만 route한다는 것이다. 
이것은 중요한 구분인데, Celestial AI의 pitch는 scale-up 솔루션이 XPU에 부착할 수 있는 HBM stack 수를 제한하는 silicon beachfront constraint를 sidestep할 수 있다는 것이기 때문이다.

이를 달성하기 위해 XPU에 부착된 HBM stack은 shared HBM pool인 Photonic Fabric에 연결되는 chiplet으로 교체된다. 
Shared HBM pool은 각각 하나의 port로 구성된 16개의 Photonic Fabric ASIC로 구성된 2U-rack-mountable system인 \textbf{Photonic Fabric Appliance (PFA)}이다.
각 ASIC는 2개의 36GB HBM3e memory와 8개의 external DDR5와 통합된 2.5 layer package이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/a83bfcd7-a955-49fc-ba8f-8d0dc8e6467a_3216x2409.jpg}
\caption{Source: \href{https://arxiv.org/pdf/2507.14000}{Celestial AI}}
\end{figure}
Optical I/O (Photonic Fabric IP)는 beachfront가 아닌 ASIC의 middle에 mount되어 다른 use case를 위해 shoreline을 free한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/546e9803-b9c8-4f31-82ef-9764fdb49f88_3869x1430.jpg}
\caption{Source: \href{https://www.youtube.com/watch?v=Rchp-QjdCgE}{Celestial AI}}
\end{figure}
Zoom out하면 각 PFA module은 최대 16개의 XPU를 지원할 수 있는 16-radix switch이다. 
각 XPU가 모든 16 port로 fan-out하는 대신 all-to-all connectivity는 switch box 내부에서 발생하며, 
여기서 각 Switch ASIC에 연결된 \textbf{Fiber Attach Unit (FAU)}가 16개의 switch I/O 각각으로 fan out한다. 
따라서 모든 XPU는 box 외부의 하나의 switch port에 하나의 Fiber link만 가진다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/cf9f8ffa-be8d-4d86-a7de-8fa4f72c45d6_4380x1830.jpg}
\caption{Source: \href{https://arxiv.org/pdf/2507.14000}{Celestial AI}}
\end{figure}
Memory를 XPU 외부에 배치하고 shared switching interface 내에 배치함으로써 
데이터가 집계되고 subsequently all-reduce communication collective에서 모든 XPU에 의해 shared memory pool에서 접근된다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/dcb8fdb0-34ec-4988-80d6-957f9d87a433_3081x3335.jpg}
\caption{Source: \href{https://arxiv.org/pdf/2507.14000}{Celestial AI}}
\end{figure}

\subsubsection{Lightmatter}
Lightmatter는 Optical Interposer 제품인 \textbf{Passage™ M1000 3D Photonic Superchip}으로 잘 알려져 있지만 CPO roadmap의 다양한 단계에 맞는 여러 솔루션을 도입하고 있으며, 여러 chiplet이 TSMC에서 tape out되고 있다.

시장에 나올 첫 번째 솔루션은 2026/2027년 textbf{Near Packaged Optics (NPO)}를 위한 optical engine이 될 것이다. 
NPO 솔루션에서 Optical Engine은 baseboard에 soldering되며, copper가 XPU의 LR SerDes를 Optical Engine에 연결한다. 
Lightmatter의 optical engine은 FAU당 40 fiber strand를 가진 최대 3개의 FAU를 지원하여 총 120 fiber strand를 제공한다. 
NPO 전략은 CPO 채택에서 Hyperscaler의 첫 번째 단계가 먼저 NPO로 operational 경험을 얻는 것이라는 아이디어에 기반하며, 
이는 Hyperscaler가 CPO에 commit할 필요가 없기 때문에 제품을 derisk한다. 
왜냐하면 궁극적으로 XPU 또는 switch의 LR SerDes와 interface하기 위해 optical 또는 copper scale-up 솔루션을 사용하도록 선택할 수 있기 때문이다.

Lightmatter의 optical engine 솔루션은 TSMC COUPE 및 GF 45nm SPCLO process를 기반으로 하므로 많은 scaling vector가 table에 있다. 
100Gbaud PAM4를 통해 lane당 200Gbit/s (uni-directional)를 제공하는 것 외에도 DWDM8로 PAM4에서 200Gbit/s를 지원하거나 fiber당 3.2T를 달성하기 위해 DWDM16을 사용하여 PAM4에서 100Gbit/s를 지원할 수 있다.

일부 다른 CPO 기업은 merchant laser source ecosystem을 사용하기로 선택한 반면, 
Lightmatter는 현재 sampling 중인 GUIDE로 알려진 자체 external laser source를 개발했다. 
다른 laser source가 InP wafer를 singulate하여 discrete laser diode를 생성하는 반면, 
GUIDE는 업계 최초의 \textbf{Very Large Scale Photonics (VLSP)} laser로 최대 50 Tbit/s의 bandwidth를 지원하기 위해 단일 silicon chip에 수백 개의 InP laser를 통합하는 새로운 class의 laser이다. 
Lightmatter는 이러한 많은 InP laser를 관리하기 위한 고유한 control 기술을 가져온다고 주장하며,
이는 또한 InP laser 수를 overprovisioning하고 여전히 작동하는 diode로 swap하여 \textbf{self repair}를 허용함으로써 전체 reliability를 증가시키는 이점을 가진다. 
800G의 144 port를 특징으로 하는 NVIDIA Quantum-X CPO Switch는 18개의 ELS를 필요로 하며, 
Lightmatter는 2개의 GUIDE laser source가 동일한 전체 bandwidth 요구사항을 충족할 수 있다고 주장한다.

Lightmatter는 2027년과 2028년에 본격적으로 CPO 솔루션을 제공하는 COUPE roadmap과 align한 다음 2029년 이후 flagship Passage™ M1000 솔루션에 집중하는 것을 목표로 한다.
Lightmatter의 M1000 3D Photonic Superchip은 host compute engine 아래에 배치되고 electrical에서 optical로 signal conversion을 처리하는 4,000 mm² optical interposer이다. 
M1000은 \href{https://youtu.be/Gjee92kYmwg?si=bP09-v5rALXtwOkY}{\textbf{Supercomputing 2025 (SC25)}에서 live rack-scale demonstration으로 시연}되었으며, Lightmatter는 이를 reference design으로 사용 가능하게 했다. Passage는 XPU와 optical engine 사이의 electrical signal과 전력을 전달하기 위해 TSV를 사용하고 둘을 연결하기 위해 SerDes를 사용한다. ASIC를 optical interposer에 직접 배치함으로써 Passage는 크고 전력을 많이 소비하는 SerDes의 필요성을 제거한다. 대신 114Tbit/s의 total I/O bandwidth를 가능하게 하기 위해 1,024개의 compact, lower-power SerDes (기존 SerDes보다 \textasciitilde{}8배 작음)를 활용한다 (각 SerDes는 112Gbit/s에서 작동). ASIC를 optical interposer의 top에 직접 배치함으로써 chip shoreline constraint도 완화된다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/1d1b3254-db98-4d3e-8f11-cc9dde2da82e_1024x583.jpg}
\caption{Source: Lightmatter}
\end{figure}
System은 redundancy를 관리하는 built-in \textbf{Optical Circuit Switch (OCS)}를 통합한다.
하나의 communication route가 실패하면 traffic이 alternate path를 통해 reroute되어 그러한 대규모 system에서 중단 없는 operation을 보장할 수 있다. 
추가로 neighboring tile은 electrically하게 함께 stitched되어 UCIe와 같은 interface를 사용하여 electronically하게 통신할 수 있다.

Passage는 각각 resistive heater와 통합된 \textasciitilde{}15 µm의 diameter를 가진 MRM을 사용하고 56 Gbit/s NRZ modulation을 달성한다. 
Module은 각각 최대 16 color (wavelength)를 전달할 수 있는 16개의 horizontal bus로 구성된다. 
이러한 color는 200 GHz grid에서 fiber당 16 wavelength를 제공하는 GUIDE에 의해 공급될 것이다.

Passage는 DWDM을 통해 각각 16 wavelength를 unidirectionally하게 (또는 8 wavelength를 bi-directionally하게) 전달하는 256개의 optical fiber를 활용하여 fiber당 1 Tbit/s에서 1.6 Tbit/s의 bandwidth를 제공한다. 
Yield를 개선하기 위해 그들은 chip에 부착된 fiber 수를 최소화하여 복잡성과 manufacturing 과제를 줄였다. 
추가로 그들은 결함이 있는 fiber를 panel에서 쉽게 분리하고 교체할 수 있는 fiber attach system을 구현하여 reliability와 serviceability를 향상시켰다. 
아래 표는 Passage가 현재 지원하는 다른 mode를 반영한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/36b44226-8a2b-48e0-8ae3-333416d66818_5464x2026.jpg}
\caption{Source: \href{https://www.youtube.com/watch?si=QXXxGjE8Yv8gvSRB&t=265&v=Gjee92kYmwg&feature=youtu.be}{Lightmatter}}
\end{figure}
PASSAGE와 관련된 주요 debate 중 하나는 optical interposer가 매우 뜨거운 XPU 바로 아래에 위치한다는 점을 고려할 때 사용된 MRM의 thermal stability이다. 
비교하자면, CPO에 대한 다른 접근 방식은 modulator가 XPU 바로 아래에 배치되는 것을 구상하지 않으므로 thermally하게 관리하기가 더 쉽다. 
이 point에 대응하여 Lightmatter는 PASSAGE의 MRM에 사용되는 control loop이 초당 2,000C의 excursion을 처리할 수 있고 0에서 105C 사이의 temperature를 처리할 수 있다고 설명했다.
즉, 60에서 80C temperature transition이 optical link를 방해하지 않고 10ms 내에 발생할 수 있다.

\href{https://www.youtube.com/watch?v=Gjee92kYmwg}{SC25 demonstration} video는 25C에서 105C 사이의 temperature variation의 illustration을 묘사하여 광범위한 operating temperature를 보여주었지만, 
이 특정한 것은 80C transition이 약 1분 정도 걸려, 상당히 낮은 1.33C per second excursion이지만 SC25에서의 별도 demonstration도 on-chip thermal aggressor를 사용하여 2,000C/s rate에 도달했으며, MRM stabilizer heater가 MRM 자체에서 훨씬 더 낮은 -2에서 +2 C/s 범위를 허용했다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/3902b92c-c1fd-4386-bf13-686adaf0b21d_2019x1959.jpg}
\caption{Source: Lightmatter}
\end{figure}

\subsubsection{Xscape Photonics}
Xscape Photonics는 4에서 16 wavelength를 제공하는 programmable laser인 ChromX를 작업하고 있는 혁신적인 기업이며, 
미래에 최대 128 wavelength를 제공할 계획이다. 최대 128개의 다른 color를 제공함으로써 ChromX는 4에서 8 wavelength만 제공하는 기존 laser와 비교하여 상당히 더 높은 bandwidth를 달성할 수 있을 것이다. 
ChromX는 external III-V laser와 WDM을 위한 여러 wavelength를 생성하는 데 도움이 되는 on-chip multicolor generator에 의존한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/3f319f25-7c39-4fa9-990d-e20d450d2919_1024x491.jpg}
\caption{Source: Xscape Photonics}
\end{figure}
Laser가 programmable하다는 사실은 다른 bandwidth 및 거리 요구사항을 충족하기 위해 다양한 type의 workload에 wavelength를 제공하는 유연성을 제공한다. 
흥미롭게도 그들의 솔루션은 단지 하나의 laser만 필요한 반면 기존 CPO 솔루션은 극도로 높은 전력과 전기 소비를 가진 여러 laser를 필요로 한다. 
더욱이 모든 wavelength는 단일 fiber를 통해 전달되어 여러 fiber를 필요로 하는 대부분의 CPO system을 괴롭히는 복잡성을 피하여 fiber coupling 문제를 크게 줄인다.

\subsubsection{Ranovus}
Ranovus는 optical chiplet 기술과 laser design 및 manufacturing 모두에 집중하고 있다. 
그들은 GlobalFoundries에서 monolithic CPO 제품 (GF가 AMF를 인수하기 전에 원래 AMF에서 tape out됨)과 
다양한 geometry의 PIC와 EIC를 자연스럽게 통합하는 TSMC COUPE 기반 제품을 포함하여 몇 가지 다른 경로에 걸쳐 제품을 tape out했다.
Odin Optical Engine은 Microring Resonator modulator를 사용하며 PAM4 modulation을 사용하여 최대 64 lane의 100Gbit/s를 제공할 수 있다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/e6fba521-f2cf-41ad-9a12-0655c5902c5e_4042x2067.jpg}
\caption{Source: Ranovus}
\end{figure}
Ranovus의 go to market은 고객이 원하는 interoperable 솔루션을 제공하는 것을 중심으로 한다. 
현재 이것은 100G PAM4 DR optics를 의미하지만, modulation을 위한 Microring Resonator의 사용은 56 Gbaud NRZ와 같은 다른 scheme으로 pivot을 가능하게 하지만WDM을 사용하여 4 lambda를 결합하여 fiber pair당 400G를 제공한다.

Ranovus는 800G chiplet에서 AMD와 interoperability를 시연했으며 Hyperscaler의 미래 custom silicon XPU를 위한 chiplet 솔루션으로 Odin direct-drive CPO 3.0을 제공하기 위해 MediaTek과 partnership을 맺었다.

\subsubsection{Scintil}
Scintil의 주요 제품은 LEAF Light로, die format (\textbf{Known Good Die (KGD)})으로 제공되거나 module로 조립될 수 있는 \textbf{Photonic System-on-Chip (PSoC)}으로, 
200 GHz 또는 100 GHz interval로 spaced된 다양한 color의 8개 또는 16개 laser를 통합하여 DWDM을 통해 단일 fiber에서 여러 wavelength를 전달할 수 있다. 
그들은 temperature variation 하에서도 wavelength 간 100 GHz 또는 200 GHz spacing을 정확하게 유지할 수 있는 electronic control을 개발했다. 
OSFP와 유사한 \textbf{External Laser Source Form Factor Pluggable (ELSFP)} module에 대한 package reference design이 \textbf{Optical Internetworking Forum (OIF)}에 의해 정의되어 고객이 이 external laser source를 통합하기 더 쉽게 만든다. 
Scintil의 솔루션은 ring modulator 기반 co-packaged optics와 잘 작동한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/ee39d31b-102a-4bc1-ae5a-d5af30c05f00_3410x1823.jpg}
\caption{Source: Scintil}
\end{figure}
Scintil의 process는 \textbf{Scintil Heterogeneous Integrated Photonics (SHIP)}라고 불린다. 
기술의 본질은 wafer-level process에서 standard silicon photonics에 III-V laser를 통합하는 것이다. 
Process는 standard silicon photonics wafer로 시작한다.
Waveguide, detector 및 mux/demux가 완비된 conventional foundry flow를 사용하여 fabrication된다. 
그런 다음 wafer가 flip되고 새 handle에 bonding되어 original substrate를 제거하고 buried oxide layer를 드러낼 수 있다. 
Unpatterned III--V material이 subsequently 이 새로 노출된 표면에 bonding된다. 
그런 다음 III-V가 lithography로 patterned되고 laser를 fabrication하기 위해 etched되어 on-board laser를 가진 monolithically integrated silicon photonics chip이 생성된다. 
이것은 E-beam writer를 사용하여 patterned된 전통적인 InP 기반 laser와 대조적이며, 여기서 DWDM을 위한 정밀한 wavelength control을 달성하는 것이 더 어려울 수 있어 tightly spaced channel을 지원하기 어렵게 만든다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/3305eaf5-610e-4faa-b7b6-579fbd35faab_1024x415.jpg}
\caption{Source: Scintil}
\end{figure}
DWDM DFB laser array를 개발하는 것은 각 wavelength의 frequency가 정밀하게 생성되어야 하기 때문에 어렵다. 
100 GHz channel separation을 달성하기 위해 silicon photonics foundry와 photolithographic process의 advanced capability가 silicon의 grating을 정확하고 반복적으로 pattern하기 위해 활용되어야 한다. 
더욱이 laser가 wafer level에서 생산되기 때문에 각 wafer에서 수백 개의 device를 fabrication할 수 있어 high-volume이고 scalable한 production을 가능하게 한다.

Scintil 솔루션의 한 가지 주요 이점은 power efficiency이다. 
Scintil 솔루션은 multiplexed color당 target power를 달성하기 위해 매우 high-power laser를 필요로 하는 combiner splitter를 따라 여러 discrete laser를 사용하는 솔루션과 달리 단일 chip에서 여러 colour (8 또는 16)를 생성하고 multiplex할 수 있다. 
Scintil 솔루션은 우수한 power efficiency와 증가된 bandwidth density를 제공하는 동시에 전송된 각 bit에 필요한 에너지를 절반으로 줄인다. 
이것은 낮은 modulation speed를 가진 여러 wavelength보다 높은 modulation speed를 가진 단일 wavelength를 사용하는 기존 co-packaged 솔루션 (NVIDIA가 현재 Q3450 CPO switch에 채택하고 있는 것 포함)과 비교된다.

\newpage
\section{Part 5: NVIDIA's CPO Supply Chain}
CPO system에서 핵심 component가 수행하는 역할을 논의했으며, 이 섹션에서는 supply chain의 특정 기업과 component BOM 비용을 NVIDIA의 공급업체에 초점을 맞춰 논의할 것이며, \textbf{Laser Source, ELS Module, FAU, FAU Alight Tool, FAU Assembly, Shuffle Box, MPO Connector, MT Ferrule, Fiber 및 E/O Testing}에 대한 주요 공급업체를 명시한다.

\subsection{Optical Engines}
NVIDIA의 X800-Q3450 CPO switch는 115.2T total throughput을 특징으로 하며 scale-out network와 함께 사용하도록 설계되었다. 초기 버전은 각각 1.6Tbit/s로 작동하는 72개의 optical engine을 사용할 것이다; 이후 버전은 각각 3.2Tbit/s에서 작동하는 36개의 optical engine으로 전환할 가능성이 있으며, 단위당 (FAU 포함) \textasciitilde{}\$1,000의 비용이 든다. 결과적으로 optical engine의 total BOM cost는 약 \$35-40k (3.2T OE 버전의 경우)에 달한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/fcac4267-bc34-4c35-80a5-8862a4b14539_1024x570.jpg}
\caption{Source: NVIDIA}
\end{figure}

\subsection{External Laser Source (ELS)}
NVIDIA X800-Q3450 CPO switch는 laser source를 위해 18개의 ELS module을 사용하며, 각각 8개의 \textbf{Continuous Wave (CW)} \textbf{Distributed Feedback (DFB)} laser chip을 포함한다. CPO system은 상대적으로 higher-power laser source를 사용해야 하며, 각 CWDFB chip은 \textasciitilde{}350mW의 전력을 제공한다.

CW laser unit을 생산할 수 있는 주요 업계 player에는 Broadcom (U.S.), Furukawa (Japan), Lumentum (U.S.), Coherent (U.S.), Yuanjie (China) 및 Shijia (China)가 포함된다. Lumentum, Coherent, Furukawa 및 Broadcom은 일반적으로 Chinese vendor (Yuanjie 및 Shijia)보다 높은 가격을 부과한다. 우리는 Lumentum이 NVIDIA의 초기 batch CPO switch shipment의 sole supplier가 될 것으로 예상하며, Coherent는 2026년 후반에 두 번째 공급업체로 진입할 수 있다. Chinese manufacturer는 CW laser source가 일반적으로 상대적으로 표준화되고 commoditized된 것으로 간주되기 때문에 앞으로 기회를 볼 수 있지만, CPO application에 필요한 high power laser source를 구축하는 데는 여전히 어느 정도의 moat가 있다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/d21f27ce-f5fa-47b6-8aaa-32cc478e517c_1024x595.jpg}
\caption{Source: NVIDIA}
\end{figure}

\subsection{Fiber Attach Units (FAUs)}
FAU는 optical fiber를 Optical Engine과 coupling하는 것을 담당하는 중요한 passive component이다. 
High-quality FAU와 그들의 정밀한 alignment는 최적의 optical 성능을 보장하는 데 critical하다. 
FAU assembly process의 현재 과제 중 하나는 coupling loss를 측정하는 데 사용되는 testing equipment가 아직 완전히 automated될 수 없다는 것이다. 
결과적으로 testing process는 여전히 manual labor에 크게 의존하며, 이는 전체 production 속도를 늦추고 더 비용이 많이 든다. 
Corning은 Spectrum X CPO system에 대해 FAU testing당 평균 10-15 minute를 추정했다.

material 및 component 비용 외에도 labor는 FAU 비용의 큰 부분이다. 
Skilled labor는 FAU unit에서 fiber의 high quality assembly 및 alignment를 보장하는 데 필수적이다. 
X800-Q3450의 각 1.6T OE는 20 fiber를 가진 FAU를 가지고 있다: 8 transmit, 8 receive 그리고 external laser용 4. 
System당 Tx/Rx용 1152를 포함하여 총 1,440 fiber이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/60b7cf9c-600e-469f-839e-76443d8e4ca5_1024x570.jpg}
\caption{Source: NVIDIA}
\end{figure}
FAU의 leading 기업은 TFC Optical (300394.SH), Senko (9069.JP) 및 FOCI (3363.TW)이다. 
TFC는 X800-Q3450 CPO switch용 FAU를 공급할 강력한 기회를 가지고 있는 반면, Senko는 Spectrum X CPO와 Broadcom의 Tomahawk 6 CPO system 모두에 대한 매우 가능성 있는 후보로 간주된다. 
한편 FOCI는 NVIDIA의 large-up CPO 솔루션에 더 집중할 가능성이 있다.

TFC의 핵심 강점은 위 단락에서 논의한 것처럼 경쟁력을 유지하는 데 핵심 요소인 China의 강력한 manufacturing capability와 숙련되었지만 비용 효율적인 대규모 labor pool에 대한 접근에 있다. 
추가로 TFC는 약 3년 전에 CPO design에 대해 NVIDIA와 협력하기 시작했으며, 그 초기 partnership은 이제 결실을 맺고 있으며, 
TFC는 NVIDIA의 진행 중인 CPO roll-out 노력에서 핵심 역할을 할 것으로 예상된다.

Senko는 CPO system을 위한 detachable FAU 솔루션을 제공하는 signature \textbf{SEAT (Senko Elastic Averaging Technology)} platform을 가지고 있다.
회사는 edge-coupling 기술에 대해 GFS와 긴밀하게 협력하고 있으며 Senko mirror를 wafer trench에 직접 통합하고 있다. 
이 접근 방식은 edge coupling을 위한 wafer-level testing을 가능하게 하며, 이는 지금까지 grating-coupling의 주요 이점이었다.

Space의 다른 주목할 만한 player에는 Sumitomo 및 \textbf{Advanced Fiber Resources (AFR)} 등이 포함된다. 
AFR은 Broadcom의 supply chain 내에서 긴밀한 관계를 가지고 있는 것으로 믿어진다.

SiPho chip은 micron scale의 waveguide channel을 특징으로 하며, 
각각 system에 들어오는 빛과 정밀한 alignment가 필요하다. 따라서 process 동안 exceptionally 높은 accuracy를 가진 coupling machine이 필요하다. 
FiconTEC (Germany)은 현재 high-precision coupling machine을 제공하는 업계를 선도하고 있다. 
그들의 machine은 unit당 \$300k 이상에 판매될 수 있지만, 우수한 accuracy로 인해 여전히 고객들에게 매우 인기가 있다. 
한편 All Ring Tech (Taiwan)도 automated fiber attach equipment를 제공한다. 
회사는 workforce의 약 10\%를 space에 투입했으며 coupling equipment revenue가 2026년부터 견인력을 얻을 것으로 예상한다. 
마지막으로 GMT Global (Taiwan)은 FAU를 통한 정확한 transmission을 보장하기 위해 빛의 wavelength를 correct하는 FAU bonding, alignment 및 inspection machine을 design한다. 
회사는 일반적으로 \$200--250K를 부과하는 비교 가능한 Japanese machine보다 \textasciitilde{}75\% 낮게 machine 가격을 책정하는 것을 목표로 한다. 
회사는 Q4 2025에 일부 pile run을 가질 것으로 예상한다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/7b6b58ee-4bc5-4aa6-bba2-53de375eaf2f_1024x587.jpg}
\caption{Source: NVIDIA}
\end{figure}

\subsection{Fiber Shuffle Box}
NVIDIA의 X800-Q3450 CPO switch의 경우 1,000개 이상의 optical fiber가 OE에서 나오므로 이를 정리하고 destination으로 routing하기 위해 shuffle box가 필요하다. 
전통적으로 이 shuffle box alignment process는 기술자가 물리적으로 fiber를 배열하여 수동으로 수행되었다. 
그러나 일부 leading 기업은 더 높은 accuracy와 efficiency로 alignment를 수행할 수 있는 automated machine을 개발했다.

Shuffle box의 가격은 일반적으로 관리하는 fiber 수와 연결된다. 
예를 들어 T\&S Communications는 48-fiber shuffle box를 \textasciitilde{}\$150에, 300-fiber 버전을 \textasciitilde{}\$1,000에, 그리고 500-fiber model을 \textasciitilde{}\$1,600에 판매한다. 
수천 개의 fiber를 가진 X800-Q3450의 경우 shuffle box는 NVIDIA에게 조달하는 데 \$3,000 이상의 비용이 든다. 
Shuffle box의 주요 BOM component에는 MT ferrule과 fiber가 포함된다.

T\&S Communications (300570.SH)는 shuffle box 산업의 leading player이다. 
회사는 shuffle box 내에서 fiber를 정렬하기 위한 automated machine을 개발했다. 
기술은 특허를 받았으며, 경쟁자가 이를 우회하는 데 시간과 추가 비용이 걸릴 수 있다. 
T\&S의 주요 고객은 Corning이며, 두 회사는 종종 고객에게 서비스를 제공하기 위해 tandem으로 작업한다. 
예를 들어 Corning은 고객 (NVIDIA, Broadcom 등)이 CPO 솔루션을 위한 fiber network를 design하는 데 도움을 주고 shuffle box 부분을 T\&S에 subcontract한다.

Molex는 또 다른 주목할 만한 player이며 시장에 꽤 일찍 진입했다. 
그러나 더 낮은 manufacturing efficiency로 인해 제품 가격은 일반적으로 T\&S보다 \textasciitilde{}20\% 더 높다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/0b63c1c3-befb-481f-b95a-9f02595a2e46_1024x546.jpg}
\caption{Source: NVIDIA}
\end{figure}

\subsection{MPO Connectors}
shuffle box의 outskirt에는 MPO connector가 있으며, 이는 shuffle box 내에 수용된 fiber를 external port에 연결한다. 
Fiber Optic MPO cable은 그런 다음 이 port에 plug하여 switch를 다른 distant switch 또는 distant NIC에 연결할 수 있다. 
MPO connector의 manufacturing process는 주로 unit 내부의 MT ferrule의 injection molding, vacuum adhesive filling 및 fiber threading을 포함한다. 
X-800 Q3450 CPO switch의 경우 144개의 MPO connector가 필요하다.

MPO connector를 생산할 수 있는 여러 player가 있으며, 
US Conec (U.S.), T\&S Communications (China), Senko (Japan), Broadex (China) 및 Optec (China)가 포함된다. 
그들은 또한 전체 network design에 component를 공급하기 위해 fiber network contract manufacturer (Corning과 같은)와 협력해야 한다.

\subsection{MT Ferrules}
MT ferrule은 FAU, shuffle box 및 MPO connector에서 사용되는 핵심 요소이다. 
그들은 여러 fiber를 parallel 방식으로 정렬하는 데 사용된다. 
US Conec (U.S.), T\&S (China), Senko (Japan), Fukushima (Japan), FOCI (Taiwan), Sumitomo (Japan) 및 TFC (China)를 포함한 많은 기업이 MT ferrule을 제조할 수 있다. MT ferrule은 특히 제조하기 어렵지 않지만 필요한 precision과 robustness를 달성하는 데는 여전히 많은 engineering이 필요하다. 
기업은 fiber connection 동안 insertion loss를 최소화하는 ferrule을 생산하기 위한 molding capability에서 경쟁한다.

이러한 기업 중 US Conec은 또한 기술을 개발한 30년 이상의 경험을 가지고 있으며 NVIDIA의 Q3450 CPO system의 주요 공급업체 중 하나가 될 것으로 예상된다. 
Fukushima도 강력한 mold design 및 manufacturing capability를 가지고 있어 경쟁력 있는 가격으로 high-quality MT ferrule을 생산할 수 있다. 
한편 FOCI, TFC 및 T\&S는 주로 자체 in-house FAU 및 shuffle box용 MT ferrule을 생산한다.
Rationale은 quality control 및 cost efficiency를 향상시키기 위해 가능한 한 많은 process를 vertically하게 통합하는 것이다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/354513ce-e754-4d18-be6c-8f9a8748c1d2_1123x592.png}
\caption{Source: \href{https://semianalysis.com/ai-networking-model/}{SemiAnalysis AI Networking Model}}
\end{figure}

\subsection{Manufacturing, Assembling, and the Testing Processes}
\noindent \textbf{OE fabrication}: 위에서 논의한 바와 같이 TSMC는 PIC, EIC를 fabrication하고 통합하며 CPO system의 역할을 하는 데 중요한 역할을 할 것이다. 
그들의 COUPE platform은 차세대 CPO endpoint를 위한 선택 솔루션으로 보인다. 
Global Foundries와 Tower는 강력한 SiPho capability를 가진 foundry이지만, 
leading edge CMOS 및 advanced packaging의 부족은 미래의 더 높은 bandwidth OE를 제공할 수 있는 능력을 제약한다.
\\\\
\noindent \textbf{Advanced packaging}: \textbf{Outsourced Semiconductor Assembly and Test (OSAT)} provider는 back-end process에 집중할 것이다.
OE packaging, OE testing 및 system packaging (laser 및 coupler integration \& testing)을 포함한다.

ASE/SPIL (3711.TW), Amkor (\$AMKR) 및 Shunsin (6451.TW)은 그러한 솔루션의 주요 provider이다. 
그 중 ASE는 미래 Rubin-rack CPO system에 대한 involvement를 포함하여 NVIDIA의 supply chain에서 핵심 공급업체로 두드러지는 반면, Shunsin은 Broadcom과 긴밀한 관계를 유지한다.

다른 주목할 만한 이름은 Fabrinet (\$FN), TFC Optical (300394.SH) 및 Foxconn (2354.TW)이다.
Fabrinet은 오랫동안 NVIDIA의 in-house optical module unit을 위한 module assembler로 봉사해 왔으며 이제 OE packaging, testing 및 full system assembly에서 capability를 적극적으로 구축하고 있다. 
Fabrinet은 또한 Micas 및 Foxconn과 함께 Broadcom의 CPO system assembly 작업의 잠재적 후보 중 하나로 간주된다.

TFC는 지난 3-4년 동안 CPO design에 대해 NVIDIA와 긴밀하게 협력해 왔으며 FAU의 주요 공급업체가 될 것이다. 
회사는 Suzhou, China의 advanced packaging facility에 투자해 왔으며, 이는 CPO supply chain에서 더 큰 역할을 확보하려는 야망을 signal한다.
\\\\
\noindent \textbf{Electro-optical (E/O) testing equipment}: Testing process 동안 위에서 언급한 service vendor는 system reliability를 보장하기 위해 electro-optical (E/O) testing tool을 사용한다. 그러나 업계는 여전히 개발 중이며 photonic engine을 위한 표준화된 testing methodology에 완전히 수렴하지 못했다. 각 vendor는 이 emerging space에서 발판을 얻기 위해 다양한 솔루션을 개발하고 있다.

CPO supply chain의 다른 주요 equipment provider에는 Keysight, Ficontec, Teradyne, Advantest, FormFactor, Chroma, Anritsu 및 Multilane이 포함된다. 
그 중 Keysight는 field의 large player이다.
그들은 premium quality (그리고 premium 가격!)를 가진 high-speed testing equipment를 제공하는 것으로 알려져 있다. 
예를 들어 그들은 \href{https://www.keysight.com/us/en/about/newsroom/news-releases/2025/0325-pr25-056-keysights-new-sampling-oscilloscopes-revolutionize-1-6t-transceiver-optical-testing-for-ai-data-centers.html}{최근 1.6T optical transceiver testing을 위한 2개의 새로운 oscilloscope를 발표}했다. 
시장에서의 position으로 인해 그들은 CPO 추세로부터 잘 benefit을 받을 수 있다.

Ficontec은 photonics testing에 강력한 기반을 가지고 있으며 이제 electrical testing capability도 적극적으로 확장하고 있다. 
주요 강점 중 하나는 wafer-level photonic testing이다.
이는 전통적으로 inefficient한 photonic testing process의 efficiency를 향상시킨다.
예를 들어 회사는 \href{https://www.ficontec.com/ficontec-releases-innovative-wafer-level-test-cell-to-complement-existing-ate/}{최근 기존 semi ATE architecture와 호환되는 PIC용 새로운 high-throughput wafer-level testing tool을 도입}했다. 
Machine은 업계 최초의 double-sided wafer testing machine으로 주장된다. 
Testing equipment 외에도 Ficontec은 FAU assembly 및 coupling machine도 제공하며, 이는 다음 섹션에서 더 자세히 살펴볼 것이다.

Teradyne도 field의 large player로 electrical testing에서 강력한 historical presence를 가지고 있지만, 
회사는 photonic testing에 진입하는 것에 대해 \textbf{very serious}해 왔다. 
예를 들어 회사는 최근 packaged optical testing을 전문으로 하는 startup을 인수했으며, 이는 CPO를 위한 capability를 구축하기 위한 strategic push를 반영하는 move이다.

Chroma는 3D sensing 및 optical communication에 사용되는 laser diode를 위한 photonic testing equipment를 공급해 왔으며, 
이 expertise를 활용하여 CPO space에 진입할 잠재력을 가지고 있다. 
그러나 회사의 innovation pace는 현재 일부 경쟁자보다 뒤처진다.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/14be80f2-b69f-4dd7-a094-2fcc3d318ef6_1024x534.png}
\caption{Source: \href{https://semianalysis.com/ai-networking-model/}{SemiAnalysis AI Networking Model}}
\end{figure}

\end{document}